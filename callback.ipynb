{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gurobipy import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本研究"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "from asyncio.constants import SENDFILE_FALLBACK_READBUFFER_SIZE\n",
    "import random\n",
    "from random import seed\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "class CVaR:\n",
    "    def __init__(self,ship_num,stack_num,height,n_init,size,size_,beta,mean,cov,OC,zk):\n",
    "        self.ship_num=ship_num\n",
    "        self.stack_num=stack_num\n",
    "        self.height=height\n",
    "        self.n_init=n_init\n",
    "        self.beta=beta\n",
    "        self.size=size\n",
    "        self.size_=size_\n",
    "        self.mean=mean\n",
    "        self.cov=cov\n",
    "        self.OC=OC\n",
    "        self.zk=zk\n",
    "        self.O1=[i+1 for i in range(ship_num)]\n",
    "        self.S=[i+1 for i in range(stack_num)]\n",
    "        self.H=[i+1 for i in range(height)]\n",
    "        self.P=[i+1 for i in range(ship_num)]\n",
    "        self.f=stack_num*height-sum(n_init)\n",
    "\n",
    "        # 期待値と分散共分散行列の準備\n",
    "        data_1 = np.random.multivariate_normal(mean, cov, size=self.size)\n",
    "\n",
    "        O_=np.argsort(data_1)\n",
    "        O_=O_+np.ones((size,ship_num)).astype(int)\n",
    "        global O\n",
    "        self.O=O_.tolist()\n",
    "        O=self.O\n",
    "\n",
    "    def solve(self):\n",
    "        def add_cutting_plane(model,where):\n",
    "            if self.zk==None:\n",
    "                self.zk=[]\n",
    "            if where == GRB.callback.MIPSOL:\n",
    "                print(len(self.zk))\n",
    "                c_var={}\n",
    "                for var in self.model._vars:\n",
    "                    if \"u\" in var.VarName:\n",
    "                        u_var=var\n",
    "                    if \"alpha\" in var.VarName:\n",
    "                        alpha_var=var\n",
    "                    if \"c_\" in var.VarName:\n",
    "                        s=int(var.VarName.split(\"_\")[-3])\n",
    "                        h=int(var.VarName.split(\"_\")[-2])\n",
    "                        i=int(var.VarName.split(\"_\")[-1])\n",
    "                        c_var[s,h,i]=var\n",
    "                model.cbLazy(u_var>=alpha_var+quicksum(quicksum(c_var[s,h,i] for s in self.S for h in self.H if h!=1)-alpha_var for i in self.zk)/((1-self.beta)*len(self.O)))\n",
    "        self.model=Model(\"CVaR\")\n",
    "        alpha=self.model.addVar(vtype=\"C\",name=\"alpha\")\n",
    "        u=self.model.addVar(vtype=\"C\",name=\"u\")\n",
    "\n",
    "\n",
    "        # 変数の定義\n",
    "        x,c,d={},{},{}\n",
    "        for s in self.S:\n",
    "            for h in self.H:\n",
    "                for p in self.P:\n",
    "                    x[s,h,p]=self.model.addVar(vtype=\"B\",name=\"x_\"+str(s)+str(h)+str(p))\n",
    "\n",
    "        for s in self.S:\n",
    "            for h in range(2,len(self.H)+1):\n",
    "                for i,o in enumerate(self.O):\n",
    "                    c[s,h,i]=self.model.addVar(vtype=\"c\",lb=0,name=\"c_\"+str(s)+\"_\"+str(h)+\"_\"+str(i))\n",
    "\n",
    "        for i in range(len(self.O)):\n",
    "            d[i]=self.model.addVar(vtype=\"C\",lb=0)\n",
    "\n",
    "        self.model.update()\n",
    "        self.model._vars=self.model.getVars()\n",
    "\n",
    "        for p in self.P:\n",
    "            self.model.addConstr(quicksum(x[s,h,p] for s in self.S for h in self.H)==self.n_init[p-1])\n",
    "\n",
    "        for s in self.S:\n",
    "            for h in self.H:\n",
    "                self.model.addConstr(quicksum(x[s,h,p] for p in self.P)<=1)\n",
    "\n",
    "        for s in self.S:\n",
    "            for h in range(1,len(self.H)):\n",
    "                self.model.addConstr(quicksum(x[s,h+1,p] for p in self.P)<=quicksum(x[s,h,p] for p in self.P))\n",
    "\n",
    "        for s in self.S:\n",
    "            for h in range(2,len(self.H)+1):\n",
    "                for h_ in range(1,h):\n",
    "                    for i,o in enumerate(self.OC):\n",
    "                        for j,p in enumerate(o):\n",
    "                            self.model.addConstr(c[s,h,i]>=quicksum(x[s,h,k] for k in o[j:])-quicksum(x[s,h_,k] for k in o[j:]))\n",
    "\n",
    "        for i in range(len(self.OC)):\n",
    "            self.model.addConstr(d[i]>=quicksum(c[s,h,i] for s in self.S for h in self.H if h!=1)-alpha)\n",
    "\n",
    "        self.model.addConstr(u>=alpha+quicksum(d[i] for i in range(len(self.O)))/((1-self.beta)*len(self.O)))\n",
    "        self.model.setObjective(u)\n",
    "        self.model._vars=self.model.getVars()\n",
    "        self.model.params.LazyConstraints = 1\n",
    "        if self.f>=self.height:\n",
    "            self.model.optimize(add_cutting_plane)\n",
    "        else:\n",
    "            print(\"実行不可\")\n",
    "        if self.model.Status==GRB.OPTIMAL:\n",
    "            EPS=1.e-6\n",
    "            self.result=np.zeros((self.height,self.stack_num))\n",
    "            for (s,h,p) in x:\n",
    "                if x[s,h,p].X>EPS:\n",
    "                    self.result[self.height-h][s-1]=int(p)\n",
    "            self.result=self.result.astype(int)\n",
    "            print(self.result)\n",
    "            self.penalty=[]\n",
    "            for k in self.O:\n",
    "                OO=k\n",
    "                a=0\n",
    "                for j in range(self.stack_num):\n",
    "                    for i in range(1,self.height):\n",
    "                        for i_ in range(i+1,self.height+1):\n",
    "                            if self.result[i-1][j]!=0 and self.result[i_-1][j]!=0:\n",
    "                                if OO.index(self.result[i-1][j])>OO.index(self.result[i_-1][j]):\n",
    "                                    # print(j+1,height-i+1,O)\n",
    "                                    a+=1\n",
    "                                    # print(\"penalty!\")\n",
    "                                        # print(i,i_,j+1,O)\n",
    "                                    break\n",
    "                self.penalty.append(a)\n",
    "            self.zk=[i for i, x in enumerate(self.penalty) if x-alpha.X>0]\n",
    "            for zk_ in self.zk:\n",
    "                self.OC.append(self.O[zk_])\n",
    "            return (self.zk,self.OC,self.penalty)\n",
    "\n",
    "    def get_optimal_sol(self):\n",
    "        if self.model.Status==GRB.OPTIMAL:\n",
    "            x_opt={}\n",
    "            c_opt={}\n",
    "            for var in self.model._vars:\n",
    "                if \"x_\" in var.VarName:\n",
    "                    # print(var)\n",
    "                    s=int(var.VarName.split(\"_\")[-3])\n",
    "                    h=int(var.VarName.split(\"_\")[-2])\n",
    "                    p=int(var.VarName.split(\"_\")[-1])\n",
    "                    x_opt[s,h,p]=var.X\n",
    "                if \"c_\" in var.VarName:\n",
    "                    s=int(var.VarName.split(\"_\")[-3])\n",
    "                    h=int(var.VarName.split(\"_\")[-2])\n",
    "                    i=int(var.VarName.split(\"_\")[-1])\n",
    "                    c_opt[s,h,i]=var.X\n",
    "                if \"alpha\" in var.VarName:\n",
    "                    alpha_opt=var.X\n",
    "                if \"u\" in var.VarName:\n",
    "                    u_opt=var.X\n",
    "            return (x_opt,c_opt,alpha_opt,u_opt)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def get_optimal_val(self):\n",
    "        if self.model.Status==GRB.OPTIMAL:\n",
    "            self.LB=self.model.ObjVal\n",
    "            return self.model.ObjVal\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def get_result(self):\n",
    "        if self.model.Status==GRB.OPTIMAL:\n",
    "            EPS=1.e-6\n",
    "            self.result=np.zeros((self.height,self.stack_num))\n",
    "            x_opt,c_opt,alpha_opt,u_opt=self.get_optimal_sol()\n",
    "            for (s,h,p) in x_opt:\n",
    "                if x_opt[s,h,p]>EPS:\n",
    "                    self.result[self.height-h][s-1]=int(p)\n",
    "            self.result=self.result.astype(int)\n",
    "            # self.penalty=[]\n",
    "            # for k in self.O:\n",
    "            #     OO=k\n",
    "            #     a=0\n",
    "            #     for j in range(self.stack_num):\n",
    "            #         for i in range(1,self.height):\n",
    "            #             for i_ in range(i+1,self.height+1):\n",
    "            #                 if self.result[i-1][j]!=0 and self.result[i_-1][j]!=0:\n",
    "            #                     if OO.index(self.result[i-1][j])>OO.index(self.result[i_-1][j]):\n",
    "            #                         # print(j+1,height-i+1,O)\n",
    "            #                         a+=1\n",
    "            #                         # print(\"penalty!\")\n",
    "            #                             # print(i,i_,j+1,O)\n",
    "            #                         break\n",
    "            #     self.penalty.append(a)\n",
    "            # self.zk=[i for i, x in enumerate(self.penalty) if x-alpha_opt>0]\n",
    "            return self.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UB= 1000\n",
      "LB= 0\n",
      "UB-LB= 1000\n",
      "LEN= 0\n",
      "Set parameter LazyConstraints to value 1\n",
      "Gurobi Optimizer version 9.5.1 build v9.5.1rc2 (win64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 35 rows, 13098 columns and 1338 nonzeros\n",
      "Model fingerprint: 0x60c06333\n",
      "Variable types: 13002 continuous, 96 integer (96 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [4e-03, 1e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 3e+00]\n",
      "[] 0\n",
      "Presolve time: 0.00s\n",
      "Presolved: 35 rows, 13098 columns, 1338 nonzeros\n",
      "Variable types: 13002 continuous, 96 integer (96 binary)\n",
      "[] 0\n",
      "Found heuristic solution: objective 0.0000000\n",
      "\n",
      "Explored 0 nodes (0 simplex iterations) in 0.85 seconds (0.00 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 1: 0 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 0.000000000000e+00, best bound 0.000000000000e+00, gap 0.0000%\n",
      "\n",
      "User-callback calls 93, time in user-callback 0.80 sec\n",
      "[[0 0 0 0]\n",
      " [6 5 2 1]\n",
      " [6 5 3 1]\n",
      " [6 5 4 2]]\n",
      "UB= 2.292\n",
      "LB= 0\n",
      "UB-LB= 2.292\n",
      "LEN= 614\n",
      "Set parameter LazyConstraints to value 1\n",
      "Gurobi Optimizer version 9.5.1 build v9.5.1rc2 (win64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 89065 rows, 13098 columns and 717262 nonzeros\n",
      "Model fingerprint: 0xacd36364\n",
      "Variable types: 13002 continuous, 96 integer (96 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [4e-03, 1e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 3e+00]\n",
      "Presolve removed 360 rows and 0 columns\n",
      "Presolve time: 1.97s\n",
      "Presolved: 88705 rows, 13098 columns, 715822 nonzeros\n",
      "Variable types: 13002 continuous, 96 integer (96 binary)\n",
      "[0, 4, 5, 6, 9, 10, 12, 13, 14, 15, 16, 17, 18, 22, 23, 25, 26, 28, 30, 31, 32, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 47, 48, 49, 50, 51, 53, 59, 60, 63, 64, 66, 67, 68, 71, 72, 73, 75, 77, 78, 80, 81, 82, 84, 88, 89, 90, 91, 97, 98, 102, 106, 107, 108, 109, 112, 113, 114, 116, 117, 118, 121, 122, 123, 125, 126, 127, 129, 130, 132, 133, 136, 139, 141, 142, 143, 144, 148, 149, 150, 151, 152, 154, 156, 159, 160, 161, 162, 166, 167, 169, 170, 171, 172, 175, 178, 179, 180, 182, 185, 186, 187, 189, 190, 192, 194, 196, 198, 200, 202, 203, 204, 207, 210, 211, 212, 213, 218, 219, 220, 221, 222, 226, 227, 230, 231, 232, 233, 235, 238, 239, 240, 241, 242, 243, 244, 245, 246, 252, 253, 254, 256, 257, 258, 259, 260, 262, 266, 267, 268, 270, 271, 272, 274, 275, 276, 278, 281, 282, 283, 285, 286, 287, 289, 291, 292, 294, 295, 298, 299, 301, 303, 305, 307, 308, 309, 313, 314, 315, 316, 317, 318, 323, 325, 326, 327, 328, 329, 330, 331, 334, 336, 338, 339, 341, 342, 343, 344, 345, 349, 352, 354, 355, 357, 359, 361, 362, 363, 365, 368, 369, 370, 372, 373, 375, 377, 378, 379, 380, 381, 382, 385, 386, 387, 388, 390, 392, 393, 394, 397, 398, 402, 404, 405, 407, 408, 410, 411, 414, 415, 417, 420, 422, 423, 424, 426, 427, 429, 430, 431, 432, 433, 436, 440, 443, 444, 449, 450, 451, 452, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 470, 472, 473, 474, 475, 476, 478, 481, 484, 486, 487, 489, 490, 491, 493, 494, 496, 499, 500, 501, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 517, 518, 521, 522, 523, 526, 527, 528, 531, 532, 535, 537, 539, 540, 541, 543, 545, 547, 548, 550, 551, 554, 555, 556, 559, 562, 563, 564, 565, 568, 569, 570, 571, 572, 574, 576, 577, 578, 579, 580, 581, 587, 588, 592, 593, 594, 595, 596, 597, 599, 600, 601, 602, 604, 607, 609, 612, 613, 614, 616, 617, 618, 619, 622, 623, 624, 625, 626, 627, 629, 630, 632, 633, 636, 641, 642, 643, 647, 650, 651, 653, 655, 656, 659, 660, 661, 664, 667, 668, 673, 674, 675, 678, 679, 680, 681, 682, 684, 685, 686, 688, 694, 695, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 713, 714, 715, 717, 719, 722, 723, 724, 725, 727, 728, 730, 732, 733, 735, 736, 737, 739, 741, 742, 743, 744, 745, 747, 748, 750, 751, 752, 755, 758, 760, 761, 762, 763, 767, 768, 769, 773, 775, 776, 780, 781, 783, 786, 787, 789, 790, 791, 792, 793, 795, 800, 802, 803, 804, 807, 808, 809, 810, 816, 818, 819, 821, 822, 825, 827, 828, 829, 831, 832, 836, 838, 840, 842, 843, 845, 847, 848, 849, 850, 852, 853, 854, 856, 857, 858, 860, 862, 863, 865, 866, 867, 868, 869, 870, 871, 872, 873, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 892, 893, 894, 895, 896, 897, 899, 900, 901, 903, 904, 906, 908, 909, 910, 915, 916, 920, 924, 926, 928, 930, 932, 933, 934, 935, 936, 941, 942, 944, 945, 946, 948, 949, 950, 952, 956, 958, 962, 963, 964, 965, 966, 967, 968, 970, 971, 972, 973, 974, 976, 977, 978, 979, 984, 985, 989, 990, 991, 993, 994, 995, 997, 998] 614\n",
      "Found heuristic solution: objective 5.8720000\n",
      "\n",
      "Deterministic concurrent LP optimizer: primal and dual simplex (primal and dual model)\n",
      "Showing first log only...\n",
      "\n",
      "Root relaxation presolved: 88706 rows, 13098 columns, 723192 nonzeros\n",
      "\n",
      "\n",
      "Use crossover to convert LP symmetric solution to basic solution...\n",
      "\n",
      "Root crossover log...\n",
      "\n",
      "  Push phase complete: Pinf 0.0000000e+00, Dinf 2.4000000e-02      5s\n",
      "\n",
      "\n",
      "Root simplex log...\n",
      "\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "     279    8.0000000e-03   0.000000e+00   2.400000e-02      5s\n",
      "     285    8.0000000e-03   0.000000e+00   0.000000e+00      6s\n",
      "Concurrent spin time: 0.04s\n",
      "\n",
      "Solved with primal simplex (primal model)\n",
      "\n",
      "Root relaxation: objective 8.000000e-03, 285 iterations, 2.57 seconds (0.58 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0    0.00800    0   20    5.87200    0.00800   100%     -    9s\n",
      "     0     0    0.00800    0   33    5.87200    0.00800   100%     -   14s\n",
      "[0, 4, 5, 6, 9, 10, 12, 13, 14, 15, 16, 17, 18, 22, 23, 25, 26, 28, 30, 31, 32, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 47, 48, 49, 50, 51, 53, 59, 60, 63, 64, 66, 67, 68, 71, 72, 73, 75, 77, 78, 80, 81, 82, 84, 88, 89, 90, 91, 97, 98, 102, 106, 107, 108, 109, 112, 113, 114, 116, 117, 118, 121, 122, 123, 125, 126, 127, 129, 130, 132, 133, 136, 139, 141, 142, 143, 144, 148, 149, 150, 151, 152, 154, 156, 159, 160, 161, 162, 166, 167, 169, 170, 171, 172, 175, 178, 179, 180, 182, 185, 186, 187, 189, 190, 192, 194, 196, 198, 200, 202, 203, 204, 207, 210, 211, 212, 213, 218, 219, 220, 221, 222, 226, 227, 230, 231, 232, 233, 235, 238, 239, 240, 241, 242, 243, 244, 245, 246, 252, 253, 254, 256, 257, 258, 259, 260, 262, 266, 267, 268, 270, 271, 272, 274, 275, 276, 278, 281, 282, 283, 285, 286, 287, 289, 291, 292, 294, 295, 298, 299, 301, 303, 305, 307, 308, 309, 313, 314, 315, 316, 317, 318, 323, 325, 326, 327, 328, 329, 330, 331, 334, 336, 338, 339, 341, 342, 343, 344, 345, 349, 352, 354, 355, 357, 359, 361, 362, 363, 365, 368, 369, 370, 372, 373, 375, 377, 378, 379, 380, 381, 382, 385, 386, 387, 388, 390, 392, 393, 394, 397, 398, 402, 404, 405, 407, 408, 410, 411, 414, 415, 417, 420, 422, 423, 424, 426, 427, 429, 430, 431, 432, 433, 436, 440, 443, 444, 449, 450, 451, 452, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 470, 472, 473, 474, 475, 476, 478, 481, 484, 486, 487, 489, 490, 491, 493, 494, 496, 499, 500, 501, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 517, 518, 521, 522, 523, 526, 527, 528, 531, 532, 535, 537, 539, 540, 541, 543, 545, 547, 548, 550, 551, 554, 555, 556, 559, 562, 563, 564, 565, 568, 569, 570, 571, 572, 574, 576, 577, 578, 579, 580, 581, 587, 588, 592, 593, 594, 595, 596, 597, 599, 600, 601, 602, 604, 607, 609, 612, 613, 614, 616, 617, 618, 619, 622, 623, 624, 625, 626, 627, 629, 630, 632, 633, 636, 641, 642, 643, 647, 650, 651, 653, 655, 656, 659, 660, 661, 664, 667, 668, 673, 674, 675, 678, 679, 680, 681, 682, 684, 685, 686, 688, 694, 695, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 713, 714, 715, 717, 719, 722, 723, 724, 725, 727, 728, 730, 732, 733, 735, 736, 737, 739, 741, 742, 743, 744, 745, 747, 748, 750, 751, 752, 755, 758, 760, 761, 762, 763, 767, 768, 769, 773, 775, 776, 780, 781, 783, 786, 787, 789, 790, 791, 792, 793, 795, 800, 802, 803, 804, 807, 808, 809, 810, 816, 818, 819, 821, 822, 825, 827, 828, 829, 831, 832, 836, 838, 840, 842, 843, 845, 847, 848, 849, 850, 852, 853, 854, 856, 857, 858, 860, 862, 863, 865, 866, 867, 868, 869, 870, 871, 872, 873, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 892, 893, 894, 895, 896, 897, 899, 900, 901, 903, 904, 906, 908, 909, 910, 915, 916, 920, 924, 926, 928, 930, 932, 933, 934, 935, 936, 941, 942, 944, 945, 946, 948, 949, 950, 952, 956, 958, 962, 963, 964, 965, 966, 967, 968, 970, 971, 972, 973, 974, 976, 977, 978, 979, 984, 985, 989, 990, 991, 993, 994, 995, 997, 998] 614\n",
      "H    0     0                       5.1120000    0.00800   100%     -   18s\n",
      "     0     0    0.00800    0   33    5.11200    0.00800   100%     -   28s\n",
      "[0, 4, 5, 6, 9, 10, 12, 13, 14, 15, 16, 17, 18, 22, 23, 25, 26, 28, 30, 31, 32, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 47, 48, 49, 50, 51, 53, 59, 60, 63, 64, 66, 67, 68, 71, 72, 73, 75, 77, 78, 80, 81, 82, 84, 88, 89, 90, 91, 97, 98, 102, 106, 107, 108, 109, 112, 113, 114, 116, 117, 118, 121, 122, 123, 125, 126, 127, 129, 130, 132, 133, 136, 139, 141, 142, 143, 144, 148, 149, 150, 151, 152, 154, 156, 159, 160, 161, 162, 166, 167, 169, 170, 171, 172, 175, 178, 179, 180, 182, 185, 186, 187, 189, 190, 192, 194, 196, 198, 200, 202, 203, 204, 207, 210, 211, 212, 213, 218, 219, 220, 221, 222, 226, 227, 230, 231, 232, 233, 235, 238, 239, 240, 241, 242, 243, 244, 245, 246, 252, 253, 254, 256, 257, 258, 259, 260, 262, 266, 267, 268, 270, 271, 272, 274, 275, 276, 278, 281, 282, 283, 285, 286, 287, 289, 291, 292, 294, 295, 298, 299, 301, 303, 305, 307, 308, 309, 313, 314, 315, 316, 317, 318, 323, 325, 326, 327, 328, 329, 330, 331, 334, 336, 338, 339, 341, 342, 343, 344, 345, 349, 352, 354, 355, 357, 359, 361, 362, 363, 365, 368, 369, 370, 372, 373, 375, 377, 378, 379, 380, 381, 382, 385, 386, 387, 388, 390, 392, 393, 394, 397, 398, 402, 404, 405, 407, 408, 410, 411, 414, 415, 417, 420, 422, 423, 424, 426, 427, 429, 430, 431, 432, 433, 436, 440, 443, 444, 449, 450, 451, 452, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 470, 472, 473, 474, 475, 476, 478, 481, 484, 486, 487, 489, 490, 491, 493, 494, 496, 499, 500, 501, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 517, 518, 521, 522, 523, 526, 527, 528, 531, 532, 535, 537, 539, 540, 541, 543, 545, 547, 548, 550, 551, 554, 555, 556, 559, 562, 563, 564, 565, 568, 569, 570, 571, 572, 574, 576, 577, 578, 579, 580, 581, 587, 588, 592, 593, 594, 595, 596, 597, 599, 600, 601, 602, 604, 607, 609, 612, 613, 614, 616, 617, 618, 619, 622, 623, 624, 625, 626, 627, 629, 630, 632, 633, 636, 641, 642, 643, 647, 650, 651, 653, 655, 656, 659, 660, 661, 664, 667, 668, 673, 674, 675, 678, 679, 680, 681, 682, 684, 685, 686, 688, 694, 695, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 713, 714, 715, 717, 719, 722, 723, 724, 725, 727, 728, 730, 732, 733, 735, 736, 737, 739, 741, 742, 743, 744, 745, 747, 748, 750, 751, 752, 755, 758, 760, 761, 762, 763, 767, 768, 769, 773, 775, 776, 780, 781, 783, 786, 787, 789, 790, 791, 792, 793, 795, 800, 802, 803, 804, 807, 808, 809, 810, 816, 818, 819, 821, 822, 825, 827, 828, 829, 831, 832, 836, 838, 840, 842, 843, 845, 847, 848, 849, 850, 852, 853, 854, 856, 857, 858, 860, 862, 863, 865, 866, 867, 868, 869, 870, 871, 872, 873, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 892, 893, 894, 895, 896, 897, 899, 900, 901, 903, 904, 906, 908, 909, 910, 915, 916, 920, 924, 926, 928, 930, 932, 933, 934, 935, 936, 941, 942, 944, 945, 946, 948, 949, 950, 952, 956, 958, 962, 963, 964, 965, 966, 967, 968, 970, 971, 972, 973, 974, 976, 977, 978, 979, 984, 985, 989, 990, 991, 993, 994, 995, 997, 998] 614\n",
      "H    0     0                       0.8640000    0.00800  99.1%     -   29s\n",
      "     0     0    0.00800    0   33    0.86400    0.00800  99.1%     -   30s\n",
      "     0     0    0.00800    0   34    0.86400    0.00800  99.1%     -   35s\n",
      "     0     0    0.01047    0   40    0.86400    0.01047  98.8%     -   41s\n",
      "     0     0    0.01047    0   43    0.86400    0.01047  98.8%     -   43s\n",
      "     0     0    0.01072    0   36    0.86400    0.01072  98.8%     -   46s\n",
      "     0     0    0.01072    0   40    0.86400    0.01072  98.8%     -   48s\n",
      "     0     0    0.01072    0   34    0.86400    0.01072  98.8%     -   50s\n",
      "     0     0    0.01075    0   48    0.86400    0.01075  98.8%     -   52s\n",
      "     0     0    0.01357    0   38    0.86400    0.01357  98.4%     -   57s\n",
      "[0, 4, 5, 6, 9, 10, 12, 13, 14, 15, 16, 17, 18, 22, 23, 25, 26, 28, 30, 31, 32, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 47, 48, 49, 50, 51, 53, 59, 60, 63, 64, 66, 67, 68, 71, 72, 73, 75, 77, 78, 80, 81, 82, 84, 88, 89, 90, 91, 97, 98, 102, 106, 107, 108, 109, 112, 113, 114, 116, 117, 118, 121, 122, 123, 125, 126, 127, 129, 130, 132, 133, 136, 139, 141, 142, 143, 144, 148, 149, 150, 151, 152, 154, 156, 159, 160, 161, 162, 166, 167, 169, 170, 171, 172, 175, 178, 179, 180, 182, 185, 186, 187, 189, 190, 192, 194, 196, 198, 200, 202, 203, 204, 207, 210, 211, 212, 213, 218, 219, 220, 221, 222, 226, 227, 230, 231, 232, 233, 235, 238, 239, 240, 241, 242, 243, 244, 245, 246, 252, 253, 254, 256, 257, 258, 259, 260, 262, 266, 267, 268, 270, 271, 272, 274, 275, 276, 278, 281, 282, 283, 285, 286, 287, 289, 291, 292, 294, 295, 298, 299, 301, 303, 305, 307, 308, 309, 313, 314, 315, 316, 317, 318, 323, 325, 326, 327, 328, 329, 330, 331, 334, 336, 338, 339, 341, 342, 343, 344, 345, 349, 352, 354, 355, 357, 359, 361, 362, 363, 365, 368, 369, 370, 372, 373, 375, 377, 378, 379, 380, 381, 382, 385, 386, 387, 388, 390, 392, 393, 394, 397, 398, 402, 404, 405, 407, 408, 410, 411, 414, 415, 417, 420, 422, 423, 424, 426, 427, 429, 430, 431, 432, 433, 436, 440, 443, 444, 449, 450, 451, 452, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 470, 472, 473, 474, 475, 476, 478, 481, 484, 486, 487, 489, 490, 491, 493, 494, 496, 499, 500, 501, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 517, 518, 521, 522, 523, 526, 527, 528, 531, 532, 535, 537, 539, 540, 541, 543, 545, 547, 548, 550, 551, 554, 555, 556, 559, 562, 563, 564, 565, 568, 569, 570, 571, 572, 574, 576, 577, 578, 579, 580, 581, 587, 588, 592, 593, 594, 595, 596, 597, 599, 600, 601, 602, 604, 607, 609, 612, 613, 614, 616, 617, 618, 619, 622, 623, 624, 625, 626, 627, 629, 630, 632, 633, 636, 641, 642, 643, 647, 650, 651, 653, 655, 656, 659, 660, 661, 664, 667, 668, 673, 674, 675, 678, 679, 680, 681, 682, 684, 685, 686, 688, 694, 695, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 713, 714, 715, 717, 719, 722, 723, 724, 725, 727, 728, 730, 732, 733, 735, 736, 737, 739, 741, 742, 743, 744, 745, 747, 748, 750, 751, 752, 755, 758, 760, 761, 762, 763, 767, 768, 769, 773, 775, 776, 780, 781, 783, 786, 787, 789, 790, 791, 792, 793, 795, 800, 802, 803, 804, 807, 808, 809, 810, 816, 818, 819, 821, 822, 825, 827, 828, 829, 831, 832, 836, 838, 840, 842, 843, 845, 847, 848, 849, 850, 852, 853, 854, 856, 857, 858, 860, 862, 863, 865, 866, 867, 868, 869, 870, 871, 872, 873, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 892, 893, 894, 895, 896, 897, 899, 900, 901, 903, 904, 906, 908, 909, 910, 915, 916, 920, 924, 926, 928, 930, 932, 933, 934, 935, 936, 941, 942, 944, 945, 946, 948, 949, 950, 952, 956, 958, 962, 963, 964, 965, 966, 967, 968, 970, 971, 972, 973, 974, 976, 977, 978, 979, 984, 985, 989, 990, 991, 993, 994, 995, 997, 998] 614\n",
      "H    0     0                       0.6840000    0.01357  98.0%     -   59s\n",
      "     0     0    0.01357    0   45    0.68400    0.01357  98.0%     -   59s\n",
      "     0     0    0.01357    0   45    0.68400    0.01357  98.0%     -   62s\n",
      "     0     0    0.01357    0   49    0.68400    0.01357  98.0%     -   64s\n",
      "     0     0    0.01357    0   37    0.68400    0.01357  98.0%     -   68s\n",
      "     0     0    0.01357    0   44    0.68400    0.01357  98.0%     -   68s\n",
      "     0     0    0.01373    0   36    0.68400    0.01373  98.0%     -   72s\n",
      "     0     0    0.01373    0   46    0.68400    0.01373  98.0%     -   74s\n",
      "     0     0    0.01373    0   46    0.68400    0.01373  98.0%     -   76s\n",
      "     0     0    0.01386    0   35    0.68400    0.01386  98.0%     -   85s\n",
      "     0     0    0.01386    0   37    0.68400    0.01386  98.0%     -   87s\n",
      "     0     0    0.01386    0   49    0.68400    0.01386  98.0%     -   91s\n",
      "     0     0    0.01386    0   49    0.68400    0.01386  98.0%     -   94s\n",
      "     0     0    0.01386    0   40    0.68400    0.01386  98.0%     -  101s\n",
      "     0     0    0.01386    0   42    0.68400    0.01386  98.0%     -  102s\n",
      "     0     0    0.01387    0   38    0.68400    0.01387  98.0%     -  108s\n",
      "     0     0    0.01387    0   42    0.68400    0.01387  98.0%     -  110s\n",
      "     0     0    0.01387    0   46    0.68400    0.01387  98.0%     -  113s\n",
      "     0     0    0.01387    0   43    0.68400    0.01387  98.0%     -  118s\n",
      "     0     2    0.01387    0   32    0.68400    0.01387  98.0%     -  124s\n",
      "     1     4    0.01387    1   32    0.68400    0.01387  98.0%  26.0  127s\n",
      "     3     8    0.01387    2   33    0.68400    0.01387  98.0%   417  131s\n",
      "    15    20    0.02965    5   53    0.68400    0.01387  98.0%   387  136s\n",
      "[0, 4, 5, 6, 9, 10, 12, 13, 14, 15, 16, 17, 18, 22, 23, 25, 26, 28, 30, 31, 32, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 47, 48, 49, 50, 51, 53, 59, 60, 63, 64, 66, 67, 68, 71, 72, 73, 75, 77, 78, 80, 81, 82, 84, 88, 89, 90, 91, 97, 98, 102, 106, 107, 108, 109, 112, 113, 114, 116, 117, 118, 121, 122, 123, 125, 126, 127, 129, 130, 132, 133, 136, 139, 141, 142, 143, 144, 148, 149, 150, 151, 152, 154, 156, 159, 160, 161, 162, 166, 167, 169, 170, 171, 172, 175, 178, 179, 180, 182, 185, 186, 187, 189, 190, 192, 194, 196, 198, 200, 202, 203, 204, 207, 210, 211, 212, 213, 218, 219, 220, 221, 222, 226, 227, 230, 231, 232, 233, 235, 238, 239, 240, 241, 242, 243, 244, 245, 246, 252, 253, 254, 256, 257, 258, 259, 260, 262, 266, 267, 268, 270, 271, 272, 274, 275, 276, 278, 281, 282, 283, 285, 286, 287, 289, 291, 292, 294, 295, 298, 299, 301, 303, 305, 307, 308, 309, 313, 314, 315, 316, 317, 318, 323, 325, 326, 327, 328, 329, 330, 331, 334, 336, 338, 339, 341, 342, 343, 344, 345, 349, 352, 354, 355, 357, 359, 361, 362, 363, 365, 368, 369, 370, 372, 373, 375, 377, 378, 379, 380, 381, 382, 385, 386, 387, 388, 390, 392, 393, 394, 397, 398, 402, 404, 405, 407, 408, 410, 411, 414, 415, 417, 420, 422, 423, 424, 426, 427, 429, 430, 431, 432, 433, 436, 440, 443, 444, 449, 450, 451, 452, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 470, 472, 473, 474, 475, 476, 478, 481, 484, 486, 487, 489, 490, 491, 493, 494, 496, 499, 500, 501, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 517, 518, 521, 522, 523, 526, 527, 528, 531, 532, 535, 537, 539, 540, 541, 543, 545, 547, 548, 550, 551, 554, 555, 556, 559, 562, 563, 564, 565, 568, 569, 570, 571, 572, 574, 576, 577, 578, 579, 580, 581, 587, 588, 592, 593, 594, 595, 596, 597, 599, 600, 601, 602, 604, 607, 609, 612, 613, 614, 616, 617, 618, 619, 622, 623, 624, 625, 626, 627, 629, 630, 632, 633, 636, 641, 642, 643, 647, 650, 651, 653, 655, 656, 659, 660, 661, 664, 667, 668, 673, 674, 675, 678, 679, 680, 681, 682, 684, 685, 686, 688, 694, 695, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 713, 714, 715, 717, 719, 722, 723, 724, 725, 727, 728, 730, 732, 733, 735, 736, 737, 739, 741, 742, 743, 744, 745, 747, 748, 750, 751, 752, 755, 758, 760, 761, 762, 763, 767, 768, 769, 773, 775, 776, 780, 781, 783, 786, 787, 789, 790, 791, 792, 793, 795, 800, 802, 803, 804, 807, 808, 809, 810, 816, 818, 819, 821, 822, 825, 827, 828, 829, 831, 832, 836, 838, 840, 842, 843, 845, 847, 848, 849, 850, 852, 853, 854, 856, 857, 858, 860, 862, 863, 865, 866, 867, 868, 869, 870, 871, 872, 873, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 892, 893, 894, 895, 896, 897, 899, 900, 901, 903, 904, 906, 908, 909, 910, 915, 916, 920, 924, 926, 928, 930, 932, 933, 934, 935, 936, 941, 942, 944, 945, 946, 948, 949, 950, 952, 956, 958, 962, 963, 964, 965, 966, 967, 968, 970, 971, 972, 973, 974, 976, 977, 978, 979, 984, 985, 989, 990, 991, 993, 994, 995, 997, 998] 614\n",
      "*   22    22               5       0.1240000    0.01387  88.8%   344  138s\n",
      "    23    21    0.03667    6   23    0.12400    0.01387  88.8%   339  140s\n",
      "    49    24     cutoff    9         0.12400    0.01387  88.8%   199  145s\n",
      "[0, 4, 5, 6, 9, 10, 12, 13, 14, 15, 16, 17, 18, 22, 23, 25, 26, 28, 30, 31, 32, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 47, 48, 49, 50, 51, 53, 59, 60, 63, 64, 66, 67, 68, 71, 72, 73, 75, 77, 78, 80, 81, 82, 84, 88, 89, 90, 91, 97, 98, 102, 106, 107, 108, 109, 112, 113, 114, 116, 117, 118, 121, 122, 123, 125, 126, 127, 129, 130, 132, 133, 136, 139, 141, 142, 143, 144, 148, 149, 150, 151, 152, 154, 156, 159, 160, 161, 162, 166, 167, 169, 170, 171, 172, 175, 178, 179, 180, 182, 185, 186, 187, 189, 190, 192, 194, 196, 198, 200, 202, 203, 204, 207, 210, 211, 212, 213, 218, 219, 220, 221, 222, 226, 227, 230, 231, 232, 233, 235, 238, 239, 240, 241, 242, 243, 244, 245, 246, 252, 253, 254, 256, 257, 258, 259, 260, 262, 266, 267, 268, 270, 271, 272, 274, 275, 276, 278, 281, 282, 283, 285, 286, 287, 289, 291, 292, 294, 295, 298, 299, 301, 303, 305, 307, 308, 309, 313, 314, 315, 316, 317, 318, 323, 325, 326, 327, 328, 329, 330, 331, 334, 336, 338, 339, 341, 342, 343, 344, 345, 349, 352, 354, 355, 357, 359, 361, 362, 363, 365, 368, 369, 370, 372, 373, 375, 377, 378, 379, 380, 381, 382, 385, 386, 387, 388, 390, 392, 393, 394, 397, 398, 402, 404, 405, 407, 408, 410, 411, 414, 415, 417, 420, 422, 423, 424, 426, 427, 429, 430, 431, 432, 433, 436, 440, 443, 444, 449, 450, 451, 452, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 470, 472, 473, 474, 475, 476, 478, 481, 484, 486, 487, 489, 490, 491, 493, 494, 496, 499, 500, 501, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 517, 518, 521, 522, 523, 526, 527, 528, 531, 532, 535, 537, 539, 540, 541, 543, 545, 547, 548, 550, 551, 554, 555, 556, 559, 562, 563, 564, 565, 568, 569, 570, 571, 572, 574, 576, 577, 578, 579, 580, 581, 587, 588, 592, 593, 594, 595, 596, 597, 599, 600, 601, 602, 604, 607, 609, 612, 613, 614, 616, 617, 618, 619, 622, 623, 624, 625, 626, 627, 629, 630, 632, 633, 636, 641, 642, 643, 647, 650, 651, 653, 655, 656, 659, 660, 661, 664, 667, 668, 673, 674, 675, 678, 679, 680, 681, 682, 684, 685, 686, 688, 694, 695, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 713, 714, 715, 717, 719, 722, 723, 724, 725, 727, 728, 730, 732, 733, 735, 736, 737, 739, 741, 742, 743, 744, 745, 747, 748, 750, 751, 752, 755, 758, 760, 761, 762, 763, 767, 768, 769, 773, 775, 776, 780, 781, 783, 786, 787, 789, 790, 791, 792, 793, 795, 800, 802, 803, 804, 807, 808, 809, 810, 816, 818, 819, 821, 822, 825, 827, 828, 829, 831, 832, 836, 838, 840, 842, 843, 845, 847, 848, 849, 850, 852, 853, 854, 856, 857, 858, 860, 862, 863, 865, 866, 867, 868, 869, 870, 871, 872, 873, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 892, 893, 894, 895, 896, 897, 899, 900, 901, 903, 904, 906, 908, 909, 910, 915, 916, 920, 924, 926, 928, 930, 932, 933, 934, 935, 936, 941, 942, 944, 945, 946, 948, 949, 950, 952, 956, 958, 962, 963, 964, 965, 966, 967, 968, 970, 971, 972, 973, 974, 976, 977, 978, 979, 984, 985, 989, 990, 991, 993, 994, 995, 997, 998] 614\n",
      "*   70    23              12       0.1160000    0.01387  88.0%   174  152s\n",
      "    89    25    0.08533   12   13    0.11600    0.01387  88.0%   157  156s\n",
      "   106    24     cutoff   14         0.11600    0.01387  88.0%   150  162s\n",
      "   115    21    0.01387    4   38    0.11600    0.01387  88.0%   154  165s\n",
      "   124    24    0.01946    5   52    0.11600    0.01432  87.7%   168  171s\n",
      "   129    22    0.08267    5   34    0.11600    0.01946  83.2%   180  175s\n",
      "   151    24    0.08550    5   27    0.11600    0.02000  82.8%   173  182s\n",
      "   173    27    0.03267    8   38    0.11600    0.02000  82.8%   168  187s\n",
      "   182    29    0.03917    8   35    0.11600    0.02000  82.8%   171  191s\n",
      "   207    29    0.02400    7   22    0.11600    0.02000  82.8%   162  198s\n",
      "   212    29    0.05600    9    8    0.11600    0.02000  82.8%   167  203s\n",
      "   222    31    0.06000    9    8    0.11600    0.02000  82.8%   170  206s\n",
      "   245    37    0.03667    7   19    0.11600    0.02400  79.3%   167  212s\n",
      "   254    38    0.03683    8   15    0.11600    0.02400  79.3%   169  215s\n",
      "   282    43     cutoff   13         0.11600    0.02400  79.3%   172  223s\n",
      "   300    39     cutoff   14         0.11600    0.02400  79.3%   170  231s\n",
      "   312    41    0.04800    9   20    0.11600    0.02400  79.3%   173  235s\n",
      "   326    40    0.08000   10    6    0.11600    0.02400  79.3%   175  240s\n",
      "   345    38    0.10900   11   10    0.11600    0.02400  79.3%   173  246s\n",
      "   363    34    0.07556   13   17    0.11600    0.02400  79.3%   171  250s\n",
      "   391    33     cutoff   14         0.11600    0.04655  59.9%   166  255s\n",
      "   423    37     cutoff   12         0.11600    0.04952  57.3%   170  264s\n",
      "   446    33    0.07186   12   24    0.11600    0.04952  57.3%   167  274s\n",
      "   477    33    0.09567   14   38    0.11600    0.04952  57.3%   163  280s\n",
      "   501    29     cutoff   16         0.11600    0.05600  51.7%   160  287s\n",
      "   527    20     cutoff   11         0.11600    0.05600  51.7%   159  292s\n",
      "   552     7     cutoff   17         0.11600    0.06000  48.3%   159  297s\n",
      "   581     0     cutoff   17         0.11600    0.06867  40.8%   156  300s\n",
      "\n",
      "Cutting planes:\n",
      "  Gomory: 3\n",
      "  MIR: 5\n",
      "  Mod-K: 1\n",
      "  RLT: 54\n",
      "  Relax-and-lift: 112\n",
      "  Lazy constraints: 1\n",
      "\n",
      "Explored 598 nodes (96444 simplex iterations) in 300.31 seconds (272.32 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 6: 0.116 0.124 0.684 ... 5.872\n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 1.160000000000e-01, best bound 1.160000000000e-01, gap 0.0000%\n",
      "\n",
      "User-callback calls 20007, time in user-callback 3.03 sec\n",
      "[[0 1 0 3]\n",
      " [0 5 0 6]\n",
      " [2 5 1 6]\n",
      " [2 5 4 6]]\n",
      "UB= 0.152\n",
      "LB= 0\n",
      "UB-LB= 0.152\n",
      "LEN= 651\n",
      "Set parameter LazyConstraints to value 1\n",
      "Gurobi Optimizer version 9.5.1 build v9.5.1rc2 (win64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 94430 rows, 13098 columns and 760404 nonzeros\n",
      "Model fingerprint: 0x5f05c1c0\n",
      "Variable types: 13002 continuous, 96 integer (96 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [4e-03, 1e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 3e+00]\n",
      "Presolve removed 492 rows and 0 columns\n",
      "Presolve time: 1.90s\n",
      "Presolved: 93938 rows, 13098 columns, 758436 nonzeros\n",
      "Variable types: 13002 continuous, 96 integer (96 binary)\n",
      "[82, 88, 110, 112, 117, 147, 155, 160, 171, 204, 205, 224, 236, 239, 240, 248, 297, 322, 370, 399, 424, 454, 490, 541, 548, 650, 665, 709, 741, 743, 797, 867, 906, 925, 950, 963, 982] 37\n",
      "Found heuristic solution: objective 5.9200000\n",
      "\n",
      "Deterministic concurrent LP optimizer: primal and dual simplex (primal and dual model)\n",
      "Showing first log only...\n",
      "\n",
      "Root relaxation presolved: 93939 rows, 13098 columns, 758882 nonzeros\n",
      "\n",
      "\n",
      "Use crossover to convert LP symmetric solution to basic solution...\n",
      "\n",
      "Root crossover log...\n",
      "\n",
      "  Push phase complete: Pinf 0.0000000e+00, Dinf 1.3860441e-15      5s\n",
      "\n",
      "\n",
      "Root simplex log...\n",
      "\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "     377    2.6000000e-02   0.000000e+00   0.000000e+00      6s\n",
      "Concurrent spin time: 0.01s\n",
      "\n",
      "Solved with primal simplex (primal model)\n",
      "\n",
      "Root relaxation: objective 2.600000e-02, 377 iterations, 2.64 seconds (0.61 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0    0.02600    0   23    5.92000    0.02600   100%     -    7s\n",
      "[82, 88, 110, 112, 117, 147, 155, 160, 171, 204, 205, 224, 236, 239, 240, 248, 297, 322, 370, 399, 424, 454, 490, 541, 548, 650, 665, 709, 741, 743, 797, 867, 906, 925, 950, 963, 982] 37\n",
      "H    0     0                       0.5360000    0.02600  95.1%     -    8s\n",
      "     0     0    0.02600    0   33    0.53600    0.02600  95.1%     -   13s\n",
      "     0     0    0.02600    0   24    0.53600    0.02600  95.1%     -   18s\n",
      "     0     0    0.02600    0   29    0.53600    0.02600  95.1%     -   19s\n",
      "     0     0    0.02600    0   26    0.53600    0.02600  95.1%     -   27s\n",
      "     0     0    0.03467    0   32    0.53600    0.03467  93.5%     -   28s\n",
      "     0     0    0.03489    0   40    0.53600    0.03489  93.5%     -   34s\n",
      "     0     0    0.03490    0   45    0.53600    0.03490  93.5%     -   35s\n",
      "     0     0    0.03898    0   46    0.53600    0.03898  92.7%     -   38s\n",
      "     0     0    0.03898    0   51    0.53600    0.03898  92.7%     -   39s\n",
      "     0     0    0.03898    0   51    0.53600    0.03898  92.7%     -   43s\n",
      "     0     0    0.03898    0   42    0.53600    0.03898  92.7%     -   48s\n",
      "     0     0    0.03898    0   44    0.53600    0.03898  92.7%     -   49s\n",
      "     0     0    0.03914    0   41    0.53600    0.03914  92.7%     -   53s\n",
      "     0     0    0.03914    0   49    0.53600    0.03914  92.7%     -   53s\n",
      "     0     0    0.03914    0   50    0.53600    0.03914  92.7%     -   56s\n",
      "     0     0    0.03914    0   54    0.53600    0.03914  92.7%     -   57s\n",
      "     0     0    0.03929    0   47    0.53600    0.03929  92.7%     -   61s\n",
      "     0     0    0.03929    0   47    0.53600    0.03929  92.7%     -   62s\n",
      "     0     0    0.03929    0   45    0.53600    0.03929  92.7%     -   65s\n",
      "     0     0    0.03929    0   50    0.53600    0.03929  92.7%     -   66s\n",
      "     0     0    0.03933    0   36    0.53600    0.03933  92.7%     -   71s\n",
      "     0     0    0.03933    0   44    0.53600    0.03933  92.7%     -   73s\n",
      "     0     0    0.03933    0   37    0.53600    0.03933  92.7%     -   80s\n",
      "     0     2    0.03933    0   37    0.53600    0.03933  92.7%     -   90s\n",
      "     3     8    0.03933    2   41    0.53600    0.03933  92.7%   340   98s\n",
      "    11    16    0.04896    4   61    0.53600    0.03933  92.7%   355  105s\n",
      "    23    27    0.04912    7   54    0.53600    0.03933  92.7%   272  111s\n",
      "    33    31    0.06400    9   22    0.53600    0.03933  92.7%   265  116s\n",
      "[82, 88, 110, 112, 117, 147, 155, 160, 171, 204, 205, 224, 236, 239, 240, 248, 297, 322, 370, 399, 424, 454, 490, 541, 548, 650, 665, 709, 741, 743, 797, 867, 906, 925, 950, 963, 982] 37\n",
      "*   36    31               7       0.4840000    0.03933  91.9%   271  117s\n",
      "    38    33    0.05689    9   51    0.48400    0.03933  91.9%   293  120s\n",
      "    48    40    0.07900   12   16    0.48400    0.03933  91.9%   290  126s\n",
      "    53    44    0.09945   13   39    0.48400    0.03933  91.9%   298  131s\n",
      "    59    48    0.14450   13   14    0.48400    0.03933  91.9%   290  135s\n",
      "    63    52    0.16400   14    6    0.48400    0.03933  91.9%   307  140s\n",
      "[82, 88, 110, 112, 117, 147, 155, 160, 171, 204, 205, 224, 236, 239, 240, 248, 297, 322, 370, 399, 424, 454, 490, 541, 548, 650, 665, 709, 741, 743, 797, 867, 906, 925, 950, 963, 982] 37\n",
      "*   69    52              17       0.3600000    0.03933  89.1%   309  141s\n",
      "    71    53    0.24822   15   19    0.36000    0.03933  89.1%   315  147s\n",
      "    78    59    0.28400   16    4    0.36000    0.03933  89.1%   325  150s\n",
      "[82, 88, 110, 112, 117, 147, 155, 160, 171, 204, 205, 224, 236, 239, 240, 248, 297, 322, 370, 399, 424, 454, 490, 541, 548, 650, 665, 709, 741, 743, 797, 867, 906, 925, 950, 963, 982] 37\n",
      "*   80    59              18       0.2840000    0.03933  86.2%   317  151s\n",
      "    88    60     cutoff   17         0.28400    0.04896  82.8%   316  155s\n",
      "   106    58    0.17153    7   42    0.28400    0.04896  82.8%   317  164s\n",
      "   119    64    0.17173    8   41    0.28400    0.04896  82.8%   296  168s\n",
      "   129    64     cutoff   12         0.28400    0.04896  82.8%   286  171s\n",
      "   143    64     cutoff   11         0.28400    0.04938  82.6%   267  178s\n",
      "   155    61    0.07833    7   21    0.28400    0.04938  82.6%   269  182s\n",
      "   164    64    0.08000    8   16    0.28400    0.04938  82.6%   262  186s\n",
      "   179    66    0.14600    9   14    0.28400    0.04938  82.6%   251  193s\n",
      "   197    64    0.25533   11   12    0.28400    0.04938  82.6%   250  196s\n",
      "   207    65     cutoff   12         0.28400    0.06400  77.5%   247  202s\n",
      "   216    65     cutoff    8         0.28400    0.06400  77.5%   253  207s\n",
      "[82, 88, 110, 112, 117, 147, 155, 160, 171, 204, 205, 224, 236, 239, 240, 248, 297, 322, 370, 399, 424, 454, 490, 541, 548, 650, 665, 709, 741, 743, 797, 867, 906, 925, 950, 963, 982] 37\n",
      "*  222    65              16       0.2680000    0.06400  76.1%   249  207s\n",
      "   232    66     cutoff   10         0.26800    0.06400  76.1%   248  212s\n",
      "   245    67     cutoff   16         0.26800    0.06400  76.1%   246  218s\n",
      "   258    72    0.10200    9   21    0.26800    0.07056  73.7%   246  225s\n",
      "   269    69    0.25300    9   10    0.26800    0.07056  73.7%   251  233s\n",
      "   294    68    0.26000   11    8    0.26800    0.07056  73.7%   242  240s\n",
      "   305    72     cutoff   12         0.26800    0.07056  73.7%   241  248s\n",
      "   315    73     cutoff   11         0.26800    0.08615  67.9%   241  254s\n",
      "   330    74     cutoff   12         0.26800    0.08615  67.9%   240  260s\n",
      "   343    73    0.23022   13   20    0.26800    0.08615  67.9%   245  269s\n",
      "   363    75     cutoff   14         0.26800    0.09800  63.4%   244  295s\n",
      "   381    75    0.12400   16    8    0.26800    0.09800  63.4%   275  306s\n",
      "   399    76    0.12400   16    8    0.26800    0.09800  63.4%   279  315s\n",
      "   422    76     cutoff   19         0.26800    0.09946  62.9%   277  340s\n",
      "   450    75     cutoff   14         0.26800    0.10190  62.0%   291  349s\n",
      "   481    74     cutoff   12         0.26800    0.10202  61.9%   291  361s\n",
      "   500    76    0.26000   15   12    0.26800    0.10510  60.8%   300  371s\n",
      "   538    75     cutoff   21         0.26800    0.12400  53.7%   289  383s\n",
      "   561    78    0.19600   19    8    0.26800    0.12400  53.7%   288  392s\n",
      "   592    72    0.19600   22   10    0.26800    0.12400  53.7%   279  402s\n",
      "   618    71     cutoff   24         0.26800    0.12400  53.7%   280  416s\n",
      "   649    69     cutoff   19         0.26800    0.12891  51.9%   276  447s\n",
      "   679    61     cutoff   20         0.26800    0.12956  51.7%   294  505s\n",
      "   703    64     cutoff   20         0.26800    0.14563  45.7%   301  521s\n",
      "   742    51     cutoff   24         0.26800    0.15877  40.8%   296  538s\n",
      "   780    43     cutoff    7         0.26800    0.16233  39.4%   292  556s\n",
      "   843    40     cutoff   25         0.26800    0.17160  36.0%   282  573s\n",
      "   895    28    0.24000    9   26    0.26800    0.19600  26.9%   275  591s\n",
      "   944    23    0.20133   22   27    0.26800    0.20133  24.9%   271  606s\n",
      "  1005     0    0.20267   24   23    0.26800    0.20267  24.4%   261  617s\n",
      "\n",
      "Cutting planes:\n",
      "  Gomory: 3\n",
      "  MIR: 2\n",
      "  Mod-K: 5\n",
      "  RLT: 41\n",
      "  Relax-and-lift: 32\n",
      "  Lazy constraints: 1\n",
      "\n",
      "Explored 1055 nodes (271303 simplex iterations) in 617.59 seconds (682.31 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 6: 0.268 0.284 0.36 ... 5.92\n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 2.680000000000e-01, best bound 2.680000000000e-01, gap 0.0000%\n",
      "\n",
      "User-callback calls 20488, time in user-callback 2.70 sec\n",
      "[[3 1 0 0]\n",
      " [6 5 0 0]\n",
      " [6 5 1 2]\n",
      " [6 5 4 2]]\n",
      "UB= 0.14\n",
      "LB= 0\n",
      "UB-LB= 0.14\n",
      "LEN= 684\n",
      "Set parameter LazyConstraints to value 1\n",
      "Gurobi Optimizer version 9.5.1 build v9.5.1rc2 (win64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 99215 rows, 13098 columns and 798882 nonzeros\n",
      "Model fingerprint: 0xd5c75338\n",
      "Variable types: 13002 continuous, 96 integer (96 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [4e-03, 1e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 3e+00]\n",
      "Presolve removed 540 rows and 0 columns\n",
      "Presolve time: 2.45s\n",
      "Presolved: 98675 rows, 13098 columns, 796722 nonzeros\n",
      "Variable types: 13002 continuous, 96 integer (96 binary)\n",
      "[0, 17, 38, 51, 70, 71, 86, 121, 177, 187, 234, 276, 282, 283, 284, 290, 434, 477, 497, 681, 693, 707, 733, 748, 752, 783, 793, 806, 844, 880, 889, 932, 989] 33\n",
      "Found heuristic solution: objective 5.9600000\n",
      "\n",
      "Deterministic concurrent LP optimizer: primal and dual simplex (primal and dual model)\n",
      "Showing first log only...\n",
      "\n",
      "Root relaxation presolved: 98676 rows, 13098 columns, 797120 nonzeros\n",
      "\n",
      "\n",
      "Root simplex log...\n",
      "\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "     123    0.0000000e+00   1.000000e+00   3.125000e+07      5s\n",
      "     268    3.2000000e-02   0.000000e+00   0.000000e+00      5s\n",
      "\n",
      "Use crossover to convert LP symmetric solution to basic solution...\n",
      "\n",
      "Root crossover log...\n",
      "\n",
      "      10 DPushes remaining with DInf 0.0000000e+00                 5s\n",
      "       0 DPushes remaining with DInf 0.0000000e+00                 5s\n",
      "\n",
      "      63 PPushes remaining with PInf 0.0000000e+00                 5s\n",
      "       0 PPushes remaining with PInf 0.0000000e+00                 6s\n",
      "\n",
      "  Push phase complete: Pinf 0.0000000e+00, Dinf 3.8163916e-16      6s\n",
      "\n",
      "\n",
      "Root simplex log...\n",
      "\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "     344    3.2000000e-02   0.000000e+00   0.000000e+00      6s\n",
      "Concurrent spin time: 0.07s\n",
      "\n",
      "Solved with primal simplex (primal model)\n",
      "\n",
      "Root relaxation: objective 3.200000e-02, 344 iterations, 2.60 seconds (0.55 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0    0.03200    0   25    5.96000    0.03200  99.5%     -    9s\n",
      "[0, 17, 38, 51, 70, 71, 86, 121, 177, 187, 234, 276, 282, 283, 284, 290, 434, 477, 497, 681, 693, 707, 733, 748, 752, 783, 793, 806, 844, 880, 889, 932, 989] 33\n",
      "H    0     0                       3.0000000    0.03200  98.9%     -   10s\n",
      "[0, 17, 38, 51, 70, 71, 86, 121, 177, 187, 234, 276, 282, 283, 284, 290, 434, 477, 497, 681, 693, 707, 733, 748, 752, 783, 793, 806, 844, 880, 889, 932, 989] 33\n",
      "H    0     0                       0.6440000    0.03200  95.0%     -   11s\n",
      "     0     0    0.03200    0   39    0.64400    0.03200  95.0%     -   15s\n",
      "     0     0    0.03200    0   27    0.64400    0.03200  95.0%     -   23s\n",
      "     0     0    0.03472    0   46    0.64400    0.03472  94.6%     -   37s\n",
      "     0     0    0.03511    0   39    0.64400    0.03511  94.5%     -   44s\n",
      "     0     0    0.03511    0   41    0.64400    0.03511  94.5%     -   46s\n",
      "     0     0    0.03550    0   39    0.64400    0.03550  94.5%     -   51s\n",
      "     0     0    0.03550    0   43    0.64400    0.03550  94.5%     -   57s\n",
      "     0     0    0.03550    0   44    0.64400    0.03550  94.5%     -   58s\n",
      "     0     0    0.03550    0   40    0.64400    0.03550  94.5%     -   65s\n",
      "     0     0    0.03550    0   43    0.64400    0.03550  94.5%     -   67s\n",
      "     0     0    0.03553    0   46    0.64400    0.03553  94.5%     -   73s\n",
      "     0     0    0.03555    0   52    0.64400    0.03555  94.5%     -   76s\n",
      "     0     0    0.03555    0   52    0.64400    0.03555  94.5%     -   82s\n",
      "     0     0    0.03555    0   51    0.64400    0.03555  94.5%     -   90s\n",
      "     0     0    0.03559    0   50    0.64400    0.03559  94.5%     -   98s\n",
      "     0     0    0.03559    0   50    0.64400    0.03559  94.5%     -  106s\n",
      "     0     2    0.03559    0   47    0.64400    0.03559  94.5%     -  121s\n",
      "     3     8    0.03672    2   44    0.64400    0.03672  94.3%  65.7  127s\n",
      "     7    12    0.03821    3   53    0.64400    0.03692  94.3%   505  132s\n",
      "    11    16    0.04207    4   51    0.64400    0.03692  94.3%   395  141s\n",
      "    15    20    0.19983    4   40    0.64400    0.03692  94.3%   590  147s\n",
      "    23    30    0.05047    6   56    0.64400    0.03692  94.3%   524  151s\n",
      "    29    34    0.05331    8   55    0.64400    0.03692  94.3%   496  157s\n",
      "    33    37    0.05942    9   56    0.64400    0.03692  94.3%   513  161s\n",
      "[0, 17, 38, 51, 70, 71, 86, 121, 177, 187, 234, 276, 282, 283, 284, 290, 434, 477, 497, 681, 693, 707, 733, 748, 752, 783, 793, 806, 844, 880, 889, 932, 989] 33\n",
      "*   34    37               7       0.4240000    0.03692  91.3%   512  162s\n",
      "    38    38    0.08000    9   22    0.42400    0.03692  91.3%   501  167s\n",
      "    43    43    0.08000   10   22    0.42400    0.03692  91.3%   470  170s\n",
      "    56    43    0.08000   11   22    0.42400    0.03692  91.3%   397  180s\n",
      "    62    43    0.08000   11   22    0.42400    0.03692  91.3%   398  186s\n",
      "    83    51    0.22000   14    6    0.42400    0.03692  91.3%   347  196s\n",
      "    93    58    0.29600   15   14    0.42400    0.03692  91.3%   352  200s\n",
      "   102    60    0.36960   16   20    0.42400    0.03692  91.3%   368  207s\n",
      "   108    56     cutoff   16         0.42400    0.03692  91.3%   371  211s\n",
      "   116    57     cutoff   15         0.42400    0.05043  88.1%   363  218s\n",
      "   133    59    0.38033    4   23    0.42400    0.05043  88.1%   341  223s\n",
      "   145    54    0.39300    5   17    0.42400    0.05043  88.1%   328  229s\n",
      "   164    55     cutoff    7         0.42400    0.05053  88.1%   304  236s\n",
      "   179    65    0.27100   11   28    0.42400    0.05053  88.1%   314  246s\n",
      "[0, 17, 38, 51, 70, 71, 86, 121, 177, 187, 234, 276, 282, 283, 284, 290, 434, 477, 497, 681, 693, 707, 733, 748, 752, 783, 793, 806, 844, 880, 889, 932, 989] 33\n",
      "*  188    65              15       0.4080000    0.05053  87.6%   316  246s\n",
      "   193    66    0.37289   13   33    0.40800    0.05053  87.6%   312  258s\n",
      "   206    63     cutoff   14         0.40800    0.06093  85.1%   331  270s\n",
      "[0, 17, 38, 51, 70, 71, 86, 121, 177, 187, 234, 276, 282, 283, 284, 290, 434, 477, 497, 681, 693, 707, 733, 748, 752, 783, 793, 806, 844, 880, 889, 932, 989] 33\n",
      "H  216    63                       0.3680000    0.06093  83.4%   330  270s\n",
      "   223    70    0.25390    8   36    0.36800    0.06093  83.4%   328  278s\n",
      "   234    72     cutoff   12         0.36800    0.06093  83.4%   325  285s\n",
      "   240    70     cutoff   13         0.36800    0.06426  82.5%   332  297s\n",
      "   256    71    0.26778    7   32    0.36800    0.08000  78.3%   344  303s\n",
      "   267    72    0.09321    8   39    0.36800    0.08000  78.3%   343  315s\n",
      "[0, 17, 38, 51, 70, 71, 86, 121, 177, 187, 234, 276, 282, 283, 284, 290, 434, 477, 497, 681, 693, 707, 733, 748, 752, 783, 793, 806, 844, 880, 889, 932, 989] 33\n",
      "*  274    72              17       0.3360000    0.08000  76.2%   336  315s\n",
      "   284    74     cutoff    9         0.33600    0.08000  76.2%   343  326s\n",
      "   292    73    0.09448    9   45    0.33600    0.08000  76.2%   351  335s\n",
      "   319    81    0.22000   11    6    0.33600    0.08000  76.2%   339  343s\n",
      "   341    76    0.28160   12   20    0.33600    0.08000  76.2%   335  388s\n",
      "   360    75    0.16800   12   14    0.33600    0.08000  76.2%   373  398s\n",
      "   377    81    0.09133   11   18    0.33600    0.09133  72.8%   373  411s\n",
      "   393    78     cutoff   14         0.33600    0.09133  72.8%   373  424s\n",
      "   414    90    0.16800   13    8    0.33600    0.09133  72.8%   365  437s\n",
      "   441    86    0.30400   17    6    0.33600    0.09133  72.8%   358  450s\n",
      "   481    82     cutoff   21         0.33600    0.10335  69.2%   341  460s\n",
      "   517    83    0.28633   14   23    0.33600    0.10335  69.2%   337  473s\n",
      "   544    89    0.30100   15   16    0.33600    0.10767  68.0%   331  488s\n",
      "   568    91    0.27267   12   28    0.33600    0.12167  63.8%   328  504s\n",
      "   600    93     cutoff   15         0.33600    0.12222  63.6%   324  523s\n",
      "   630    96     cutoff   17         0.33600    0.12222  63.6%   322  536s\n",
      "   655    88     cutoff   19         0.33600    0.12222  63.6%   323  555s\n",
      "   681    91    0.22000   21    8    0.33600    0.12222  63.6%   331  618s\n",
      "   706    83    0.32467   23   18    0.33600    0.13267  60.5%   332  639s\n",
      "   748    96    0.16833   16   29    0.33600    0.13790  59.0%   323  659s\n",
      "   793    96    0.23600   23   13    0.33600    0.13790  59.0%   317  677s\n",
      "   821    90    0.26800   24   10    0.33600    0.14244  57.6%   321  703s\n",
      "   881    89     cutoff   15         0.33600    0.17200  48.8%   313  725s\n",
      "   934    79     cutoff   19         0.33600    0.18963  43.6%   309  749s\n",
      "   990    75    0.27200   25    6    0.33600    0.20142  40.1%   305  769s\n",
      "  1032    63     cutoff   21         0.33600    0.22000  34.5%   303  791s\n",
      "  1092    57     cutoff   22         0.33600    0.23300  30.7%   299  810s\n",
      "  1132     7    0.31333   24   16    0.33600    0.23600  29.8%   301  831s\n",
      "\n",
      "Cutting planes:\n",
      "  Gomory: 1\n",
      "  MIR: 6\n",
      "  Zero half: 1\n",
      "  Mod-K: 3\n",
      "  RLT: 45\n",
      "  Relax-and-lift: 35\n",
      "  Lazy constraints: 1\n",
      "\n",
      "Explored 1215 nodes (355528 simplex iterations) in 832.85 seconds (778.85 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 7: 0.336 0.368 0.408 ... 5.96\n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 3.360000000000e-01, best bound 3.360000000000e-01, gap 0.0000%\n",
      "\n",
      "User-callback calls 31544, time in user-callback 3.70 sec\n",
      "[[0 0 2 3]\n",
      " [0 1 2 5]\n",
      " [0 1 6 5]\n",
      " [4 6 6 5]]\n",
      "UB= 0.14\n",
      "LB= 0\n",
      "UB-LB= 0.14\n",
      "LEN= 758\n",
      "Set parameter LazyConstraints to value 1\n",
      "Gurobi Optimizer version 9.5.1 build v9.5.1rc2 (win64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 109945 rows, 13098 columns and 885166 nonzeros\n",
      "Model fingerprint: 0x6cc5ff99\n",
      "Variable types: 13002 continuous, 96 integer (96 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [4e-03, 1e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 3e+00]\n",
      "Presolve removed 684 rows and 0 columns\n",
      "Presolve time: 2.92s\n",
      "Presolved: 109261 rows, 13098 columns, 882430 nonzeros\n",
      "Variable types: 13002 continuous, 96 integer (96 binary)\n",
      "[3, 9, 13, 43, 49, 70, 91, 113, 121, 148, 153, 154, 181, 188, 195, 214, 235, 236, 257, 261, 269, 278, 284, 287, 293, 316, 367, 383, 408, 417, 424, 443, 448, 452, 456, 467, 492, 497, 527, 534, 566, 572, 583, 621, 641, 664, 665, 688, 694, 712, 717, 720, 729, 731, 750, 753, 761, 766, 784, 798, 805, 812, 818, 880, 903, 910, 912, 915, 919, 922, 924, 957, 962, 989] 74\n",
      "Found heuristic solution: objective 6.0160000\n",
      "\n",
      "Deterministic concurrent LP optimizer: primal and dual simplex (primal and dual model)\n",
      "Showing first log only...\n",
      "\n",
      "Root relaxation presolved: 109262 rows, 13098 columns, 883320 nonzeros\n",
      "\n",
      "\n",
      "Root simplex log...\n",
      "\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0    0.0000000e+00   5.000000e+00   2.100000e+07      6s\n",
      "     396    4.2000000e-02   0.000000e+00   0.000000e+00      6s\n",
      "\n",
      "Use crossover to convert LP symmetric solution to basic solution...\n",
      "\n",
      "Root crossover log...\n",
      "\n",
      "       0 DPushes remaining with DInf 0.0000000e+00                 7s\n",
      "\n",
      "      59 PPushes remaining with PInf 0.0000000e+00                 7s\n",
      "       0 PPushes remaining with PInf 0.0000000e+00                 7s\n",
      "\n",
      "  Push phase complete: Pinf 0.0000000e+00, Dinf 6.6439909e-16      7s\n",
      "\n",
      "\n",
      "Root simplex log...\n",
      "\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "     486    4.2000000e-02   0.000000e+00   0.000000e+00      7s\n",
      "Concurrent spin time: 0.03s\n",
      "\n",
      "Solved with primal simplex (primal model)\n",
      "\n",
      "Root relaxation: objective 4.200000e-02, 486 iterations, 2.96 seconds (0.69 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0    0.04200    0   20    6.01600    0.04200  99.3%     -   10s\n",
      "[3, 9, 13, 43, 49, 70, 91, 113, 121, 148, 153, 154, 181, 188, 195, 214, 235, 236, 257, 261, 269, 278, 284, 287, 293, 316, 367, 383, 408, 417, 424, 443, 448, 452, 456, 467, 492, 497, 527, 534, 566, 572, 583, 621, 641, 664, 665, 688, 694, 712, 717, 720, 729, 731, 750, 753, 761, 766, 784, 798, 805, 812, 818, 880, 903, 910, 912, 915, 919, 922, 924, 957, 962, 989] 74\n",
      "H    0     0                       2.3880000    0.04200  98.2%     -   12s\n",
      "     0     0    0.04600    0   40    2.38800    0.04600  98.1%     -   18s\n",
      "[3, 9, 13, 43, 49, 70, 91, 113, 121, 148, 153, 154, 181, 188, 195, 214, 235, 236, 257, 261, 269, 278, 284, 287, 293, 316, 367, 383, 408, 417, 424, 443, 448, 452, 456, 467, 492, 497, 527, 534, 566, 572, 583, 621, 641, 664, 665, 688, 694, 712, 717, 720, 729, 731, 750, 753, 761, 766, 784, 798, 805, 812, 818, 880, 903, 910, 912, 915, 919, 922, 924, 957, 962, 989] 74\n",
      "H    0     0                       1.4160000    0.04600  96.8%     -   22s\n",
      "     0     0    0.04696    0   34    1.41600    0.04696  96.7%     -   29s\n",
      "     0     0    0.04696    0   34    1.41600    0.04696  96.7%     -   30s\n",
      "     0     0    0.04881    0   42    1.41600    0.04881  96.6%     -   34s\n",
      "     0     0    0.04917    0   34    1.41600    0.04917  96.5%     -   36s\n",
      "     0     0    0.05133    0   40    1.41600    0.05133  96.4%     -   39s\n",
      "     0     0    0.05133    0   30    1.41600    0.05133  96.4%     -   41s\n",
      "     0     0    0.05136    0   51    1.41600    0.05136  96.4%     -   44s\n",
      "     0     0    0.05141    0   46    1.41600    0.05141  96.4%     -   46s\n",
      "     0     0    0.05142    0   39    1.41600    0.05142  96.4%     -   48s\n",
      "     0     0    0.05175    0   42    1.41600    0.05175  96.3%     -   53s\n",
      "     0     0    0.05175    0   42    1.41600    0.05175  96.3%     -   54s\n",
      "     0     0    0.05181    0   39    1.41600    0.05181  96.3%     -   57s\n",
      "     0     0    0.05181    0   48    1.41600    0.05181  96.3%     -   58s\n",
      "     0     0    0.05188    0   48    1.41600    0.05188  96.3%     -   61s\n",
      "     0     0    0.05188    0   51    1.41600    0.05188  96.3%     -   65s\n",
      "     0     0    0.05188    0   53    1.41600    0.05188  96.3%     -   66s\n",
      "     0     0    0.05188    0   38    1.41600    0.05188  96.3%     -   71s\n",
      "     0     0    0.05188    0   39    1.41600    0.05188  96.3%     -   73s\n",
      "     0     0    0.05200    0   26    1.41600    0.05200  96.3%     -   78s\n",
      "[3, 9, 13, 43, 49, 70, 91, 113, 121, 148, 153, 154, 181, 188, 195, 214, 235, 236, 257, 261, 269, 278, 284, 287, 293, 316, 367, 383, 408, 417, 424, 443, 448, 452, 456, 467, 492, 497, 527, 534, 566, 572, 583, 621, 641, 664, 665, 688, 694, 712, 717, 720, 729, 731, 750, 753, 761, 766, 784, 798, 805, 812, 818, 880, 903, 910, 912, 915, 919, 922, 924, 957, 962, 989] 74\n",
      "H    0     0                       1.1280000    0.05200  95.4%     -   80s\n",
      "     0     0    0.05200    0   35    1.12800    0.05200  95.4%     -   81s\n",
      "     0     0    0.05200    0   43    1.12800    0.05200  95.4%     -   84s\n",
      "     0     0    0.05200    0   43    1.12800    0.05200  95.4%     -   85s\n",
      "     0     0    0.05200    0   39    1.12800    0.05200  95.4%     -   90s\n",
      "[3, 9, 13, 43, 49, 70, 91, 113, 121, 148, 153, 154, 181, 188, 195, 214, 235, 236, 257, 261, 269, 278, 284, 287, 293, 316, 367, 383, 408, 417, 424, 443, 448, 452, 456, 467, 492, 497, 527, 534, 566, 572, 583, 621, 641, 664, 665, 688, 694, 712, 717, 720, 729, 731, 750, 753, 761, 766, 784, 798, 805, 812, 818, 880, 903, 910, 912, 915, 919, 922, 924, 957, 962, 989] 74\n",
      "H    0     0                       0.7440000    0.05200  93.0%     -   92s\n",
      "     0     0    0.05200    0   42    0.74400    0.05200  93.0%     -   92s\n",
      "     0     0    0.05200    0   32    0.74400    0.05200  93.0%     -   98s\n",
      "     0     2    0.05200    0   32    0.74400    0.05200  93.0%     -  101s\n",
      "     1     4    0.05200    1   34    0.74400    0.05200  93.0%  93.0  109s\n",
      "     3     8    0.05200    2   28    0.74400    0.05200  93.0%   828  112s\n",
      "     7    12    0.05600    3   17    0.74400    0.05200  93.0%   553  115s\n",
      "    11    14    0.05800    4   16    0.74400    0.05200  93.0%   411  127s\n",
      "    15    18    0.06056    5   37    0.74400    0.05200  93.0%  1135  132s\n",
      "    19    22    0.25000    5   16    0.74400    0.05200  93.0%  1082  137s\n",
      "    23    24    0.06583    6   29    0.74400    0.05200  93.0%   972  141s\n",
      "[3, 9, 13, 43, 49, 70, 91, 113, 121, 148, 153, 154, 181, 188, 195, 214, 235, 236, 257, 261, 269, 278, 284, 287, 293, 316, 367, 383, 408, 417, 424, 443, 448, 452, 456, 467, 492, 497, 527, 534, 566, 572, 583, 621, 641, 664, 665, 688, 694, 712, 717, 720, 729, 731, 750, 753, 761, 766, 784, 798, 805, 812, 818, 880, 903, 910, 912, 915, 919, 922, 924, 957, 962, 989] 74\n",
      "*   27    24               6       0.4960000    0.05200  89.5%   850  141s\n",
      "    29    24    0.08633    7   22    0.49600    0.05200  89.5%   854  151s\n",
      "    35    27    0.25067    7   19    0.49600    0.05200  89.5%   773  157s\n",
      "    40    30    0.44500    8   17    0.49600    0.05200  89.5%   739  163s\n",
      "    49    33    0.36133    8   22    0.49600    0.05200  89.5%   654  171s\n",
      "    54    34    0.36400    9   17    0.49600    0.05200  89.5%   669  179s\n",
      "    62    40    0.42400   10   23    0.49600    0.05200  89.5%   645  185s\n",
      "    70    40    0.49333   12   11    0.49600    0.05200  89.5%   612  195s\n",
      "    77    40     cutoff   13         0.49600    0.05469  89.0%   602  208s\n",
      "[3, 9, 13, 43, 49, 70, 91, 113, 121, 148, 153, 154, 181, 188, 195, 214, 235, 236, 257, 261, 269, 278, 284, 287, 293, 316, 367, 383, 408, 417, 424, 443, 448, 452, 456, 467, 492, 497, 527, 534, 566, 572, 583, 621, 641, 664, 665, 688, 694, 712, 717, 720, 729, 731, 750, 753, 761, 766, 784, 798, 805, 812, 818, 880, 903, 910, 912, 915, 919, 922, 924, 957, 962, 989] 74\n",
      "*   81    40              11       0.4400000    0.05469  87.6%   596  208s\n",
      "    87    46    0.05530    4   44    0.44000    0.05469  87.6%   580  213s\n",
      "    95    45    0.06026    6   37    0.44000    0.05469  87.6%   562  217s\n",
      "   106    47    0.07042    7   36    0.44000    0.05469  87.6%   524  260s\n",
      "   114    46    0.07819    8   34    0.44000    0.05469  87.6%   515  267s\n",
      "   127    47    0.10348    9   43    0.44000    0.05469  87.6%   495  274s\n",
      "   140    47    0.11630   10   41    0.44000    0.05469  87.6%   483  284s\n",
      "   148    49    0.11867   11   31    0.44000    0.05469  87.6%   480  291s\n",
      "   156    61    0.12000   12   14    0.44000    0.05469  87.6%   485  299s\n",
      "   170    58     cutoff   16         0.44000    0.05585  87.3%   476  313s\n",
      "   183    66    0.10800    6   22    0.44000    0.05585  87.3%   486  323s\n",
      "   197    63    0.11333    7   27    0.44000    0.05585  87.3%   480  330s\n",
      "   218    60    0.20967    9   16    0.44000    0.05585  87.3%   454  342s\n",
      "   235    61    0.12793    9   28    0.44000    0.05585  87.3%   454  351s\n",
      "   250    67     cutoff   11         0.44000    0.05585  87.3%   451  366s\n",
      "   272    67    0.20600   12   23    0.44000    0.05585  87.3%   447  375s\n",
      "   284    68    0.20800   13   16    0.44000    0.05585  87.3%   445  388s\n",
      "   309    67    0.43117   15   18    0.44000    0.07717  82.5%   440  399s\n",
      "   324    72    0.07733    7   46    0.44000    0.07733  82.4%   439  409s\n",
      "   339    73    0.10730    9   34    0.44000    0.07735  82.4%   434  423s\n",
      "   362    81    0.11700   10   16    0.44000    0.07735  82.4%   423  440s\n",
      "   378    75    0.24028   12   40    0.44000    0.07735  82.4%   441  451s\n",
      "   402    78    0.24279   13   34    0.44000    0.07735  82.4%   439  470s\n",
      "   425    72    0.43100   13   18    0.44000    0.09127  79.3%   447  485s\n",
      "   453    72    0.21800   13   20    0.44000    0.11750  73.3%   433  502s\n",
      "   473    79    0.26000   15   20    0.44000    0.12000  72.7%   430  516s\n",
      "   494    83     cutoff   18         0.44000    0.12000  72.7%   447  538s\n",
      "   510    87    0.31840   20   15    0.44000    0.12000  72.7%   465  552s\n",
      "   540    85     cutoff   21         0.44000    0.12178  72.3%   466  565s\n",
      "   572    81     cutoff   13         0.44000    0.14856  66.2%   465  579s\n",
      "   600    72    0.20600   10   23    0.44000    0.14856  66.2%   471  595s\n",
      "   645    79     cutoff   13         0.44000    0.15520  64.7%   462  609s\n",
      "   682    83     cutoff   15         0.44000    0.16800  61.8%   454  627s\n",
      "   712    76    0.26000   15   14    0.44000    0.16800  61.8%   464  645s\n",
      "   739    76     cutoff   18         0.44000    0.16800  61.8%   469  664s\n",
      "   773    77     cutoff   16         0.44000    0.16929  61.5%   475  688s\n",
      "   818    68    0.30200   13   16    0.44000    0.17207  60.9%   477  707s\n",
      "   867    71     cutoff   28         0.44000    0.20000  54.5%   476  729s\n",
      "   916    66    0.35200   18    6    0.44000    0.20000  54.5%   477  750s\n",
      "   945    56     cutoff   20         0.44000    0.20000  54.5%   489  774s\n",
      "   981    61    0.36840   16   23    0.44000    0.24000  45.5%   497  804s\n",
      "  1038    59    0.43800   18   16    0.44000    0.25000  43.2%   489  841s\n",
      "  1098    55    0.36667   19   26    0.44000    0.26000  40.9%   482  879s\n",
      "  1154    54     cutoff   20         0.44000    0.26000  40.9%   473  927s\n",
      "  1193    34    0.36059    9   36    0.44000    0.27113  38.4%   473  976s\n",
      "  1235    13     cutoff   22         0.44000    0.29800  32.3%   474 1021s\n",
      "  1268     0     cutoff   19         0.44000    0.33550  23.7%   476 1028s\n",
      "\n",
      "Cutting planes:\n",
      "  Gomory: 4\n",
      "  MIR: 14\n",
      "  Mod-K: 6\n",
      "  RLT: 73\n",
      "  Relax-and-lift: 439\n",
      "  Lazy constraints: 1\n",
      "\n",
      "Explored 1283 nodes (609824 simplex iterations) in 1029.28 seconds (1147.10 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 7: 0.44 0.496 0.744 ... 6.016\n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 4.400000000000e-01, best bound 4.400000000000e-01, gap 0.0000%\n",
      "\n",
      "User-callback calls 37844, time in user-callback 3.59 sec\n",
      "[[0 0 3 1]\n",
      " [0 0 6 5]\n",
      " [1 2 6 5]\n",
      " [4 2 6 5]]\n",
      "UB= 0.14\n",
      "LB= 0\n",
      "UB-LB= 0.14\n",
      "LEN= 800\n",
      "Set parameter LazyConstraints to value 1\n",
      "Gurobi Optimizer version 9.5.1 build v9.5.1rc2 (win64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 116035 rows, 13098 columns and 934138 nonzeros\n",
      "Model fingerprint: 0xfa76dbb8\n",
      "Variable types: 13002 continuous, 96 integer (96 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [4e-03, 1e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 3e+00]\n",
      "Presolve removed 792 rows and 0 columns\n",
      "Presolve time: 4.53s\n",
      "Presolved: 115243 rows, 13098 columns, 930970 nonzeros\n",
      "Variable types: 13002 continuous, 96 integer (96 binary)\n",
      "[2, 30, 36, 51, 68, 70, 76, 95, 96, 132, 160, 203, 246, 252, 258, 333, 351, 391, 409, 414, 488, 542, 572, 577, 628, 660, 663, 689, 694, 715, 744, 746, 770, 780, 825, 846, 851, 862, 882, 928, 946, 973] 42\n",
      "Found heuristic solution: objective 6.0520000\n",
      "\n",
      "Deterministic concurrent LP optimizer: primal and dual simplex (primal and dual model)\n",
      "Showing first log only...\n",
      "\n",
      "Root relaxation presolved: 115244 rows, 13098 columns, 931476 nonzeros\n",
      "\n",
      "\n",
      "Root simplex log...\n",
      "\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0    0.0000000e+00   5.000000e+00   2.100000e+07     10s\n",
      "     465    5.6000000e-02   0.000000e+00   0.000000e+00     11s\n",
      "\n",
      "Use crossover to convert LP symmetric solution to basic solution...\n",
      "\n",
      "Root crossover log...\n",
      "\n",
      "      20 DPushes remaining with DInf 0.0000000e+00                12s\n",
      "       0 DPushes remaining with DInf 0.0000000e+00                12s\n",
      "\n",
      "      66 PPushes remaining with PInf 0.0000000e+00                12s\n",
      "       0 PPushes remaining with PInf 0.0000000e+00                14s\n",
      "\n",
      "  Push phase complete: Pinf 0.0000000e+00, Dinf 7.1383871e-15     14s\n",
      "\n",
      "\n",
      "Root simplex log...\n",
      "\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "     554    5.6000000e-02   0.000000e+00   0.000000e+00     14s\n",
      "Concurrent spin time: 0.35s\n",
      "\n",
      "Solved with primal simplex (primal model)\n",
      "\n",
      "Root relaxation: objective 5.600000e-02, 554 iterations, 7.24 seconds (0.86 work units)\n",
      "Total elapsed time = 18.69s\n",
      "Total elapsed time = 21.95s\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0    0.05600    0   22    6.05200    0.05600  99.1%     -   23s\n",
      "[2, 30, 36, 51, 68, 70, 76, 95, 96, 132, 160, 203, 246, 252, 258, 333, 351, 391, 409, 414, 488, 542, 572, 577, 628, 660, 663, 689, 694, 715, 744, 746, 770, 780, 825, 846, 851, 862, 882, 928, 946, 973] 42\n",
      "H    0     0                       3.5040000    0.05600  98.4%     -   29s\n",
      "     0     0    0.05600    0   22    3.50400    0.05600  98.4%     -   39s\n",
      "     0     0    0.05900    0   26    3.50400    0.05900  98.3%     -   53s\n",
      "[2, 30, 36, 51, 68, 70, 76, 95, 96, 132, 160, 203, 246, 252, 258, 333, 351, 391, 409, 414, 488, 542, 572, 577, 628, 660, 663, 689, 694, 715, 744, 746, 770, 780, 825, 846, 851, 862, 882, 928, 946, 973] 42\n",
      "H    0     0                       3.0000000    0.05900  98.0%     -   55s\n",
      "     0     0    0.05900    0   28    3.00000    0.05900  98.0%     -   61s\n",
      "     0     0    0.06300    0   32    3.00000    0.06300  97.9%     -   71s\n",
      "     0     0    0.06456    0   28    3.00000    0.06456  97.8%     -   80s\n",
      "     0     0    0.06456    0   31    3.00000    0.06456  97.8%     -   85s\n",
      "     0     0    0.06456    0   36    3.00000    0.06456  97.8%     -   93s\n",
      "     0     0    0.06456    0   34    3.00000    0.06456  97.8%     -   97s\n",
      "     0     0    0.06456    0   32    3.00000    0.06456  97.8%     -  105s\n",
      "     0     0    0.06456    0   38    3.00000    0.06456  97.8%     -  106s\n",
      "     0     0    0.08029    0   38    3.00000    0.08029  97.3%     -  123s\n",
      "     0     0    0.08062    0   44    3.00000    0.08062  97.3%     -  129s\n",
      "     0     0    0.08445    0   48    3.00000    0.08445  97.2%     -  138s\n",
      "     0     0    0.08445    0   49    3.00000    0.08445  97.2%     -  140s\n",
      "     0     0    0.08450    0   56    3.00000    0.08450  97.2%     -  146s\n",
      "     0     0    0.08460    0   56    3.00000    0.08460  97.2%     -  149s\n",
      "     0     0    0.08460    0   54    3.00000    0.08460  97.2%     -  156s\n",
      "     0     0    0.08460    0   53    3.00000    0.08460  97.2%     -  160s\n",
      "     0     0    0.08478    0   47    3.00000    0.08478  97.2%     -  167s\n",
      "     0     0    0.08478    0   49    3.00000    0.08478  97.2%     -  169s\n",
      "     0     0    0.08491    0   51    3.00000    0.08491  97.2%     -  176s\n",
      "     0     0    0.08498    0   51    3.00000    0.08498  97.2%     -  178s\n",
      "     0     0    0.08498    0   53    3.00000    0.08498  97.2%     -  185s\n",
      "     0     0    0.08499    0   55    3.00000    0.08499  97.2%     -  187s\n",
      "     0     0    0.08501    0   51    3.00000    0.08501  97.2%     -  193s\n",
      "     0     0    0.08514    0   55    3.00000    0.08514  97.2%     -  195s\n",
      "     0     0    0.08524    0   57    3.00000    0.08524  97.2%     -  201s\n",
      "     0     0    0.08524    0   57    3.00000    0.08524  97.2%     -  203s\n",
      "     0     0    0.08524    0   51    3.00000    0.08524  97.2%     -  209s\n",
      "     0     0    0.08542    0   55    3.00000    0.08542  97.2%     -  210s\n",
      "     0     0    0.08551    0   48    3.00000    0.08551  97.1%     -  219s\n",
      "[2, 30, 36, 51, 68, 70, 76, 95, 96, 132, 160, 203, 246, 252, 258, 333, 351, 391, 409, 414, 488, 542, 572, 577, 628, 660, 663, 689, 694, 715, 744, 746, 770, 780, 825, 846, 851, 862, 882, 928, 946, 973] 42\n",
      "H    0     0                       2.0120000    0.08551  95.7%     -  222s\n",
      "     0     0    0.08551    0   48    2.01200    0.08551  95.7%     -  223s\n",
      "     0     0    0.08553    0   51    2.01200    0.08553  95.7%     -  229s\n",
      "     0     0    0.08558    0   50    2.01200    0.08558  95.7%     -  232s\n",
      "     0     0    0.08578    0   48    2.01200    0.08578  95.7%     -  238s\n",
      "     0     0    0.08578    0   49    2.01200    0.08578  95.7%     -  239s\n",
      "     0     0    0.08578    0   51    2.01200    0.08578  95.7%     -  244s\n",
      "     0     0    0.08578    0   52    2.01200    0.08578  95.7%     -  246s\n",
      "     0     0    0.08589    0   50    2.01200    0.08589  95.7%     -  256s\n",
      "     0     0    0.08598    0   47    2.01200    0.08598  95.7%     -  259s\n",
      "     0     0    0.08598    0   50    2.01200    0.08598  95.7%     -  267s\n",
      "     0     0    0.08598    0   50    2.01200    0.08598  95.7%     -  271s\n",
      "     0     0    0.08615    0   50    2.01200    0.08615  95.7%     -  276s\n",
      "     0     0    0.08615    0   50    2.01200    0.08615  95.7%     -  278s\n",
      "     0     0    0.08615    0   43    2.01200    0.08615  95.7%     -  284s\n",
      "     0     0    0.08615    0   49    2.01200    0.08615  95.7%     -  287s\n",
      "     0     0    0.08615    0   38    2.01200    0.08615  95.7%     -  296s\n",
      "     0     0    0.08615    0   38    2.01200    0.08615  95.7%     -  299s\n",
      "     0     0    0.08615    0   42    2.01200    0.08615  95.7%     -  308s\n",
      "     0     0    0.08615    0   45    2.01200    0.08615  95.7%     -  311s\n",
      "     0     0    0.08615    0   44    2.01200    0.08615  95.7%     -  319s\n",
      "     0     0    0.08615    0   44    2.01200    0.08615  95.7%     -  324s\n",
      "     0     2    0.08615    0   39    2.01200    0.08615  95.7%     -  336s\n",
      "     1     4    0.08615    1   44    2.01200    0.08615  95.7%  49.0  343s\n",
      "     3     8    0.08629    2   48    2.01200    0.08629  95.7%   425  352s\n",
      "    11    16    0.09714    4   52    2.01200    0.08643  95.7%   445  360s\n",
      "    15    20    0.35200    4   34    2.01200    0.08643  95.7%   395  367s\n",
      "    19    24    0.09752    5   49    2.01200    0.08643  95.7%   461  371s\n",
      "    27    33    0.10178    7   54    2.01200    0.08643  95.7%   438  380s\n",
      "[2, 30, 36, 51, 68, 70, 76, 95, 96, 132, 160, 203, 246, 252, 258, 333, 351, 391, 409, 414, 488, 542, 572, 577, 628, 660, 663, 689, 694, 715, 744, 746, 770, 780, 825, 846, 851, 862, 882, 928, 946, 973] 42\n",
      "H   31    33                       0.9680000    0.08643  91.1%   398  381s\n",
      "    32    35    0.10616    8   54    0.96800    0.08643  91.1%   425  386s\n",
      "    38    36    0.30667    8   21    0.96800    0.08643  91.1%   397  400s\n",
      "    43    36    0.34100    9   16    0.96800    0.08643  91.1%   514  414s\n",
      "    49    40    0.34100   10   14    0.96800    0.08643  91.1%   661  423s\n",
      "    59    44    0.58267   12   19    0.96800    0.08643  91.1%   650  434s\n",
      "    67    49    0.66800   13   12    0.96800    0.08643  91.1%   642  439s\n",
      "    78    55    0.67333   14    6    0.96800    0.08643  91.1%   570  452s\n",
      "    86    59    0.76533   15    5    0.96800    0.08643  91.1%   593  457s\n",
      "[2, 30, 36, 51, 68, 70, 76, 95, 96, 132, 160, 203, 246, 252, 258, 333, 351, 391, 409, 414, 488, 542, 572, 577, 628, 660, 663, 689, 694, 715, 744, 746, 770, 780, 825, 846, 851, 862, 882, 928, 946, 973] 42\n",
      "*   88    59              17       0.7920000    0.08643  89.1%   580  457s\n",
      "    96    68     cutoff   11         0.79200    0.09837  87.6%   561  465s\n",
      "   107    79    0.35200    4   30    0.79200    0.09837  87.6%   526  473s\n",
      "   118    83    0.38204    8   40    0.79200    0.09837  87.6%   502  482s\n",
      "   130    86    0.57953   12   21    0.79200    0.09837  87.6%   496  490s\n",
      "[2, 30, 36, 51, 68, 70, 76, 95, 96, 132, 160, 203, 246, 252, 258, 333, 351, 391, 409, 414, 488, 542, 572, 577, 628, 660, 663, 689, 694, 715, 744, 746, 770, 780, 825, 846, 851, 862, 882, 928, 946, 973] 42\n",
      "*  133    86              15       0.6680000    0.09837  85.3%   487  490s\n",
      "   145    74     cutoff   17         0.66800    0.10647  84.1%   482  505s\n",
      "   165    78    0.37267    8   33    0.66800    0.11858  82.2%   458  519s\n",
      "   175    86    0.43467   11   27    0.66800    0.11861  82.2%   481  535s\n",
      "   191    89    0.46790   14   31    0.66800    0.11861  82.2%   509  546s\n",
      "   202    92    0.54150   15   27    0.66800    0.11861  82.2%   509  570s\n",
      "   207    89    0.66133   16   25    0.66800    0.11861  82.2%   564  583s\n",
      "[2, 30, 36, 51, 68, 70, 76, 95, 96, 132, 160, 203, 246, 252, 258, 333, 351, 391, 409, 414, 488, 542, 572, 577, 628, 660, 663, 689, 694, 715, 744, 746, 770, 780, 825, 846, 851, 862, 882, 928, 946, 973] 42\n",
      "*  215    89              20       0.6120000    0.11861  80.6%   546  583s\n",
      "   222    88    0.59200   16   14    0.61200    0.12583  79.4%   571  594s\n",
      "   239    95    0.16667   11   25    0.61200    0.12583  79.4%   551  606s\n",
      "   248   102    0.21480   13   25    0.61200    0.12583  79.4%   567  615s\n",
      "[2, 30, 36, 51, 68, 70, 76, 95, 96, 132, 160, 203, 246, 252, 258, 333, 351, 391, 409, 414, 488, 542, 572, 577, 628, 660, 663, 689, 694, 715, 744, 746, 770, 780, 825, 846, 851, 862, 882, 928, 946, 973] 42\n",
      "*  258   102              20       0.6040000    0.12583  79.2%   561  615s\n",
      "   271   103     cutoff   15         0.60400    0.12583  79.2%   546  625s\n",
      "   290    99     cutoff   16         0.60400    0.14000  76.8%   546  632s\n",
      "   310   107    0.14058   13   48    0.60400    0.14050  76.7%   534  642s\n",
      "   320   113    0.16800   16   41    0.60400    0.14050  76.7%   543  654s\n",
      "   334   121    0.18727   19   35    0.60400    0.14050  76.7%   547  666s\n",
      "   342   130    0.20733   21   33    0.60400    0.14050  76.7%   548  676s\n",
      "   359   136    0.34917   24   14    0.60400    0.14050  76.7%   541  695s\n",
      "   391   146    0.47967   27   19    0.60400    0.14050  76.7%   533  707s\n",
      "   409   147     cutoff   28         0.60400    0.14050  76.7%   525  727s\n",
      "   430   148     cutoff   28         0.60400    0.14050  76.7%   540  741s\n",
      "   459   145    0.58741   28   13    0.60400    0.14050  76.7%   541  758s\n",
      "   472   148     cutoff   30         0.60400    0.16767  72.2%   563  776s\n",
      "   495   153    0.44214   15   23    0.60400    0.16767  72.2%   559  793s\n",
      "   524   155     cutoff   17         0.60400    0.17200  71.5%   548  809s\n",
      "   552   163     cutoff   16         0.60400    0.20733  65.7%   540  832s\n",
      "   570   163     cutoff   23         0.60400    0.20733  65.7%   547  855s\n",
      "   588   167    0.50743   23   23    0.60400    0.21480  64.4%   568  883s\n",
      "   622   165    0.34917   23   12    0.60400    0.21533  64.3%   558  913s\n",
      "   638   163    0.40400   24    6    0.60400    0.22600  62.6%   588  940s\n",
      "   670   161    0.40000   16    6    0.60400    0.26805  55.6%   601  968s\n",
      "   694   156     cutoff   18         0.60400    0.27041  55.2%   615  999s\n",
      "   725   166     cutoff   15         0.60400    0.29600  51.0%   623 1025s\n",
      "   759   165    0.57200   18    7    0.60400    0.29607  51.0%   620 1059s\n",
      "   804   157     cutoff   23         0.60400    0.29637  50.9%   613 1088s\n",
      "   834   158    0.46649   18   21    0.60400    0.29891  50.5%   622 1130s\n",
      "   871   163    0.50800   19   10    0.60400    0.30000  50.3%   619 1168s\n",
      "   914   169    0.56767   25   16    0.60400    0.30000  50.3%   617 1204s\n",
      "   956   163     cutoff   21         0.60400    0.30000  50.3%   611 1268s\n",
      "  1020   166    0.52356   25   16    0.60400    0.30000  50.3%   605 1309s\n",
      "  1073   168     cutoff   23         0.60400    0.30036  50.3%   597 1349s\n",
      "  1121   167    0.38711   21   20    0.60400    0.30200  50.0%   599 1388s\n",
      "  1158   167     cutoff   23         0.60400    0.30707  49.2%   601 1449s\n",
      "  1204   174     cutoff   29         0.60400    0.30833  49.0%   608 1498s\n",
      "  1255   172     cutoff   32         0.60400    0.31067  48.6%   608 1536s\n",
      "  1313   169    0.43200   29    8    0.60400    0.31067  48.6%   610 1599s\n",
      "  1368   160    0.52857   26   11    0.60400    0.31246  48.3%   616 1645s\n",
      "  1403   155     cutoff   27         0.60400    0.31750  47.4%   626 1690s\n"
     ]
    }
   ],
   "source": [
    "size=1000\n",
    "size_=100\n",
    "beta=0.75\n",
    "inst=[4,4,[2,2,1,1,3,3]]\n",
    "pcvar=[]\n",
    "import numpy as np\n",
    "# mean=np.arange(1,ship_num+1)\n",
    "mean=np.arange(1,len(inst[2])+1)\n",
    "covl=[1 for i in range(len(inst[2]))]\n",
    "# a=list(a)\n",
    "# a=random.sample(a,ship_num)\n",
    "# mean=np.sort(a)\n",
    "cov=np.zeros((len(inst[2]),len(inst[2])))\n",
    "for i in range(len(inst[2])):\n",
    "    for j in range(len(inst[2])):\n",
    "        if i==j:\n",
    "            cov[i][i]=covl[1]\n",
    "for i in range(len(inst[2])):\n",
    "    for j in range(len(inst[2])):\n",
    "        if i!=j:\n",
    "            cov[i][j]=0\n",
    "\n",
    "\n",
    "OC=[]\n",
    "EPS=0.1\n",
    "a=0\n",
    "UB=1000\n",
    "LB=0\n",
    "UB_k=[]\n",
    "zk=[]\n",
    "while UB-LB>=EPS:\n",
    "    print(\"UB=\",UB)\n",
    "    print(\"LB=\",LB)\n",
    "    print(\"UB-LB=\",UB-LB)\n",
    "    print(\"LEN=\",len(OC))\n",
    "    model=CVaR(len(inst[2]),inst[0],inst[1],inst[2],size,size_,beta,mean,cov,OC,zk)\n",
    "    zk,OC,penalty=model.solve()\n",
    "    n=0\n",
    "    while n<beta*size:\n",
    "        n+=1\n",
    "    tau=n\n",
    "    N_sigma=np.sort(penalty)\n",
    "    alpha_=N_sigma[tau-1]\n",
    "\n",
    "    UB=((tau/size-beta)*alpha_+sum(N_sigma[tau:]))/((1-beta)*size)\n",
    "    UB_k.append(UB)\n",
    "    UB=min(UB_k)\n",
    "    # for b in zk:\n",
    "    #     if not O[b] in OC:\n",
    "    #         OC.append(O[b])\n",
    "    a+=1\n",
    "    # print(OC)\n",
    "print(\"finish!!\")\n",
    "# model=CVaR(len(inst[2]),inst[0],inst[1],inst[2],100,100,0.75,mean,cov,OC,[])\n",
    "# model.solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08\n",
      "<gurobi.Var x_111 (value -0.0)>\n",
      "<gurobi.Var x_112 (value -0.0)>\n",
      "<gurobi.Var x_113 (value -0.0)>\n",
      "<gurobi.Var x_114 (value 0.0)>\n",
      "<gurobi.Var x_115 (value 1.0)>\n",
      "<gurobi.Var x_116 (value -0.0)>\n",
      "<gurobi.Var x_121 (value -0.0)>\n",
      "<gurobi.Var x_122 (value -0.0)>\n",
      "<gurobi.Var x_123 (value -0.0)>\n",
      "<gurobi.Var x_124 (value -0.0)>\n",
      "<gurobi.Var x_125 (value 1.0)>\n",
      "<gurobi.Var x_126 (value -0.0)>\n",
      "<gurobi.Var x_131 (value 0.0)>\n",
      "<gurobi.Var x_132 (value -0.0)>\n",
      "<gurobi.Var x_133 (value -0.0)>\n",
      "<gurobi.Var x_134 (value -0.0)>\n",
      "<gurobi.Var x_135 (value 1.0)>\n",
      "<gurobi.Var x_136 (value -0.0)>\n",
      "<gurobi.Var x_141 (value 1.0)>\n",
      "<gurobi.Var x_142 (value -0.0)>\n",
      "<gurobi.Var x_143 (value -0.0)>\n",
      "<gurobi.Var x_144 (value -0.0)>\n",
      "<gurobi.Var x_145 (value 0.0)>\n",
      "<gurobi.Var x_146 (value -0.0)>\n",
      "<gurobi.Var x_211 (value -0.0)>\n",
      "<gurobi.Var x_212 (value -0.0)>\n",
      "<gurobi.Var x_213 (value -0.0)>\n",
      "<gurobi.Var x_214 (value 1.0)>\n",
      "<gurobi.Var x_215 (value 0.0)>\n",
      "<gurobi.Var x_216 (value -0.0)>\n",
      "<gurobi.Var x_221 (value 1.0)>\n",
      "<gurobi.Var x_222 (value 0.0)>\n",
      "<gurobi.Var x_223 (value -0.0)>\n",
      "<gurobi.Var x_224 (value -0.0)>\n",
      "<gurobi.Var x_225 (value -0.0)>\n",
      "<gurobi.Var x_226 (value -0.0)>\n",
      "<gurobi.Var x_231 (value 0.0)>\n",
      "<gurobi.Var x_232 (value -0.0)>\n",
      "<gurobi.Var x_233 (value -0.0)>\n",
      "<gurobi.Var x_234 (value -0.0)>\n",
      "<gurobi.Var x_235 (value -0.0)>\n",
      "<gurobi.Var x_236 (value -0.0)>\n",
      "<gurobi.Var x_241 (value 0.0)>\n",
      "<gurobi.Var x_242 (value 0.0)>\n",
      "<gurobi.Var x_243 (value -0.0)>\n",
      "<gurobi.Var x_244 (value -0.0)>\n",
      "<gurobi.Var x_245 (value -0.0)>\n",
      "<gurobi.Var x_246 (value -0.0)>\n",
      "<gurobi.Var x_311 (value -0.0)>\n",
      "<gurobi.Var x_312 (value -0.0)>\n",
      "<gurobi.Var x_313 (value -0.0)>\n",
      "<gurobi.Var x_314 (value -0.0)>\n",
      "<gurobi.Var x_315 (value -0.0)>\n",
      "<gurobi.Var x_316 (value 1.0)>\n",
      "<gurobi.Var x_321 (value -0.0)>\n",
      "<gurobi.Var x_322 (value -0.0)>\n",
      "<gurobi.Var x_323 (value 1.0)>\n",
      "<gurobi.Var x_324 (value -0.0)>\n",
      "<gurobi.Var x_325 (value -0.0)>\n",
      "<gurobi.Var x_326 (value -0.0)>\n",
      "<gurobi.Var x_331 (value -0.0)>\n",
      "<gurobi.Var x_332 (value -0.0)>\n",
      "<gurobi.Var x_333 (value -0.0)>\n",
      "<gurobi.Var x_334 (value 0.0)>\n",
      "<gurobi.Var x_335 (value -0.0)>\n",
      "<gurobi.Var x_336 (value -0.0)>\n",
      "<gurobi.Var x_341 (value -0.0)>\n",
      "<gurobi.Var x_342 (value -0.0)>\n",
      "<gurobi.Var x_343 (value -0.0)>\n",
      "<gurobi.Var x_344 (value -0.0)>\n",
      "<gurobi.Var x_345 (value -0.0)>\n",
      "<gurobi.Var x_346 (value -0.0)>\n",
      "<gurobi.Var x_411 (value -0.0)>\n",
      "<gurobi.Var x_412 (value -0.0)>\n",
      "<gurobi.Var x_413 (value -0.0)>\n",
      "<gurobi.Var x_414 (value -0.0)>\n",
      "<gurobi.Var x_415 (value 0.0)>\n",
      "<gurobi.Var x_416 (value 1.0)>\n",
      "<gurobi.Var x_421 (value -0.0)>\n",
      "<gurobi.Var x_422 (value -0.0)>\n",
      "<gurobi.Var x_423 (value -0.0)>\n",
      "<gurobi.Var x_424 (value -0.0)>\n",
      "<gurobi.Var x_425 (value -0.0)>\n",
      "<gurobi.Var x_426 (value 1.0)>\n",
      "<gurobi.Var x_431 (value -0.0)>\n",
      "<gurobi.Var x_432 (value 1.0)>\n",
      "<gurobi.Var x_433 (value 0.0)>\n",
      "<gurobi.Var x_434 (value -0.0)>\n",
      "<gurobi.Var x_435 (value 0.0)>\n",
      "<gurobi.Var x_436 (value 0.0)>\n",
      "<gurobi.Var x_441 (value 0.0)>\n",
      "<gurobi.Var x_442 (value 1.0)>\n",
      "<gurobi.Var x_443 (value -0.0)>\n",
      "<gurobi.Var x_444 (value 0.0)>\n",
      "<gurobi.Var x_445 (value -0.0)>\n",
      "<gurobi.Var x_446 (value -0.0)>\n"
     ]
    }
   ],
   "source": [
    "b=model.get_optimal_val()\n",
    "print(b)\n",
    "result=model.get_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 2],\n",
       "       [5, 0, 0, 2],\n",
       "       [5, 1, 3, 6],\n",
       "       [5, 4, 6, 6]])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gurobi Optimizer version 9.5.1 build v9.5.1rc2 (win64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 32 rows, 48 columns and 159 nonzeros\n",
      "Model fingerprint: 0xdf3425ed\n",
      "Variable types: 0 continuous, 48 integer (48 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 4e+00]\n",
      "Found heuristic solution: objective 5.0000000\n",
      "Presolve removed 0 rows and 3 columns\n",
      "Presolve time: 0.00s\n",
      "Presolved: 32 rows, 45 columns, 150 nonzeros\n",
      "Variable types: 0 continuous, 45 integer (45 binary)\n",
      "Found heuristic solution: objective 1.0000000\n",
      "\n",
      "Root relaxation: objective 0.000000e+00, 17 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "*    0     0               0       0.0000000    0.00000  0.00%     -    0s\n",
      "\n",
      "Explored 1 nodes (17 simplex iterations) in 0.03 seconds (0.00 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 3: 0 1 5 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 0.000000000000e+00, best bound 0.000000000000e+00, gap 0.0000%\n",
      "====================================================\n",
      "[[2 0 0]\n",
      " [4 0 1]\n",
      " [6 0 3]\n",
      " [8 5 7]]\n",
      "the objective function 0.0\n",
      "Gurobi Optimizer version 9.5.1 build v9.5.1rc2 (win64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 50 rows, 48 columns and 213 nonzeros\n",
      "Model fingerprint: 0x01acd957\n",
      "Variable types: 0 continuous, 48 integer (48 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 4e+00]\n",
      "Found heuristic solution: objective 5.0000000\n",
      "Presolve removed 18 rows and 3 columns\n",
      "Presolve time: 0.00s\n",
      "Presolved: 32 rows, 45 columns, 168 nonzeros\n",
      "Variable types: 0 continuous, 45 integer (45 binary)\n",
      "Found heuristic solution: objective 2.0000000\n",
      "\n",
      "Root relaxation: objective 0.000000e+00, 23 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "*    0     0               0       0.0000000    0.00000  0.00%     -    0s\n",
      "\n",
      "Explored 1 nodes (23 simplex iterations) in 0.03 seconds (0.00 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 3: 0 2 5 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 0.000000000000e+00, best bound 0.000000000000e+00, gap 0.0000%\n",
      "====================================================\n",
      "[[0 0 0]\n",
      " [2 1 0]\n",
      " [5 4 3]\n",
      " [8 7 6]]\n",
      "the objective function 0.0\n",
      "Gurobi Optimizer version 9.5.1 build v9.5.1rc2 (win64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 65 rows, 48 columns and 258 nonzeros\n",
      "Model fingerprint: 0xc8a18611\n",
      "Variable types: 0 continuous, 48 integer (48 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 4e+00]\n",
      "Found heuristic solution: objective 6.0000000\n",
      "Presolve removed 33 rows and 3 columns\n",
      "Presolve time: 0.00s\n",
      "Presolved: 32 rows, 45 columns, 183 nonzeros\n",
      "Found heuristic solution: objective 4.0000000\n",
      "Variable types: 0 continuous, 45 integer (45 binary)\n",
      "Found heuristic solution: objective 3.0000000\n",
      "\n",
      "Root relaxation: objective 2.000000e+00, 32 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "*    0     0               0       2.0000000    2.00000  0.00%     -    0s\n",
      "\n",
      "Explored 1 nodes (32 simplex iterations) in 0.04 seconds (0.00 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 4: 2 3 4 6 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 2.000000000000e+00, best bound 2.000000000000e+00, gap 0.0000%\n",
      "====================================================\n",
      "[[0 0 0]\n",
      " [0 1 3]\n",
      " [2 5 4]\n",
      " [6 7 8]]\n",
      "the objective function 2.0\n",
      "Gurobi Optimizer version 9.5.1 build v9.5.1rc2 (win64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 77 rows, 48 columns and 294 nonzeros\n",
      "Model fingerprint: 0x019516f6\n",
      "Variable types: 0 continuous, 48 integer (48 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 4e+00]\n",
      "Found heuristic solution: objective 5.0000000\n",
      "Presolve removed 45 rows and 3 columns\n",
      "Presolve time: 0.00s\n",
      "Presolved: 32 rows, 45 columns, 195 nonzeros\n",
      "Variable types: 0 continuous, 45 integer (45 binary)\n",
      "Found heuristic solution: objective 4.0000000\n",
      "\n",
      "Root relaxation: objective 3.000000e+00, 24 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "*    0     0               0       3.0000000    3.00000  0.00%     -    0s\n",
      "\n",
      "Explored 1 nodes (24 simplex iterations) in 0.02 seconds (0.00 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 3: 3 4 5 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 3.000000000000e+00, best bound 3.000000000000e+00, gap 0.0000%\n",
      "====================================================\n",
      "[[0 0 0]\n",
      " [0 1 3]\n",
      " [2 6 4]\n",
      " [7 8 5]]\n",
      "the objective function 3.0\n",
      "Gurobi Optimizer version 9.5.1 build v9.5.1rc2 (win64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 86 rows, 48 columns and 321 nonzeros\n",
      "Model fingerprint: 0xdcb80871\n",
      "Variable types: 0 continuous, 48 integer (48 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 4e+00]\n",
      "Found heuristic solution: objective 5.0000000\n",
      "Presolve removed 54 rows and 3 columns\n",
      "Presolve time: 0.02s\n",
      "Presolved: 32 rows, 45 columns, 204 nonzeros\n",
      "Variable types: 0 continuous, 45 integer (45 binary)\n",
      "\n",
      "Root relaxation: objective 3.000000e+00, 38 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "*    0     0               0       3.0000000    3.00000  0.00%     -    0s\n",
      "\n",
      "Explored 1 nodes (38 simplex iterations) in 0.05 seconds (0.00 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 2: 3 5 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 3.000000000000e+00, best bound 3.000000000000e+00, gap 0.0000%\n",
      "====================================================\n",
      "[[3 0 0]\n",
      " [4 0 0]\n",
      " [5 2 1]\n",
      " [6 8 7]]\n",
      "the objective function 3.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAASf0lEQVR4nO3dfbBkdX3n8fdnGaJLjCjORClgGFbZTaEraK4sxmR3EuMWWCknWckubko0a2qyFkahtKKb2gLUSmK2EoeKKBRRCkwlYkoJmZBxEys+4CNyhwwPA2ZrtNZiKFauoEOICo58948+V5um+3bfO30f+uf7VXWqzsOvz/n+bnd/zq/P7YdUFZKk2fcv1rsASdJ0GOiS1AgDXZIaYaBLUiMMdElqxKb1OvDmzZtr27Zt63V4SZpJe/fu/UZVbRm2bd0Cfdu2bczPz6/X4SVpJiX52qhtXnKRpEYY6JLUCANdkhphoEtSIwx0SWrE2EBP8uQkX0pyW5L9Sd4+pM2Tknw4yYEkNyfZtirVSpJGmmSE/gjwC1V1OnAGcHaSswbavA74ZlU9B9gF/MFUq5QkjTU20Kvn4W7x6G4a/M7dHcC13fxHgJcmydSqlCSNNdE19CRHJdkH3A98vKpuHmhyAnAPQFUdBg4Bzxiyn51J5pPMLywsHFHhP4qSrHiS1L6JAr2qvl9VZwAnAmcmed5KDlZVV1XVXFXNbdky9JOrWkJVjZwm2S6pbct6l0tVfQv4JHD2wKZ7gZMAkmwCjgUemEJ9kqQJTfIuly1JntbN/0vgZcCXB5rtBl7TzZ8LfKIcFkrSmprky7mOB65NchS9E8BfVNWNSd4BzFfVbuADwJ8mOQA8CJy3ahVLkoYaG+hVdTvwgiHrL+6b/y7wq9MtTZK0HH5SVJIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1IixgZ7kpCSfTHJXkv1J3jSkzfYkh5Ls66aLV6dcSdIomyZocxh4c1XdmuQngL1JPl5Vdw20+0xV/dL0S5QkTWLsCL2q7quqW7v5fwLuBk5Y7cIkScuzrGvoSbYBLwBuHrL5xUluS/KxJM8dcfudSeaTzC8sLCy/WknSSBMHepKnAB8FLqyqhwY23wqcXFWnA+8Bbhi2j6q6qqrmqmpuy5YtKyxZkjTMRIGe5Gh6Yf5nVXX94PaqeqiqHu7m9wBHJ9k81UolSUua5F0uAT4A3F1V7x7R5lldO5Kc2e33gWkWKkla2iTvcnkJ8GrgjiT7unW/A2wFqKorgXOB1yc5DHwHOK+qavrlSpJGGRvoVfVZIGPaXA5cPq2iJEnL5ydFJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjRgb6ElOSvLJJHcl2Z/kTUPaJMkfJzmQ5PYkL1ydcpcvyRMmaaNo+fHZct82qklG6IeBN1fVacBZwAVJThtocw5wajftBK6YapUr1P8A2rVr19D10noZ9Ths4fHZct82srGBXlX3VdWt3fw/AXcDJww02wF8sHq+CDwtyfFTr3aFqooLL7yQqlrvUqQnqKofTK1puW8b0bKuoSfZBrwAuHlg0wnAPX3LB3li6JNkZ5L5JPMLCwvLLHVl+kfmw5Y3muOOO27oS9VxEwx/iTtuOu6449a5xz+0kvp9Ka9pWelzb6XTajz3MumZM8lTgE8Dv1tV1w9suxF4V1V9tlv+e+CtVTU/an9zc3M1Pz9y81QsPtH7+zhs3UaSZE1rW+vjrdSs1Lkcs/j4nNQs9m1WnntJ9lbV3LBtE43QkxwNfBT4s8Ew79wLnNS3fGK3bkNIwmWXXeZIThtSy680Wu7bRjTJu1wCfAC4u6rePaLZbuD83ptdchZwqKrum2KdK9J/9rvooouGrpfWy6jHYQuPz5b7tpFtmqDNS4BXA3ck2det+x1gK0BVXQnsAV4OHAC+Dfz61CtdIR9A2shafny23LeNamygd9fFl3y9VL177oJpFSVJWj4/KSpJjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWrE2EBPcnWS+5PcOWL79iSHkuzrpounX6YkaZxNE7S5Brgc+OASbT5TVb80lYokSSsydoReVTcBD65BLZKkIzCta+gvTnJbko8lee6oRkl2JplPMr+wsDClQ0uSYDqBfitwclWdDrwHuGFUw6q6qqrmqmpuy5YtUzi0JGnREQd6VT1UVQ9383uAo5NsPuLKJEnLcsSBnuRZSdLNn9nt84Ej3a8kaXnGvsslyYeA7cDmJAeBS4CjAarqSuBc4PVJDgPfAc6rqlq1iiVJQ40N9Kp61Zjtl9N7W6MkaR35SVFJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjxgZ6kquT3J/kzhHbk+SPkxxIcnuSF06/TEnSOJOM0K8Bzl5i+znAqd20E7jiyMuSJC3X2ECvqpuAB5dosgP4YPV8EXhakuOnVaAkaTKbprCPE4B7+pYPduvuG2yYZCe9UTxbt25d2dEuPXZltzsSlx5as0PVJU9d0z7WJU9ds2MBK+7bEf1d1vD+W/PHZ8t9A597y5SqGt8o2QbcWFXPG7LtRuBdVfXZbvnvgbdW1fxS+5ybm6v5+SWbqEFJmOQxN6vHk1Zbkr1VNTds2zTe5XIvcFLf8ondOknSGppGoO8Gzu/e7XIWcKiqnnC5RZK0usZeQ0/yIWA7sDnJQeAS4GiAqroS2AO8HDgAfBv49dUqVpI02thAr6pXjdlewAVTq0iStCJ+UlSSGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktSIiQI9ydlJ/jHJgSRvG7L9tUkWkuzrpt+YfqmSpKVsGtcgyVHAe4GXAQeBW5Lsrqq7Bpp+uKresAo1SpImMMkI/UzgQFV9taoeBa4DdqxuWZKk5Zok0E8A7ulbPtitG/TKJLcn+UiSk4btKMnOJPNJ5hcWFlZQriRplGn9U/SvgW1V9Xzg48C1wxpV1VVVNVdVc1u2bJnSoSVJMFmg3wv0j7hP7Nb9QFU9UFWPdIvvB356OuVJkiY1SaDfApya5JQkPwacB+zub5Dk+L7FVwB3T69ESdIkxr7LpaoOJ3kD8LfAUcDVVbU/yTuA+araDbwxySuAw8CDwGtXsWZJ0hCpqnU58NzcXM3Pz6/LsbV+krCWj7m1Pp602pLsraq5Ydv8pKgkNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakREwV6krOT/GOSA0neNmT7k5J8uNt+c5JtU69UkrSksYGe5CjgvcA5wGnAq5KcNtDsdcA3q+o5wC7gD6ZdqCRpaZOM0M8EDlTVV6vqUeA6YMdAmx3Atd38R4CXJsn0ypQkjbNpgjYnAPf0LR8E/t2oNlV1OMkh4BnAN/obJdkJ7ATYunXrCkvWrFvLc/3Tn/70NTuWtN4mCfSpqaqrgKsA5ubmai2PrY2hyrtdWi2TXHK5Fzipb/nEbt3QNkk2AccCD0yjQEnSZCYJ9FuAU5OckuTHgPOA3QNtdgOv6ebPBT5RDsUkaU2NveTSXRN/A/C3wFHA1VW1P8k7gPmq2g18APjTJAeAB+mFviRpDU10Db2q9gB7BtZd3Df/XeBXp1uaJGk5/KSoJDXCQJekRhjoktQIA12SGpH1endhkgXga2t4yM0MfHK1MfZvtrXcv5b7Bmvfv5OrasuwDesW6GstyXxVza13HavF/s22lvvXct9gY/XPSy6S1AgDXZIa8aMU6FetdwGrzP7Ntpb713LfYAP170fmGrokte5HaYQuSU0z0CWpERsi0JP8myT7+qaHklyY5NIk9/atf3nX/iVJbk8yn+TUbt3TkvxdkpF9SnJmkpu6H7z+hyTvT3JakoODt+uON/jLTIvbtiX5TtfmriQfTHL0iLbPTPLnSb6aZG+SLyT5lZX/tdZGkp/qan0kyVuWaDer/fu17jF0R5LPJzl9RLtZ7d+Orn/7uufJz45oN5P9W5TkRUkOJzl3xPaZ7F+S7UkO9WXfxeNvRe8XZDbSRO8rev8fcDJwKfCWIW2up/dDGz8L/FG37g+B7Uvs95n0Psj04r5153brPw/8h771PwV8ZYl9bQPu7Kv3E8CvDWkX4AvAf+9bdzLwW+v9d57gfvhJ4EXA7w67Dxro388AT+/mzwFubqx/T+GH/yN7PvDllvrX1br43NsDnNtS/4DtwI3Lvd2GGKEPeCm9MF3qU6TfA47ppu8leTZwUlV9aonbXABcW1VfWFxRVR+pqq8DH+Lx3+F+HnBdNxL/TJJbu+lnBndaVd8HvkTvd1UH/QLwaFVd2df+a1X1nlH77s7Mn07yV92o4l3daPJL3Wjy2V27a5JckeSLXbvtSa5OcneSaxaP17WZT7I/yduX+PsM9uv+qrqF3t96lFnu3+er6pvd4hfpDRBa6t/D1SUD8OPAsHc/zGz/Or8FfBS4f8T2We/f8q33mWjImelq4A3d/KXA/wVu79YvjqjOoPck/CS9J+J1wKlj9ns9sGPEtmcC9wGbuuW7gefRO2E8uVt3Kr0f9IDHj9Cf3NXx/CH7fSOwa8QxR+17O/At4HjgSfR+3u/t3bY3AZd189d0/Q6wA3gI+Lf0LqPtBc7o2h1XPxzNfGqxTmAXsG/I9LaBOi9l9Ah95vvXtX0L8P7W+gf8CvBlej868+KW+kdvAPXpbn/XMHyEPsv9207vZzxvAz4GPHeS/FzTH4keJ72fuHsF8D+6VVcA76Q3ungn8EfAf6uqfcBZ3W3+Pb0wTpIP0xtRvrl6I++JVNXXk9wJvDTJ14HDVXVnkmOBy5OcAXwf+Nd9N3t2kn3AKcDfVNXtE/TvvfQuEz0K/OIS+76lqu7rbvMV4O+69XcAP9/X7q+rqpLcAXy9qu7obrOf3klnH/Cfk+yk92MmxwOnAbdX1UWT/G2WYxb7l+Tngdd1dTfVv6r6S+Avu+fIO7uaW+nfZcBbq+qxJGOazmT/bqX3nS0Pp/e/wxvonXiWtKECnd61zFsXw7g/lJP8CXBjf+P07sn/Se8SyXuA36b3h3xjknngkq7pbwD7gZ8G/mrEsRcvuyxeggG4qFs+nd6Z97t97b9SVWck2Qx8LskrqvdzfP32A69cXKiqC7r282P2/Ujf/GN9y4/x+PvskSFtftAuySn0Rp8vqqpvdi8FnwyQZBePf3Auuq6q3jVk/TAz3b8kzwfeD5xTVcN+1Hym+9dX901J/lWSzVXV/yVSs9y/OXqXRaH35VgvT3K4qm5ooX9V9VBf3XuSvG/I/fcEGy3QX8UPw5Qkxy+eKem9fLxzoP35wJ6qejDJMfT+kI8BxyyOTvr2dQ/wpSR/U1U3d+v+E/C57sRxPfD7wLfpXccHOBY42I0CXkPvZdPjVNU3kryN3quKwUD/BPB7SV5fVVd0646ZdN9T8FTgn4FDSZ5J74T5qa7uaYzQZ7Z/SbbSu89fXVX/Z0SzWe7fc+gNOirJC+ldPhg8ac1s/6rqlMX5LihvHAhzmOH+JXkWvVF/JTmT3kln2KDjcTZMoCf5ceBlwG/2rf5f3Uuionct/Tf72h8DvBb4j92qd9P7b/ejwH8d3H93WeU84A+T/CS94L8J+N/d9m8l+QLwrKr6anez9wEfTXJ+1+6fR5R/A3Bpkp+rqs/0HbOS/DKwK8lvAwvdPt5K7yXVJPtesaq6Lck/0LuOeg/wuUlv2z2g5uk9KB9LciFw2sDIYWb7B1wMPAN4XzfKO1wD35g34/17JXB+ku8B3wH+S3UXZ/v2P8v9m2T/s9y/c4HXJzlM7/47b/D+G8aP/ktSIzbi2xYlSStgoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RG/H8Rse+PzQ1aRwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_2 = np.random.multivariate_normal(mean, cov, size=size_)\n",
    "\n",
    "OR_=np.argsort(data_2)\n",
    "OR_=OR_+np.ones((size_,len(inst[2]))).astype(int)\n",
    "OR=OR_.tolist()\n",
    "penalty_cvar=[]\n",
    "for k in OR:\n",
    "    OR2=k\n",
    "    a=0\n",
    "    for j in range(inst[0]):\n",
    "        for i in range(1,inst[1]):\n",
    "            for i_ in range(i+1,inst[1]+1):\n",
    "                if result[i-1][j]!=0 and result[i_-1][j]!=0:\n",
    "                    if OR2.index(result[i-1][j])>OR2.index(result[i_-1][j]):\n",
    "                        # print(j+1,height-i+1,O)\n",
    "                        a+=1\n",
    "                        # print(\"penalty!\")\n",
    "                            # print(i,i_,j+1,O)\n",
    "                        break\n",
    "    penalty_cvar.append(a)\n",
    "penalty_cvar=np.sort(penalty_cvar)\n",
    "penalty_cvar=penalty_cvar[round(0.75*size_):]\n",
    "pcvar=[]\n",
    "pcvar+=list(penalty_cvar)\n",
    "data=(tuple(pcvar),)\n",
    "l=[\"75%-CVaR\"]\n",
    "for g in range(1,6):\n",
    "  prob=[]\n",
    "  ship_num=len(inst[2])\n",
    "  robust(ship_num,inst[0],inst[1],inst[2],g,size_,mean,cov)\n",
    "  prob+=list(penalty_r)\n",
    "#   print(prob)\n",
    "  data+=(tuple(prob),)\n",
    "  l.append(\"Gamma=\"+str(g))\n",
    "  # print(\"data=\",data)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.boxplot(data,labels=l)\n",
    "plt.show()\n",
    "# plt.hist(penalty_cvar,bins=50)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gurobi Optimizer version 9.5.1 build v9.5.1rc2 (win64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 32 rows, 48 columns and 159 nonzeros\n",
      "Model fingerprint: 0xdf3425ed\n",
      "Variable types: 0 continuous, 48 integer (48 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 4e+00]\n",
      "Found heuristic solution: objective 5.0000000\n",
      "Presolve removed 0 rows and 3 columns\n",
      "Presolve time: 0.00s\n",
      "Presolved: 32 rows, 45 columns, 150 nonzeros\n",
      "Variable types: 0 continuous, 45 integer (45 binary)\n",
      "Found heuristic solution: objective 1.0000000\n",
      "\n",
      "Root relaxation: objective 0.000000e+00, 17 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "*    0     0               0       0.0000000    0.00000  0.00%     -    0s\n",
      "\n",
      "Explored 1 nodes (17 simplex iterations) in 0.03 seconds (0.00 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 3: 0 1 5 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 0.000000000000e+00, best bound 0.000000000000e+00, gap 0.0000%\n",
      "====================================================\n",
      "[[2 0 0]\n",
      " [4 0 1]\n",
      " [6 0 3]\n",
      " [8 5 7]]\n",
      "the objective function 0.0\n",
      "Gurobi Optimizer version 9.5.1 build v9.5.1rc2 (win64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 50 rows, 48 columns and 213 nonzeros\n",
      "Model fingerprint: 0x01acd957\n",
      "Variable types: 0 continuous, 48 integer (48 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 4e+00]\n",
      "Found heuristic solution: objective 5.0000000\n",
      "Presolve removed 18 rows and 3 columns\n",
      "Presolve time: 0.00s\n",
      "Presolved: 32 rows, 45 columns, 168 nonzeros\n",
      "Variable types: 0 continuous, 45 integer (45 binary)\n",
      "Found heuristic solution: objective 2.0000000\n",
      "\n",
      "Root relaxation: objective 0.000000e+00, 23 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "*    0     0               0       0.0000000    0.00000  0.00%     -    0s\n",
      "\n",
      "Explored 1 nodes (23 simplex iterations) in 0.03 seconds (0.00 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 3: 0 2 5 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 0.000000000000e+00, best bound 0.000000000000e+00, gap 0.0000%\n",
      "====================================================\n",
      "[[0 0 0]\n",
      " [2 1 0]\n",
      " [5 4 3]\n",
      " [8 7 6]]\n",
      "the objective function 0.0\n",
      "Gurobi Optimizer version 9.5.1 build v9.5.1rc2 (win64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 65 rows, 48 columns and 258 nonzeros\n",
      "Model fingerprint: 0xc8a18611\n",
      "Variable types: 0 continuous, 48 integer (48 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 4e+00]\n",
      "Found heuristic solution: objective 6.0000000\n",
      "Presolve removed 33 rows and 3 columns\n",
      "Presolve time: 0.00s\n",
      "Presolved: 32 rows, 45 columns, 183 nonzeros\n",
      "Found heuristic solution: objective 4.0000000\n",
      "Variable types: 0 continuous, 45 integer (45 binary)\n",
      "Found heuristic solution: objective 3.0000000\n",
      "\n",
      "Root relaxation: objective 2.000000e+00, 32 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "*    0     0               0       2.0000000    2.00000  0.00%     -    0s\n",
      "\n",
      "Explored 1 nodes (32 simplex iterations) in 0.04 seconds (0.00 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 4: 2 3 4 6 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 2.000000000000e+00, best bound 2.000000000000e+00, gap 0.0000%\n",
      "====================================================\n",
      "[[0 0 0]\n",
      " [0 1 3]\n",
      " [2 5 4]\n",
      " [6 7 8]]\n",
      "the objective function 2.0\n",
      "Gurobi Optimizer version 9.5.1 build v9.5.1rc2 (win64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 77 rows, 48 columns and 294 nonzeros\n",
      "Model fingerprint: 0x019516f6\n",
      "Variable types: 0 continuous, 48 integer (48 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 4e+00]\n",
      "Found heuristic solution: objective 5.0000000\n",
      "Presolve removed 45 rows and 3 columns\n",
      "Presolve time: 0.00s\n",
      "Presolved: 32 rows, 45 columns, 195 nonzeros\n",
      "Variable types: 0 continuous, 45 integer (45 binary)\n",
      "Found heuristic solution: objective 4.0000000\n",
      "\n",
      "Root relaxation: objective 3.000000e+00, 24 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "*    0     0               0       3.0000000    3.00000  0.00%     -    0s\n",
      "\n",
      "Explored 1 nodes (24 simplex iterations) in 0.03 seconds (0.00 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 3: 3 4 5 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 3.000000000000e+00, best bound 3.000000000000e+00, gap 0.0000%\n",
      "====================================================\n",
      "[[0 0 0]\n",
      " [0 1 3]\n",
      " [2 6 4]\n",
      " [7 8 5]]\n",
      "the objective function 3.0\n",
      "Gurobi Optimizer version 9.5.1 build v9.5.1rc2 (win64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 86 rows, 48 columns and 321 nonzeros\n",
      "Model fingerprint: 0xdcb80871\n",
      "Variable types: 0 continuous, 48 integer (48 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 4e+00]\n",
      "Found heuristic solution: objective 5.0000000\n",
      "Presolve removed 54 rows and 3 columns\n",
      "Presolve time: 0.00s\n",
      "Presolved: 32 rows, 45 columns, 204 nonzeros\n",
      "Variable types: 0 continuous, 45 integer (45 binary)\n",
      "\n",
      "Root relaxation: objective 3.000000e+00, 38 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "*    0     0               0       3.0000000    3.00000  0.00%     -    0s\n",
      "\n",
      "Explored 1 nodes (38 simplex iterations) in 0.02 seconds (0.00 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 2: 3 5 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 3.000000000000e+00, best bound 3.000000000000e+00, gap 0.0000%\n",
      "====================================================\n",
      "[[3 0 0]\n",
      " [4 0 0]\n",
      " [5 2 1]\n",
      " [6 8 7]]\n",
      "the objective function 3.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x720 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len= 2500\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(2,3,figsize=(20,10),tight_layout=True)\n",
    "for g in range(1,6):\n",
    "  prob=[]\n",
    "  ship_num=len(inst[2])\n",
    "  robust(ship_num,inst[0],inst[1],inst[2],g,size_,mean,cov)\n",
    "  prob+=list(penalty_r)\n",
    "  # print(prob)\n",
    "  # data+=(tuple(prob),)\n",
    "  # l.append(\"Gamma=\"+str(g))\n",
    "  # print(\"data=\",data)\n",
    "# plt.boxplot(data,labels=l)\n",
    "# plt.show()\n",
    "\n",
    "  ax[int(g/3)][g%3].hist(prob,bins=5,range=(0,5))\n",
    "  ax[int(g/3)][g%3].set_title(\"Gamma=\"+str(g))\n",
    "ax[0][0].hist(penalty_cvar,bins=5,range=(0,5))\n",
    "ax[0][0].set_title(\"75%-CVaR\")\n",
    "plt.show()\n",
    "print(\"len=\",len(prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [0 4 1]\n",
      " [2 5 3]\n",
      " [6 8 7]]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0bc652a6662848c169ddaad7e75fc7966486f1f662e7670e7ffb2b305ef6abae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
