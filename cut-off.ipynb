{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gurobipy import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本研究"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CVaR(ship_num,stack_num,height,n_init,size,size_,beta,mean,cov,OC): \n",
    "  O1=[i+1 for i in range(ship_num)]\n",
    "  S=[i+1 for i in range(stack_num)]\n",
    "  H=[i+1 for i in range(height)]\n",
    "  P=[i+1 for i in range(ship_num)]\n",
    "  f=stack_num*height-sum(n_init)\n",
    "\n",
    "  from asyncio.constants import SENDFILE_FALLBACK_READBUFFER_SIZE\n",
    "  import random\n",
    "  from random import seed\n",
    "  import numpy as np\n",
    "  from scipy.stats import multivariate_normal\n",
    "\n",
    "  # 期待値と分散共分散行列の準備\n",
    "  data_1 = np.random.multivariate_normal(mean, cov, size=size)\n",
    "\n",
    "  O_=np.argsort(data_1)\n",
    "  O_=O_+np.ones((size,ship_num)).astype(int)\n",
    "  global O\n",
    "  O=O_.tolist()\n",
    "  m=Model(\"CVaR\")\n",
    "\n",
    "  alpha=m.addVar(vtype=\"C\")\n",
    "\n",
    "  # 変数の定義\n",
    "  x,c,d={},{},{}\n",
    "  for s in S:\n",
    "    for h in H:\n",
    "      for p in P:\n",
    "        x[s,h,p]=m.addVar(vtype=\"B\")\n",
    "\n",
    "  for s in S:\n",
    "    for h in range(2,len(H)+1):\n",
    "      for i,o in enumerate(O):\n",
    "        c[s,h,i]=m.addVar(vtype=\"c\",lb=0)\n",
    "\n",
    "  for i in range(len(O)):\n",
    "    d[i]=m.addVar(vtype=\"C\",lb=0)\n",
    "\n",
    "  for p in P:\n",
    "    m.addConstr(quicksum(x[s,h,p] for s in S for h in H)==n_init[p-1])\n",
    "\n",
    "  for s in S:\n",
    "    for h in H:\n",
    "      m.addConstr(quicksum(x[s,h,p] for p in P)<=1)\n",
    "\n",
    "  for s in S:\n",
    "    for h in range(1,len(H)):\n",
    "      m.addConstr(quicksum(x[s,h+1,p] for p in P)<=quicksum(x[s,h,p] for p in P))\n",
    "\n",
    "  for s in S:\n",
    "    for h in range(2,len(H)+1):\n",
    "      for h_ in range(1,h):\n",
    "        for i,o in enumerate(OC):\n",
    "          for j,p in enumerate(o):\n",
    "            m.addConstr(c[s,h,i]>=quicksum(x[s,h,k] for k in o[j:])-quicksum(x[s,h_,k] for k in o[j:]))\n",
    "\n",
    "  for i in range(len(OC)):\n",
    "    m.addConstr(d[i]>=quicksum(c[s,h,i] for s in S for h in H if h!=1)-alpha)\n",
    "\n",
    "\n",
    "  m.setObjective(alpha+quicksum(d[i] for i in range(len(O)))/((1-beta)*len(O)))\n",
    "\n",
    "  if f>=height:\n",
    "    m.optimize()\n",
    "  else:\n",
    "    print(\"f<h\")\n",
    "  # m.optimize()\n",
    "\n",
    "  print(\"================================================\")\n",
    "\n",
    "  EPS=1.e-6\n",
    "\n",
    "  if m.Status == GRB.OPTIMAL:\n",
    "\n",
    "    global result\n",
    "    result=np.zeros((height,stack_num))\n",
    "    for (s,h,p) in x:\n",
    "      if x[s,h,p].X>EPS:\n",
    "        result[height-h][s-1]=int(p)\n",
    "\n",
    "    result=result.astype(int)\n",
    "    # print(\"VaR=\",alpha.X)\n",
    "    # print(\"the objective function\", m.objVal)\n",
    "    global LB\n",
    "    LB=m.objVal\n",
    "    # print(result)\n",
    "\n",
    "    global penalty\n",
    "    penalty=[]\n",
    "    for k in O:\n",
    "      OO=k\n",
    "      a=0\n",
    "      for j in range(stack_num):\n",
    "          for i in range(1,height):\n",
    "              for i_ in range(i+1,height+1):\n",
    "                  if result[i-1][j]!=0 and result[i_-1][j]!=0:\n",
    "                      if OO.index(result[i-1][j])>OO.index(result[i_-1][j]):\n",
    "                          # print(j+1,height-i+1,O)\n",
    "                          a+=1\n",
    "                          # print(\"penalty!\")\n",
    "                              # print(i,i_,j+1,O)\n",
    "                          break\n",
    "      penalty.append(a)\n",
    "      # print(a)\n",
    "    penalty=np.sort(penalty)\n",
    "    # print(penalty)\n",
    "\n",
    "    # penalty=penalty[round(0.8*size):]\n",
    "    # print(penalty)\n",
    "\n",
    "def robust(ship_num,stack_num,height,n,Gamma,size,size_,mean,cov):\n",
    "  # nと初期配置を変更しなければいけない\n",
    "  num=sum(n)\n",
    "\n",
    "  import numpy as np\n",
    "\n",
    "  O1=[i+1 for i in range(ship_num)]\n",
    "\n",
    "  Q=[i+1 for i in range(stack_num)]\n",
    "  L=[i+1 for i in range(height)]\n",
    "  P=[i+1 for i in range(ship_num)]\n",
    "  I=[i+1 for i in range(num)]\n",
    "  f=stack_num*height-len(I)\n",
    "  a=1\n",
    "  gamma=[]\n",
    "  for i in n:\n",
    "      for j in range(1,i+1):\n",
    "          gamma.append(a)\n",
    "      a+=1\n",
    "  \n",
    "  m=Model(\"BI\")\n",
    "\n",
    "  # 変数の定義\n",
    "  alpha,beta={},{}\n",
    "  for i in I:\n",
    "      for q in Q:\n",
    "          alpha[i,q]=m.addVar(vtype=\"B\")\n",
    "          beta[i,q]=m.addVar(vtype=\"B\")\n",
    "  J=[]\n",
    "  for i in I:\n",
    "      J.append([])\n",
    "      for j in I:\n",
    "          if gamma[i-1]<gamma[j-1]:\n",
    "              if gamma[j-1]-gamma[i-1]<=Gamma:\n",
    "                  J[i-1].append(j)\n",
    "\n",
    "  for q in Q:\n",
    "      m.addConstr(quicksum((alpha[i,q]+beta[i,q]) for i in I)<=len(L))\n",
    "\n",
    "  for i in I:\n",
    "      m.addConstr(quicksum((alpha[i,q]+beta[i,q]) for q in Q)==1)\n",
    "\n",
    "  for i in I:\n",
    "      for j in J[i-1]:\n",
    "          for q in Q:\n",
    "              m.addConstr(alpha[i,q]+alpha[j,q]+beta[j,q]<=1)\n",
    "\n",
    "  m.setObjective(quicksum(beta[i,q] for i in I for q in Q))\n",
    "\n",
    "  if f>=height:\n",
    "      m.optimize()\n",
    "\n",
    "  EPS=1.e-6\n",
    "\n",
    "  if m.Status == GRB.OPTIMAL:\n",
    "      print(\"====================================================\")\n",
    "\n",
    "      EPS=1.e-6\n",
    "      a=[]\n",
    "      for q in Q:\n",
    "          a.append([])\n",
    "      for (i,q) in alpha:\n",
    "          if alpha[i,q].X>EPS:\n",
    "              a[q-1].append(gamma[i-1])\n",
    "      \n",
    "      for (i,q) in beta:\n",
    "          if beta[i,q].X>EPS:\n",
    "              a[q-1].append(gamma[i-1])\n",
    "\n",
    "      for q in Q:\n",
    "          a[q-1]=sorted(a[q-1],reverse=True)\n",
    "\n",
    "      global result_r\n",
    "      result_r=np.zeros((height,stack_num))\n",
    "      for q in Q:\n",
    "          for i,r in enumerate(a[q-1]):\n",
    "              result_r[height-i-1][q-1]=r\n",
    "          # print(i,r)\n",
    "\n",
    "      result_r=result_r.astype(int)\n",
    "      \n",
    "      print(result_r)\n",
    "      print(\"the objective function\", m.objVal)\n",
    "\n",
    "      from asyncio.constants import SENDFILE_FALLBACK_READBUFFER_SIZE\n",
    "      import random\n",
    "      from random import seed\n",
    "      import numpy as np\n",
    "      from scipy.stats import multivariate_normal\n",
    "\n",
    "      np.random.seed()\n",
    "      data_1 = np.random.multivariate_normal(mean, cov, size=size_)\n",
    "\n",
    "      O_=np.argsort(data_1)\n",
    "      O_=O_+np.ones((size_,ship_num)).astype(int)\n",
    "      OR=O_.tolist()\n",
    "\n",
    "\n",
    "      global penalty_r\n",
    "      penalty_r=[]\n",
    "      for k in OR:\n",
    "          O=k\n",
    "          a=0\n",
    "          for j in range(stack_num):\n",
    "              for i in range(1,height):\n",
    "                  for i_ in range(i+1,height+1):\n",
    "                      if result_r[i-1][j]!=0 and result_r[i_-1][j]!=0:\n",
    "                          if O.index(result_r[i-1][j])>O.index(result_r[i_-1][j]):\n",
    "                              a+=1\n",
    "                              # if Gamma ==2:\n",
    "                                # print(j+1,height-i+1,O)\n",
    "                              # print(\"penalty!\")\n",
    "                              # print(i,i_,j+1,O)\n",
    "                              break\n",
    "          penalty_r.append(a)\n",
    "      \n",
    "      penalty_r=np.sort(penalty_r)\n",
    "      penalty_r=penalty_r[round(0.75*size_):]\n",
    "      # print(penalty_r)\n",
    "\n",
    "      # import matplotlib.pyplot as plt\n",
    "      # plt.boxplot(penalty_r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2023-09-30\n",
      "Gurobi Optimizer version 9.5.1 build v9.5.1rc2 (win64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 34 rows, 39097 columns and 336 nonzeros\n",
      "Model fingerprint: 0xf8619215\n",
      "Variable types: 39001 continuous, 96 integer (96 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [1e-03, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 2e+00]\n",
      "Found heuristic solution: objective 0.0000000\n",
      "\n",
      "Explored 0 nodes (0 simplex iterations) in 0.02 seconds (0.00 work units)\n",
      "Thread count was 1 (of 8 available processors)\n",
      "\n",
      "Solution count 1: 0 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 0.000000000000e+00, best bound 0.000000000000e+00, gap 0.0000%\n",
      "================================================\n",
      "Gurobi Optimizer version 9.5.1 build v9.5.1rc2 (win64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 43534 rows, 39097 columns and 350136 nonzeros\n",
      "Model fingerprint: 0xc0eccd2b\n",
      "Variable types: 39001 continuous, 96 integer (96 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [1e-03, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 2e+00]\n",
      "Presolve removed 28380 rows and 37741 columns\n",
      "Presolve time: 1.13s\n",
      "Presolved: 15154 rows, 1356 columns, 121296 nonzeros\n",
      "Variable types: 1260 continuous, 96 integer (96 binary)\n",
      "Found heuristic solution: objective 1.8133333\n",
      "\n",
      "Use crossover to convert LP symmetric solution to basic solution...\n",
      "\n",
      "Root relaxation: objective 0.000000e+00, 159 iterations, 0.11 seconds (0.08 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0    0.00000    0   26    1.81333    0.00000   100%     -    1s\n",
      "H    0     0                       0.8133333    0.00000   100%     -    1s\n",
      "     0     0    0.00000    0   29    0.81333    0.00000   100%     -    1s\n",
      "H    0     0                       0.3013333    0.00000   100%     -    3s\n",
      "     0     0    0.00000    0   20    0.30133    0.00000   100%     -    3s\n",
      "     0     0    0.00000    0   20    0.30133    0.00000   100%     -    3s\n",
      "     0     0    0.00000    0   20    0.30133    0.00000   100%     -    4s\n",
      "     0     0    0.00133    0   23    0.30133    0.00133   100%     -    5s\n",
      "     0     0    0.00133    0   22    0.30133    0.00133   100%     -    5s\n",
      "     0     0    0.00267    0   33    0.30133    0.00267  99.1%     -    5s\n",
      "     0     0    0.00267    0   33    0.30133    0.00267  99.1%     -    5s\n",
      "     0     0    0.00267    0   31    0.30133    0.00267  99.1%     -    5s\n",
      "H    0     0                       0.1413333    0.00267  98.1%     -    5s\n",
      "H    0     0                       0.0640000    0.00267  95.8%     -    6s\n",
      "     0     0    0.00267    0   24    0.06400    0.00267  95.8%     -    6s\n",
      "     0     0    0.00267    0   21    0.06400    0.00267  95.8%     -    6s\n",
      "     0     0    0.00267    0   20    0.06400    0.00267  95.8%     -    7s\n",
      "     0     0    0.00267    0   24    0.06400    0.00267  95.8%     -    7s\n",
      "     0     0    0.00267    0   28    0.06400    0.00267  95.8%     -    8s\n",
      "     0     0    0.00267    0   30    0.06400    0.00267  95.8%     -    8s\n",
      "     0     0    0.00267    0   30    0.06400    0.00267  95.8%     -    9s\n",
      "     0     2    0.00267    0   26    0.06400    0.00267  95.8%     -    9s\n",
      "     1     4    0.00900    1   34    0.06400    0.00267  95.8%   450   10s\n",
      "\n",
      "Cutting planes:\n",
      "  RLT: 45\n",
      "\n",
      "Explored 17 nodes (7485 simplex iterations) in 12.95 seconds (6.75 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 5: 0.064 0.141333 0.301333 ... 1.81333\n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 6.400000000000e-02, best bound 6.400000000000e-02, gap 0.0000%\n",
      "================================================\n",
      "Gurobi Optimizer version 9.5.1 build v9.5.1rc2 (win64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 87034 rows, 39097 columns and 699936 nonzeros\n",
      "Model fingerprint: 0x79b61816\n",
      "Variable types: 39001 continuous, 96 integer (96 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [1e-03, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 2e+00]\n",
      "Presolve removed 67416 rows and 37369 columns\n",
      "Presolve time: 2.25s\n",
      "Presolved: 19618 rows, 1728 columns, 157008 nonzeros\n",
      "Variable types: 1632 continuous, 96 integer (96 binary)\n",
      "Found heuristic solution: objective 3.6013333\n",
      "\n",
      "Use crossover to convert LP symmetric solution to basic solution...\n",
      "\n",
      "Root relaxation: objective 0.000000e+00, 260 iterations, 0.39 seconds (0.12 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0    0.00000    0   16    3.60133    0.00000   100%     -    3s\n",
      "H    0     0                       2.6146667    0.00000   100%     -    3s\n",
      "H    0     0                       2.1093333    0.00000   100%     -    3s\n",
      "     0     0    0.00000    0   30    2.10933    0.00000   100%     -    4s\n",
      "H    0     0                       0.8920000    0.00000   100%     -    5s\n",
      "     0     0    0.00000    0   16    0.89200    0.00000   100%     -    6s\n",
      "     0     0    0.00000    0   16    0.89200    0.00000   100%     -    7s\n",
      "     0     0    0.00000    0   16    0.89200    0.00000   100%     -    8s\n",
      "     0     0    0.00000    0   20    0.89200    0.00000   100%     -    9s\n",
      "     0     0    0.00133    0   24    0.89200    0.00133   100%     -   10s\n",
      "     0     0    0.00267    0   24    0.89200    0.00267   100%     -   10s\n",
      "     0     0    0.00267    0   24    0.89200    0.00267   100%     -   10s\n",
      "     0     0    0.00267    0   24    0.89200    0.00267   100%     -   11s\n",
      "     0     0    0.00267    0   26    0.89200    0.00267   100%     -   12s\n",
      "H    0     0                       0.3653333    0.00267  99.3%     -   12s\n",
      "H    0     0                       0.1226667    0.00267  97.8%     -   13s\n",
      "     0     0    0.00267    0   26    0.12267    0.00267  97.8%     -   13s\n",
      "     0     0    0.00267    0   26    0.12267    0.00267  97.8%     -   13s\n",
      "     0     2    0.00267    0   26    0.12267    0.00267  97.8%     -   14s\n",
      "     3     4    0.00667    2    8    0.12267    0.00533  95.7%   124   16s\n",
      "\n",
      "Cutting planes:\n",
      "  Gomory: 2\n",
      "  MIR: 1\n",
      "  RLT: 36\n",
      "\n",
      "Explored 11 nodes (6846 simplex iterations) in 17.56 seconds (9.05 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 6: 0.122667 0.365333 0.892 ... 3.60133\n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 1.226666666667e-01, best bound 1.226666666667e-01, gap 0.0000%\n",
      "================================================\n",
      "Gurobi Optimizer version 9.5.1 build v9.5.1rc2 (win64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 130534 rows, 39097 columns and 1049736 nonzeros\n",
      "Model fingerprint: 0xb1a67ff9\n",
      "Variable types: 39001 continuous, 96 integer (96 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [1e-03, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 2e+00]\n",
      "Presolve removed 106430 rows and 36842 columns\n",
      "Presolve time: 2.99s\n",
      "Presolved: 24104 rows, 2255 columns, 193892 nonzeros\n",
      "Variable types: 2159 continuous, 96 integer (96 binary)\n",
      "Found heuristic solution: objective 4.9493333\n",
      "\n",
      "Use crossover to convert LP symmetric solution to basic solution...\n",
      "\n",
      "Root relaxation: objective 0.000000e+00, 451 iterations, 0.24 seconds (0.14 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0    0.00000    0   16    4.94933    0.00000   100%     -    3s\n",
      "H    0     0                       3.7306667    0.00000   100%     -    4s\n",
      "H    0     0                       2.2800000    0.00000   100%     -    5s\n",
      "H    0     0                       1.8613333    0.00000   100%     -    5s\n",
      "     0     0    0.00000    0   27    1.86133    0.00000   100%     -    5s\n",
      "     0     0    0.00000    0   16    1.86133    0.00000   100%     -    8s\n",
      "     0     0    0.00000    0   24    1.86133    0.00000   100%     -    8s\n",
      "H    0     0                       1.5146667    0.00000   100%     -   10s\n",
      "H    0     0                       1.1626667    0.00000   100%     -   10s\n",
      "     0     0    0.00000    0   20    1.16267    0.00000   100%     -   10s\n",
      "H    0     0                       0.5306667    0.00000   100%     -   11s\n",
      "H    0     0                       0.5200000    0.00133   100%     -   12s\n",
      "     0     0    0.00133    0   20    0.52000    0.00133   100%     -   12s\n",
      "H    0     0                       0.2240000    0.00267  98.8%     -   13s\n",
      "     0     0    0.00267    0   16    0.22400    0.00267  98.8%     -   14s\n",
      "     0     0    0.00267    0   16    0.22400    0.00267  98.8%     -   14s\n",
      "     0     0    0.00533    0   20    0.22400    0.00533  97.6%     -   16s\n",
      "     0     0    0.00533    0   20    0.22400    0.00533  97.6%     -   16s\n",
      "     0     0    0.00933    0   20    0.22400    0.00933  95.8%     -   18s\n",
      "     0     0    0.00933    0   28    0.22400    0.00933  95.8%     -   18s\n",
      "     0     0    0.00933    0   29    0.22400    0.00933  95.8%     -   19s\n",
      "     0     0    0.00933    0   27    0.22400    0.00933  95.8%     -   19s\n",
      "     0     0    0.00933    0   21    0.22400    0.00933  95.8%     -   20s\n",
      "     0     0    0.00933    0   21    0.22400    0.00933  95.8%     -   21s\n",
      "     0     0    0.00933    0   16    0.22400    0.00933  95.8%     -   22s\n",
      "     0     0    0.00933    0   20    0.22400    0.00933  95.8%     -   23s\n",
      "     0     0    0.00933    0   16    0.22400    0.00933  95.8%     -   23s\n",
      "     0     2    0.00933    0   16    0.22400    0.00933  95.8%     -   24s\n",
      "     1     2     cutoff    1         0.22400    0.00940  95.8%  4921   27s\n",
      "     5     4     cutoff    3         0.22400    0.01600  92.9%  1269   30s\n",
      "\n",
      "Cutting planes:\n",
      "  RLT: 67\n",
      "\n",
      "Explored 13 nodes (19113 simplex iterations) in 31.41 seconds (27.57 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 9: 0.224 0.52 0.530667 ... 4.94933\n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 2.240000000000e-01, best bound 2.240000000000e-01, gap 0.0000%\n",
      "================================================\n",
      "Gurobi Optimizer version 9.5.1 build v9.5.1rc2 (win64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 174034 rows, 39097 columns and 1399536 nonzeros\n",
      "Model fingerprint: 0x2b752aeb\n",
      "Variable types: 39001 continuous, 96 integer (96 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [1e-03, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 2e+00]\n",
      "Presolve removed 147320 rows and 36608 columns\n",
      "Presolve time: 3.79s\n",
      "Presolved: 26714 rows, 2489 columns, 214880 nonzeros\n",
      "Variable types: 2393 continuous, 96 integer (96 binary)\n",
      "Found heuristic solution: objective 5.3386667\n",
      "\n",
      "Use crossover to convert LP symmetric solution to basic solution...\n",
      "\n",
      "Root relaxation: objective 0.000000e+00, 420 iterations, 0.30 seconds (0.15 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0    0.00000    0   16    5.33867    0.00000   100%     -    5s\n",
      "H    0     0                       4.0000000    0.00000   100%     -    5s\n",
      "H    0     0                       1.4626667    0.00000   100%     -    6s\n",
      "     0     0    0.00000    0   24    1.46267    0.00000   100%     -    6s\n",
      "     0     0    0.00000    0   16    1.46267    0.00000   100%     -    7s\n",
      "     0     0    0.00000    0   16    1.46267    0.00000   100%     -    9s\n",
      "H    0     0                       1.1666667    0.00000   100%     -   10s\n",
      "     0     0    0.00000    0   29    1.16667    0.00000   100%     -   10s\n",
      "     0     0    0.00000    0   28    1.16667    0.00000   100%     -   13s\n",
      "     0     0    0.00000    0   24    1.16667    0.00000   100%     -   14s\n",
      "     0     0    0.00222    0   20    1.16667    0.00222   100%     -   16s\n",
      "     0     0    0.00456    0   28    1.16667    0.00456   100%     -   17s\n",
      "     0     0    0.00933    0   16    1.16667    0.00933  99.2%     -   19s\n",
      "H    0     0                       0.7866667    0.00933  98.8%     -   19s\n",
      "     0     0    0.00933    0   20    0.78667    0.00933  98.8%     -   20s\n",
      "     0     0    0.00933    0   20    0.78667    0.00933  98.8%     -   20s\n",
      "     0     0    0.00933    0   20    0.78667    0.00933  98.8%     -   21s\n",
      "     0     0    0.00933    0   26    0.78667    0.00933  98.8%     -   21s\n",
      "     0     0    0.00978    0   24    0.78667    0.00978  98.8%     -   22s\n",
      "     0     0    0.00978    0   16    0.78667    0.00978  98.8%     -   22s\n",
      "     0     0    0.00978    0   34    0.78667    0.00978  98.8%     -   23s\n",
      "     0     0    0.00978    0   30    0.78667    0.00978  98.8%     -   23s\n",
      "     0     0    0.01096    0   28    0.78667    0.01096  98.6%     -   24s\n",
      "     0     0    0.01111    0   24    0.78667    0.01111  98.6%     -   25s\n",
      "     0     0    0.01289    0   20    0.78667    0.01289  98.4%     -   26s\n",
      "     0     0    0.01311    0   20    0.78667    0.01311  98.3%     -   26s\n",
      "     0     0    0.01437    0   24    0.78667    0.01437  98.2%     -   27s\n",
      "     0     0    0.01437    0   24    0.78667    0.01437  98.2%     -   27s\n",
      "     0     0    0.01600    0   16    0.78667    0.01600  98.0%     -   29s\n",
      "     0     0    0.01667    0   20    0.78667    0.01667  97.9%     -   29s\n",
      "     0     0    0.01733    0   20    0.78667    0.01733  97.8%     -   30s\n",
      "     0     0    0.01733    0   20    0.78667    0.01733  97.8%     -   31s\n",
      "     0     0    0.01794    0   56    0.78667    0.01794  97.7%     -   32s\n",
      "     0     0    0.01800    0   55    0.78667    0.01800  97.7%     -   32s\n",
      "     0     0    0.01803    0   41    0.78667    0.01803  97.7%     -   34s\n",
      "     0     0    0.01816    0   34    0.78667    0.01816  97.7%     -   34s\n",
      "H    0     0                       0.4640000    0.01867  96.0%     -   35s\n",
      "     0     0    0.01867    0   50    0.46400    0.01867  96.0%     -   35s\n",
      "     0     0    0.01867    0   50    0.46400    0.01867  96.0%     -   36s\n",
      "     0     0    0.01867    0   40    0.46400    0.01867  96.0%     -   37s\n",
      "     0     0    0.01933    0   32    0.46400    0.01933  95.8%     -   38s\n",
      "     0     0    0.01969    0   54    0.46400    0.01969  95.8%     -   40s\n",
      "     0     0    0.01970    0   50    0.46400    0.01970  95.8%     -   41s\n",
      "     0     0    0.01975    0   46    0.46400    0.01975  95.7%     -   42s\n",
      "     0     0    0.02022    0   46    0.46400    0.02022  95.6%     -   42s\n",
      "H    0     0                       0.4213333    0.02022  95.2%     -   43s\n",
      "     0     0    0.02022    0   51    0.42133    0.02022  95.2%     -   43s\n",
      "     0     0    0.02024    0   46    0.42133    0.02024  95.2%     -   44s\n",
      "     0     0    0.02033    0   46    0.42133    0.02033  95.2%     -   45s\n",
      "     0     0    0.02035    0   46    0.42133    0.02035  95.2%     -   46s\n",
      "     0     0    0.02050    0   44    0.42133    0.02050  95.1%     -   47s\n",
      "     0     0    0.02050    0   45    0.42133    0.02050  95.1%     -   47s\n",
      "     0     0    0.02052    0   44    0.42133    0.02052  95.1%     -   48s\n",
      "     0     0    0.02052    0   44    0.42133    0.02052  95.1%     -   49s\n",
      "     0     0    0.02133    0   52    0.42133    0.02133  94.9%     -   50s\n",
      "     0     0    0.02133    0   40    0.42133    0.02133  94.9%     -   51s\n",
      "     0     0    0.02133    0   46    0.42133    0.02133  94.9%     -   52s\n",
      "     0     0    0.02133    0   43    0.42133    0.02133  94.9%     -   53s\n",
      "H    0     0                       0.3120000    0.02167  93.1%     -   55s\n",
      "     0     0    0.02167    0   38    0.31200    0.02167  93.1%     -   55s\n",
      "     0     0    0.02167    0   40    0.31200    0.02167  93.1%     -   56s\n",
      "     0     0    0.02200    0   47    0.31200    0.02200  92.9%     -   57s\n",
      "     0     0    0.02267    0   40    0.31200    0.02267  92.7%     -   58s\n",
      "     0     0    0.02300    0   44    0.31200    0.02300  92.6%     -   59s\n",
      "     0     0    0.02307    0   45    0.31200    0.02307  92.6%     -   59s\n",
      "     0     0    0.02367    0   41    0.31200    0.02367  92.4%     -   61s\n",
      "     0     0    0.02367    0   43    0.31200    0.02367  92.4%     -   61s\n",
      "     0     0    0.02367    0   41    0.31200    0.02367  92.4%     -   62s\n",
      "     0     0    0.02369    0   49    0.31200    0.02369  92.4%     -   62s\n",
      "     0     0    0.02400    0   41    0.31200    0.02400  92.3%     -   64s\n",
      "     0     0    0.02400    0   40    0.31200    0.02400  92.3%     -   64s\n",
      "     0     0    0.02400    0   40    0.31200    0.02400  92.3%     -   65s\n",
      "     0     0    0.02415    0   48    0.31200    0.02415  92.3%     -   67s\n",
      "     0     0    0.02416    0   48    0.31200    0.02416  92.3%     -   67s\n",
      "     0     0    0.02441    0   49    0.31200    0.02441  92.2%     -   68s\n",
      "     0     0    0.02442    0   49    0.31200    0.02442  92.2%     -   69s\n",
      "     0     0    0.02600    0   55    0.31200    0.02600  91.7%     -   70s\n",
      "     0     0    0.02644    0   55    0.31200    0.02644  91.5%     -   71s\n",
      "     0     0    0.02656    0   56    0.31200    0.02656  91.5%     -   72s\n",
      "     0     0    0.02656    0   56    0.31200    0.02656  91.5%     -   73s\n",
      "     0     0    0.02659    0   56    0.31200    0.02659  91.5%     -   73s\n",
      "     0     0    0.02667    0   55    0.31200    0.02667  91.5%     -   75s\n",
      "     0     0    0.02667    0   55    0.31200    0.02667  91.5%     -   75s\n",
      "     0     0    0.02667    0   48    0.31200    0.02667  91.5%     -   76s\n",
      "     0     0    0.02667    0   48    0.31200    0.02667  91.5%     -   77s\n",
      "     0     2    0.02667    0   48    0.31200    0.02667  91.5%     -   78s\n",
      "     1     2     cutoff    1         0.31200    0.02667  91.5% 21285   93s\n",
      "     5     8    0.29741    3   31    0.31200    0.04004  87.2%  4990   96s\n",
      "    17     2     cutoff    5         0.31200    0.04089  86.9%  1842  100s\n",
      "\n",
      "Cutting planes:\n",
      "  MIR: 233\n",
      "  RLT: 192\n",
      "\n",
      "Explored 25 nodes (52323 simplex iterations) in 100.51 seconds (109.72 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 8: 0.312 0.421333 0.464 ... 5.33867\n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 3.120000000000e-01, best bound 3.120000000000e-01, gap 0.0000%\n",
      "================================================\n",
      "Gurobi Optimizer version 9.5.1 build v9.5.1rc2 (win64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 217534 rows, 39097 columns and 1749336 nonzeros\n",
      "Model fingerprint: 0xb3dc4fbc\n",
      "Variable types: 39001 continuous, 96 integer (96 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [1e-03, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 2e+00]\n",
      "Presolve removed 188500 rows and 36400 columns\n",
      "Presolve time: 4.90s\n",
      "Presolved: 29034 rows, 2697 columns, 233536 nonzeros\n",
      "Variable types: 2601 continuous, 96 integer (96 binary)\n",
      "Found heuristic solution: objective 5.6573333\n",
      "\n",
      "Root simplex log...\n",
      "\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0      handle free variables                          6s\n",
      "     357   -0.0000000e+00   0.000000e+00   0.000000e+00      6s\n",
      "     357   -0.0000000e+00   0.000000e+00   0.000000e+00      6s\n",
      "     357    0.0000000e+00   0.000000e+00   0.000000e+00      6s\n",
      "\n",
      "Use crossover to convert LP symmetric solution to basic solution...\n",
      "\n",
      "Root crossover log...\n",
      "\n",
      "      74 PPushes remaining with PInf 0.0000000e+00                 6s\n",
      "       0 PPushes remaining with PInf 0.0000000e+00                 6s\n",
      "\n",
      "  Push phase complete: Pinf 0.0000000e+00, Dinf 0.0000000e+00      6s\n",
      "\n",
      "\n",
      "Root simplex log...\n",
      "\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "     434    0.0000000e+00   0.000000e+00   0.000000e+00      6s\n",
      "\n",
      "Root relaxation: objective 0.000000e+00, 434 iterations, 0.37 seconds (0.16 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0    0.00000    0   16    5.65733    0.00000   100%     -    6s\n",
      "H    0     0                       4.0000000    0.00000   100%     -    6s\n",
      "H    0     0                       3.2200000    0.00000   100%     -    7s\n",
      "     0     0    0.00000    0   27    3.22000    0.00000   100%     -    8s\n",
      "H    0     0                       2.5733333    0.00000   100%     -    9s\n",
      "H    0     0                       1.7440000    0.00000   100%     -   11s\n",
      "H    0     0                       1.0600000    0.00000   100%     -   15s\n",
      "     0     0    0.00000    0   16    1.06000    0.00000   100%     -   16s\n",
      "     0     0    0.00000    0   27    1.06000    0.00000   100%     -   17s\n",
      "     0     0    0.00000    0   16    1.06000    0.00000   100%     -   20s\n",
      "     0     0    0.00000    0   24    1.06000    0.00000   100%     -   21s\n",
      "     0     0    0.00733    0   20    1.06000    0.00733  99.3%     -   24s\n",
      "     0     0    0.00933    0   24    1.06000    0.00933  99.1%     -   26s\n",
      "     0     0    0.00933    0   20    1.06000    0.00933  99.1%     -   27s\n",
      "     0     0    0.00933    0   20    1.06000    0.00933  99.1%     -   27s\n",
      "     0     0    0.00933    0   20    1.06000    0.00933  99.1%     -   28s\n",
      "     0     0    0.00933    0   20    1.06000    0.00933  99.1%     -   28s\n",
      "     0     0    0.00933    0   16    1.06000    0.00933  99.1%     -   30s\n",
      "     0     0    0.00933    0   16    1.06000    0.00933  99.1%     -   30s\n",
      "     0     0    0.00933    0   20    1.06000    0.00933  99.1%     -   31s\n",
      "     0     0    0.00933    0   26    1.06000    0.00933  99.1%     -   32s\n",
      "     0     0    0.00978    0   24    1.06000    0.00978  99.1%     -   33s\n",
      "     0     0    0.00978    0   24    1.06000    0.00978  99.1%     -   34s\n",
      "     0     0    0.01133    0   24    1.06000    0.01133  98.9%     -   37s\n",
      "     0     0    0.01148    0   28    1.06000    0.01148  98.9%     -   37s\n",
      "     0     0    0.01333    0   24    1.06000    0.01333  98.7%     -   39s\n",
      "     0     0    0.01363    0   20    1.06000    0.01363  98.7%     -   39s\n",
      "     0     0    0.01778    0   32    1.06000    0.01778  98.3%     -   42s\n",
      "     0     0    0.01822    0   25    1.06000    0.01822  98.3%     -   44s\n",
      "     0     0    0.01822    0   24    1.06000    0.01822  98.3%     -   45s\n",
      "     0     0    0.01822    0   29    1.06000    0.01822  98.3%     -   45s\n",
      "     0     0    0.01822    0   29    1.06000    0.01822  98.3%     -   46s\n",
      "     0     0    0.01901    0   52    1.06000    0.01901  98.2%     -   49s\n",
      "H    0     0                       0.9893333    0.01901  98.1%     -   50s\n",
      "     0     0    0.01933    0   62    0.98933    0.01933  98.0%     -   51s\n",
      "     0     0    0.01959    0   60    0.98933    0.01959  98.0%     -   53s\n",
      "H    0     0                       0.3893333    0.01959  95.0%     -   53s\n",
      "     0     0    0.02005    0   62    0.38933    0.02005  94.8%     -   54s\n",
      "     0     0    0.02121    0   58    0.38933    0.02121  94.6%     -   56s\n",
      "     0     0    0.02128    0   62    0.38933    0.02128  94.5%     -   57s\n",
      "     0     0    0.02178    0   60    0.38933    0.02178  94.4%     -   58s\n",
      "     0     0    0.02178    0   56    0.38933    0.02178  94.4%     -   59s\n",
      "     0     0    0.02178    0   54    0.38933    0.02178  94.4%     -   60s\n",
      "     0     0    0.02195    0   54    0.38933    0.02195  94.4%     -   61s\n",
      "     0     0    0.02267    0   32    0.38933    0.02267  94.2%     -   64s\n",
      "     0     0    0.02267    0   40    0.38933    0.02267  94.2%     -   64s\n",
      "     0     0    0.02267    0   40    0.38933    0.02267  94.2%     -   65s\n",
      "     0     0    0.02289    0   40    0.38933    0.02289  94.1%     -   65s\n",
      "     0     0    0.02294    0   53    0.38933    0.02294  94.1%     -   67s\n",
      "     0     0    0.02295    0   57    0.38933    0.02295  94.1%     -   67s\n",
      "     0     0    0.02311    0   40    0.38933    0.02311  94.1%     -   69s\n",
      "     0     0    0.02311    0   40    0.38933    0.02311  94.1%     -   69s\n",
      "     0     0    0.02311    0   40    0.38933    0.02311  94.1%     -   70s\n",
      "     0     0    0.02422    0   40    0.38933    0.02422  93.8%     -   72s\n",
      "     0     0    0.02470    0   54    0.38933    0.02470  93.7%     -   73s\n",
      "     0     0    0.02489    0   40    0.38933    0.02489  93.6%     -   75s\n",
      "     0     0    0.02515    0   53    0.38933    0.02515  93.5%     -   75s\n",
      "     0     0    0.02571    0   56    0.38933    0.02571  93.4%     -   77s\n",
      "     0     0    0.02609    0   51    0.38933    0.02609  93.3%     -   78s\n",
      "     0     0    0.02667    0   46    0.38933    0.02667  93.2%     -   80s\n",
      "     0     0    0.02667    0   48    0.38933    0.02667  93.2%     -   80s\n",
      "     0     0    0.02667    0   40    0.38933    0.02667  93.2%     -   82s\n",
      "     0     0    0.02689    0   42    0.38933    0.02689  93.1%     -   84s\n",
      "     0     0    0.02689    0   42    0.38933    0.02689  93.1%     -   84s\n",
      "     0     0    0.02797    0   54    0.38933    0.02797  92.8%     -   85s\n",
      "     0     0    0.02827    0   46    0.38933    0.02827  92.7%     -   86s\n",
      "     0     0    0.02933    0   40    0.38933    0.02933  92.5%     -   87s\n",
      "     0     0    0.02990    0   53    0.38933    0.02990  92.3%     -   89s\n",
      "     0     0    0.03100    0   53    0.38933    0.03100  92.0%     -   90s\n",
      "     0     0    0.03126    0   56    0.38933    0.03126  92.0%     -   91s\n",
      "     0     0    0.03135    0   55    0.38933    0.03135  91.9%     -   92s\n",
      "     0     0    0.03152    0   55    0.38933    0.03152  91.9%     -   93s\n",
      "     0     0    0.03188    0   52    0.38933    0.03188  91.8%     -   94s\n",
      "     0     0    0.03200    0   51    0.38933    0.03200  91.8%     -   95s\n",
      "     0     0    0.03200    0   51    0.38933    0.03200  91.8%     -   96s\n",
      "     0     0    0.03208    0   54    0.38933    0.03208  91.8%     -   96s\n",
      "     0     0    0.03217    0   54    0.38933    0.03217  91.7%     -   97s\n",
      "     0     0    0.03242    0   56    0.38933    0.03242  91.7%     -   99s\n",
      "     0     0    0.03244    0   51    0.38933    0.03244  91.7%     -   99s\n",
      "     0     0    0.03333    0   48    0.38933    0.03333  91.4%     -  100s\n",
      "     0     0    0.03333    0   48    0.38933    0.03333  91.4%     -  101s\n",
      "     0     2    0.03333    0   48    0.38933    0.03333  91.4%     -  102s\n",
      "     1     2     cutoff    1         0.38933    0.03333  91.4% 26641  122s\n",
      "     5     6    0.38028    3   28    0.38933    0.05600  85.6%  6095  125s\n",
      "    13     2     cutoff    5         0.38933    0.05600  85.6%  2700  136s\n",
      "    21     2    0.26352    6   20    0.38933    0.08000  79.5%  2347  140s\n",
      "\n",
      "Cutting planes:\n",
      "  MIR: 197\n",
      "  RLT: 213\n",
      "\n",
      "Explored 27 nodes (63290 simplex iterations) in 140.27 seconds (142.13 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 8: 0.389333 0.989333 1.06 ... 5.65733\n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 3.893333333332e-01, best bound 3.893333333332e-01, gap 0.0000%\n",
      "================================================\n",
      "Gurobi Optimizer version 9.5.1 build v9.5.1rc2 (win64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 261034 rows, 39097 columns and 2099136 nonzeros\n",
      "Model fingerprint: 0xecb67225\n",
      "Variable types: 39001 continuous, 96 integer (96 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [1e-03, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 2e+00]\n",
      "Presolve removed 230260 rows and 36244 columns (presolve time = 5s) ...\n",
      "Presolve removed 230260 rows and 36244 columns\n",
      "Presolve time: 5.95s\n",
      "Presolved: 30774 rows, 2853 columns, 247528 nonzeros\n",
      "Variable types: 2757 continuous, 96 integer (96 binary)\n",
      "Found heuristic solution: objective 5.8373333\n",
      "\n",
      "Root simplex log...\n",
      "\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0      handle free variables                          7s\n",
      "     341   -0.0000000e+00   0.000000e+00   0.000000e+00      7s\n",
      "     341   -0.0000000e+00   0.000000e+00   0.000000e+00      7s\n",
      "     341    0.0000000e+00   0.000000e+00   0.000000e+00      7s\n",
      "\n",
      "Use crossover to convert LP symmetric solution to basic solution...\n",
      "\n",
      "Root crossover log...\n",
      "\n",
      "      72 PPushes remaining with PInf 0.0000000e+00                 7s\n",
      "       0 PPushes remaining with PInf 0.0000000e+00                 7s\n",
      "\n",
      "  Push phase complete: Pinf 0.0000000e+00, Dinf 0.0000000e+00      7s\n",
      "\n",
      "\n",
      "Root simplex log...\n",
      "\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "     416    0.0000000e+00   0.000000e+00   0.000000e+00      7s\n",
      "\n",
      "Root relaxation: objective 0.000000e+00, 416 iterations, 0.31 seconds (0.17 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0    0.00000    0   16    5.83733    0.00000   100%     -    7s\n",
      "H    0     0                       4.0000000    0.00000   100%     -    7s\n",
      "     0     0    0.00000    0   23    4.00000    0.00000   100%     -    9s\n",
      "H    0     0                       3.2426667    0.00000   100%     -    9s\n",
      "H    0     0                       2.5866667    0.00000   100%     -   13s\n",
      "     0     0    0.00000    0   16    2.58667    0.00000   100%     -   14s\n",
      "     0     0    0.00000    0   24    2.58667    0.00000   100%     -   15s\n",
      "     0     0    0.00000    0   20    2.58667    0.00000   100%     -   18s\n",
      "     0     0    0.00000    0   20    2.58667    0.00000   100%     -   20s\n",
      "     0     0    0.00356    0   16    2.58667    0.00356   100%     -   24s\n",
      "H    0     0                       2.3733333    0.00356   100%     -   24s\n",
      "H    0     0                       1.4173333    0.00356   100%     -   24s\n",
      "     0     0    0.00667    0   20    1.41733    0.00667   100%     -   26s\n",
      "     0     0    0.00889    0   16    1.41733    0.00889  99.4%     -   28s\n",
      "     0     0    0.00889    0   24    1.41733    0.00889  99.4%     -   29s\n",
      "H    0     0                       0.4720000    0.00889  98.1%     -   30s\n",
      "     0     0    0.00889    0   24    0.47200    0.00889  98.1%     -   30s\n",
      "     0     0    0.00933    0   16    0.47200    0.00933  98.0%     -   33s\n",
      "     0     0    0.00933    0   16    0.47200    0.00933  98.0%     -   33s\n",
      "     0     0    0.00933    0   24    0.47200    0.00933  98.0%     -   34s\n",
      "     0     0    0.00933    0   16    0.47200    0.00933  98.0%     -   35s\n",
      "     0     0    0.00978    0   16    0.47200    0.00978  97.9%     -   37s\n",
      "     0     0    0.00978    0   16    0.47200    0.00978  97.9%     -   37s\n",
      "     0     0    0.01067    0   20    0.47200    0.01067  97.7%     -   38s\n",
      "     0     0    0.01156    0   23    0.47200    0.01156  97.6%     -   40s\n",
      "     0     0    0.01156    0   24    0.47200    0.01156  97.6%     -   41s\n",
      "     0     0    0.01267    0   20    0.47200    0.01267  97.3%     -   42s\n",
      "     0     0    0.01378    0   16    0.47200    0.01378  97.1%     -   43s\n",
      "     0     0    0.01378    0   20    0.47200    0.01378  97.1%     -   43s\n",
      "     0     0    0.01659    0   20    0.47200    0.01659  96.5%     -   45s\n",
      "     0     0    0.01689    0   20    0.47200    0.01689  96.4%     -   46s\n",
      "     0     0    0.01689    0   20    0.47200    0.01689  96.4%     -   46s\n",
      "     0     0    0.01689    0   20    0.47200    0.01689  96.4%     -   47s\n",
      "     0     0    0.01800    0   24    0.47200    0.01800  96.2%     -   48s\n",
      "     0     0    0.01822    0   28    0.47200    0.01822  96.1%     -   49s\n",
      "     0     0    0.01956    0   24    0.47200    0.01956  95.9%     -   52s\n",
      "     0     0    0.01989    0   20    0.47200    0.01989  95.8%     -   53s\n",
      "     0     0    0.02111    0   50    0.47200    0.02111  95.5%     -   55s\n",
      "     0     0    0.02207    0   50    0.47200    0.02207  95.3%     -   56s\n",
      "     0     0    0.02278    0   46    0.47200    0.02278  95.2%     -   59s\n",
      "     0     0    0.02278    0   46    0.47200    0.02278  95.2%     -   59s\n",
      "     0     0    0.02356    0   20    0.47200    0.02356  95.0%     -   62s\n",
      "     0     0    0.02356    0   20    0.47200    0.02356  95.0%     -   62s\n",
      "     0     0    0.02356    0   16    0.47200    0.02356  95.0%     -   63s\n",
      "     0     0    0.02356    0   20    0.47200    0.02356  95.0%     -   63s\n",
      "     0     0    0.02476    0   45    0.47200    0.02476  94.8%     -   65s\n",
      "     0     0    0.02500    0   48    0.47200    0.02500  94.7%     -   66s\n",
      "     0     0    0.02578    0   16    0.47200    0.02578  94.5%     -   67s\n",
      "     0     0    0.02600    0   16    0.47200    0.02600  94.5%     -   68s\n",
      "     0     0    0.02748    0   49    0.47200    0.02748  94.2%     -   69s\n",
      "     0     0    0.02756    0   40    0.47200    0.02756  94.2%     -   70s\n",
      "     0     0    0.02800    0   40    0.47200    0.02800  94.1%     -   71s\n",
      "     0     0    0.02800    0   32    0.47200    0.02800  94.1%     -   71s\n",
      "     0     0    0.02889    0   32    0.47200    0.02889  93.9%     -   73s\n",
      "     0     0    0.02911    0   40    0.47200    0.02911  93.8%     -   74s\n",
      "     0     0    0.02911    0   40    0.47200    0.02911  93.8%     -   74s\n",
      "     0     0    0.02933    0   49    0.47200    0.02933  93.8%     -   75s\n",
      "     0     0    0.02933    0   46    0.47200    0.02933  93.8%     -   76s\n",
      "     0     0    0.02989    0   58    0.47200    0.02989  93.7%     -   78s\n",
      "     0     0    0.03035    0   60    0.47200    0.03035  93.6%     -   79s\n",
      "     0     0    0.03086    0   60    0.47200    0.03086  93.5%     -   80s\n",
      "     0     0    0.03100    0   60    0.47200    0.03100  93.4%     -   81s\n",
      "     0     0    0.03170    0   58    0.47200    0.03170  93.3%     -   82s\n",
      "     0     0    0.03180    0   60    0.47200    0.03180  93.3%     -   82s\n",
      "     0     0    0.03279    0   57    0.47200    0.03279  93.1%     -   84s\n",
      "     0     0    0.03295    0   57    0.47200    0.03295  93.0%     -   85s\n",
      "     0     0    0.03334    0   56    0.47200    0.03334  92.9%     -   86s\n",
      "     0     0    0.03417    0   56    0.47200    0.03417  92.8%     -   86s\n",
      "     0     0    0.03689    0   32    0.47200    0.03689  92.2%     -   88s\n",
      "     0     0    0.03711    0   32    0.47200    0.03711  92.1%     -   89s\n",
      "     0     0    0.03789    0   56    0.47200    0.03789  92.0%     -   90s\n",
      "     0     0    0.03833    0   56    0.47200    0.03833  91.9%     -   90s\n",
      "     0     0    0.03867    0   56    0.47200    0.03867  91.8%     -   92s\n",
      "     0     0    0.03900    0   56    0.47200    0.03900  91.7%     -   92s\n",
      "     0     0    0.03911    0   52    0.47200    0.03911  91.7%     -   93s\n",
      "     0     0    0.03978    0   56    0.47200    0.03978  91.6%     -   94s\n",
      "     0     0    0.03978    0   56    0.47200    0.03978  91.6%     -   95s\n",
      "     0     0    0.03978    0   56    0.47200    0.03978  91.6%     -   96s\n",
      "     0     0    0.04111    0   56    0.47200    0.04111  91.3%     -   98s\n",
      "     0     0    0.04111    0   56    0.47200    0.04111  91.3%     -   98s\n",
      "     0     0    0.04156    0   32    0.47200    0.04156  91.2%     -   99s\n",
      "     0     0    0.04172    0   55    0.47200    0.04172  91.2%     -  100s\n",
      "     0     0    0.04222    0   40    0.47200    0.04222  91.1%     -  101s\n",
      "     0     0    0.04222    0   40    0.47200    0.04222  91.1%     -  102s\n",
      "     0     0    0.04228    0   56    0.47200    0.04228  91.0%     -  103s\n",
      "     0     0    0.04267    0   48    0.47200    0.04267  91.0%     -  104s\n",
      "     0     0    0.04267    0   48    0.47200    0.04267  91.0%     -  105s\n",
      "     0     2    0.04267    0   48    0.47200    0.04267  91.0%     -  107s\n",
      "     3     4    0.05993    2   16    0.47200    0.05970  87.4%   402  113s\n",
      "     9     4     cutoff    4         0.47200    0.06267  86.7%  1346  117s\n",
      "\n",
      "Cutting planes:\n",
      "  MIR: 12\n",
      "  RLT: 272\n",
      "\n",
      "Explored 17 nodes (31224 simplex iterations) in 118.37 seconds (71.78 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 7: 0.472 1.41733 2.37333 ... 5.83733\n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 4.720000000000e-01, best bound 4.720000000000e-01, gap 0.0000%\n",
      "================================================\n",
      "Gurobi Optimizer version 9.5.1 build v9.5.1rc2 (win64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 304534 rows, 39097 columns and 2448936 nonzeros\n",
      "Model fingerprint: 0x821e06ed\n",
      "Variable types: 39001 continuous, 96 integer (96 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [1e-03, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 2e+00]\n",
      "Presolve removed 0 rows and 11700 columns (presolve time = 5s) ...\n",
      "Presolve removed 272745 rows and 36153 columns\n",
      "Presolve time: 7.80s\n",
      "Presolved: 31789 rows, 2944 columns, 255690 nonzeros\n",
      "Variable types: 2848 continuous, 96 integer (96 binary)\n",
      "Found heuristic solution: objective 5.9853333\n",
      "\n",
      "Root simplex log...\n",
      "\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0      handle free variables                          9s\n",
      "     389   -0.0000000e+00   0.000000e+00   0.000000e+00      9s\n",
      "     389   -0.0000000e+00   0.000000e+00   0.000000e+00      9s\n",
      "     389    0.0000000e+00   0.000000e+00   0.000000e+00      9s\n",
      "\n",
      "Use crossover to convert LP symmetric solution to basic solution...\n",
      "\n",
      "Root crossover log...\n",
      "\n",
      "      78 PPushes remaining with PInf 0.0000000e+00                 9s\n",
      "       0 PPushes remaining with PInf 0.0000000e+00                 9s\n",
      "\n",
      "  Push phase complete: Pinf 0.0000000e+00, Dinf 0.0000000e+00      9s\n",
      "\n",
      "\n",
      "Root simplex log...\n",
      "\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "     470    0.0000000e+00   0.000000e+00   0.000000e+00      9s\n",
      "\n",
      "Root relaxation: objective 0.000000e+00, 470 iterations, 0.36 seconds (0.19 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0    0.00000    0   16    5.98533    0.00000   100%     -    9s\n",
      "H    0     0                       4.0000000    0.00000   100%     -    9s\n",
      "H    0     0                       1.9666667    0.00000   100%     -   10s\n",
      "     0     0    0.00000    0   16    1.96667    0.00000   100%     -   11s\n",
      "     0     0    0.00000    0   16    1.96667    0.00000   100%     -   14s\n",
      "     0     0    0.00000    0   16    1.96667    0.00000   100%     -   15s\n",
      "     0     0    0.00000    0   20    1.96667    0.00000   100%     -   17s\n",
      "     0     0    0.00000    0   29    1.96667    0.00000   100%     -   18s\n",
      "     0     0    0.00267    0   16    1.96667    0.00267   100%     -   22s\n",
      "     0     0    0.00667    0   20    1.96667    0.00667   100%     -   25s\n",
      "     0     0    0.01200    0   24    1.96667    0.01200  99.4%     -   29s\n",
      "     0     0    0.01200    0   24    1.96667    0.01200  99.4%     -   29s\n",
      "     0     0    0.01200    0   16    1.96667    0.01200  99.4%     -   30s\n",
      "     0     0    0.01200    0   20    1.96667    0.01200  99.4%     -   30s\n",
      "     0     0    0.01200    0   16    1.96667    0.01200  99.4%     -   32s\n",
      "     0     0    0.01200    0   20    1.96667    0.01200  99.4%     -   32s\n",
      "     0     0    0.01200    0   20    1.96667    0.01200  99.4%     -   33s\n",
      "     0     0    0.01200    0   16    1.96667    0.01200  99.4%     -   34s\n",
      "     0     0    0.01200    0   16    1.96667    0.01200  99.4%     -   35s\n",
      "     0     2    0.01200    0   16    1.96667    0.01200  99.4%     -   36s\n",
      "     3     6     cutoff    2         1.96667    0.01733  99.1%  8706   54s\n",
      "     7    10    0.83293    3   27    1.96667    0.04400  97.8%  4822   58s\n",
      "    11    12    0.87481    4   27    1.96667    0.04400  97.8%  3518   61s\n",
      "*   13    12               4       1.0506667    0.04400  95.8%  3502   61s\n",
      "    15    14    0.88933    5   12    1.05067    0.04400  95.8%  3056   67s\n",
      "*   28    10               7       0.5466667    0.04400  92.0%  1818   68s\n",
      "    29     4     cutoff    5         0.54667    0.04852  91.1%  1755   74s\n",
      "    34     2    0.05867    5   14    0.54667    0.05200  90.5%  1773   75s\n",
      "    41     0     cutoff    7         0.54667    0.54667  0.00%  1739   80s\n",
      "\n",
      "Cutting planes:\n",
      "  RLT: 66\n",
      "\n",
      "Explored 42 nodes (79391 simplex iterations) in 80.76 seconds (119.91 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 5: 0.546667 1.05067 1.96667 ... 5.98533\n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 5.466666666667e-01, best bound 5.466666666667e-01, gap 0.0000%\n",
      "================================================\n",
      "Gurobi Optimizer version 9.5.1 build v9.5.1rc2 (win64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 348034 rows, 39097 columns and 2798736 nonzeros\n",
      "Model fingerprint: 0xa8737f85\n",
      "Variable types: 39001 continuous, 96 integer (96 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [1e-03, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 2e+00]\n",
      "Presolve removed 0 rows and 7800 columns (presolve time = 5s) ...\n",
      "Presolve removed 314795 rows and 36023 columns\n",
      "Presolve time: 7.84s\n",
      "Presolved: 33239 rows, 3074 columns, 267350 nonzeros\n",
      "Variable types: 2978 continuous, 96 integer (96 binary)\n",
      "Found heuristic solution: objective 6.1266667\n",
      "\n",
      "Root simplex log...\n",
      "\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0      handle free variables                          9s\n",
      "     346   -0.0000000e+00   0.000000e+00   0.000000e+00      9s\n",
      "     346   -0.0000000e+00   0.000000e+00   0.000000e+00      9s\n",
      "     346    0.0000000e+00   0.000000e+00   0.000000e+00      9s\n",
      "\n",
      "Use crossover to convert LP symmetric solution to basic solution...\n",
      "\n",
      "Root crossover log...\n",
      "\n",
      "      75 PPushes remaining with PInf 0.0000000e+00                 9s\n",
      "       0 PPushes remaining with PInf 0.0000000e+00                 9s\n",
      "\n",
      "  Push phase complete: Pinf 0.0000000e+00, Dinf 0.0000000e+00      9s\n",
      "\n",
      "\n",
      "Root simplex log...\n",
      "\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "     424    0.0000000e+00   0.000000e+00   0.000000e+00      9s\n",
      "\n",
      "Root relaxation: objective 0.000000e+00, 424 iterations, 0.31 seconds (0.19 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0    0.00000    0   16    6.12667    0.00000   100%     -    9s\n",
      "H    0     0                       4.0000000    0.00000   100%     -    9s\n",
      "H    0     0                       2.3506667    0.00000   100%     -   11s\n",
      "     0     0    0.00000    0   29    2.35067    0.00000   100%     -   11s\n",
      "H    0     0                       2.3466667    0.00000   100%     -   11s\n",
      "H    0     0                       1.5120000    0.00000   100%     -   12s\n",
      "     0     0    0.00000    0   30    1.51200    0.00000   100%     -   14s\n",
      "     0     0    0.00000    0   26    1.51200    0.00000   100%     -   16s\n",
      "     0     0    0.00133    0   16    1.51200    0.00133   100%     -   18s\n",
      "     0     0    0.00133    0   21    1.51200    0.00133   100%     -   19s\n",
      "     0     0    0.00267    0   24    1.51200    0.00267   100%     -   20s\n",
      "     0     0    0.00400    0   27    1.51200    0.00400   100%     -   21s\n",
      "H    0     0                       0.6080000    0.01200  98.0%     -   25s\n",
      "     0     0    0.01200    0   20    0.60800    0.01200  98.0%     -   25s\n",
      "     0     0    0.01200    0   30    0.60800    0.01200  98.0%     -   25s\n",
      "     0     0    0.01200    0   24    0.60800    0.01200  98.0%     -   26s\n",
      "     0     0    0.01200    0   20    0.60800    0.01200  98.0%     -   28s\n",
      "     0     0    0.01200    0   20    0.60800    0.01200  98.0%     -   28s\n",
      "     0     0    0.01200    0   20    0.60800    0.01200  98.0%     -   29s\n",
      "     0     0    0.01267    0   30    0.60800    0.01267  97.9%     -   31s\n",
      "     0     0    0.01267    0   24    0.60800    0.01267  97.9%     -   32s\n",
      "     0     0    0.01267    0   26    0.60800    0.01267  97.9%     -   33s\n",
      "     0     0    0.01311    0   24    0.60800    0.01311  97.8%     -   35s\n",
      "     0     0    0.01311    0   24    0.60800    0.01311  97.8%     -   35s\n",
      "     0     0    0.01378    0   29    0.60800    0.01378  97.7%     -   36s\n",
      "     0     0    0.01400    0   27    0.60800    0.01400  97.7%     -   37s\n",
      "     0     0    0.01556    0   16    0.60800    0.01556  97.4%     -   38s\n",
      "     0     0    0.01867    0   24    0.60800    0.01867  96.9%     -   40s\n",
      "     0     0    0.02000    0   24    0.60800    0.02000  96.7%     -   42s\n",
      "     0     0    0.02067    0   24    0.60800    0.02067  96.6%     -   43s\n",
      "     0     0    0.02067    0   24    0.60800    0.02067  96.6%     -   44s\n",
      "     0     0    0.02067    0   24    0.60800    0.02067  96.6%     -   44s\n",
      "     0     0    0.02067    0   24    0.60800    0.02067  96.6%     -   45s\n",
      "     0     0    0.02067    0   24    0.60800    0.02067  96.6%     -   45s\n",
      "     0     0    0.02200    0   16    0.60800    0.02200  96.4%     -   47s\n",
      "     0     0    0.02200    0   24    0.60800    0.02200  96.4%     -   48s\n",
      "     0     0    0.02353    0   37    0.60800    0.02353  96.1%     -   49s\n",
      "     0     0    0.02400    0   41    0.60800    0.02400  96.1%     -   51s\n",
      "     0     0    0.02400    0   20    0.60800    0.02400  96.1%     -   52s\n",
      "     0     0    0.02400    0   24    0.60800    0.02400  96.1%     -   53s\n",
      "     0     0    0.02400    0   24    0.60800    0.02400  96.1%     -   54s\n",
      "     0     0    0.02417    0   34    0.60800    0.02417  96.0%     -   54s\n",
      "     0     0    0.02556    0   32    0.60800    0.02556  95.8%     -   56s\n",
      "     0     0    0.02578    0   36    0.60800    0.02578  95.8%     -   57s\n",
      "     0     0    0.02800    0   24    0.60800    0.02800  95.4%     -   59s\n",
      "     0     0    0.02867    0   40    0.60800    0.02867  95.3%     -   61s\n",
      "     0     0    0.02889    0   32    0.60800    0.02889  95.2%     -   64s\n",
      "     0     0    0.02911    0   40    0.60800    0.02911  95.2%     -   66s\n",
      "     0     0    0.02933    0   32    0.60800    0.02933  95.2%     -   68s\n",
      "     0     0    0.02933    0   44    0.60800    0.02933  95.2%     -   69s\n",
      "     0     0    0.02933    0   40    0.60800    0.02933  95.2%     -   70s\n",
      "     0     0    0.02933    0   44    0.60800    0.02933  95.2%     -   71s\n",
      "     0     0    0.03000    0   32    0.60800    0.03000  95.1%     -   72s\n",
      "     0     0    0.03000    0   32    0.60800    0.03000  95.1%     -   73s\n",
      "     0     0    0.03000    0   40    0.60800    0.03000  95.1%     -   74s\n",
      "     0     0    0.03007    0   40    0.60800    0.03007  95.1%     -   75s\n",
      "     0     0    0.03010    0   40    0.60800    0.03010  95.1%     -   78s\n",
      "     0     0    0.03022    0   32    0.60800    0.03022  95.0%     -   79s\n",
      "     0     0    0.03022    0   32    0.60800    0.03022  95.0%     -   80s\n",
      "     0     0    0.03050    0   42    0.60800    0.03050  95.0%     -   81s\n",
      "     0     0    0.03089    0   44    0.60800    0.03089  94.9%     -   82s\n",
      "     0     0    0.03089    0   44    0.60800    0.03089  94.9%     -   83s\n",
      "     0     0    0.03156    0   40    0.60800    0.03156  94.8%     -   84s\n",
      "     0     0    0.03164    0   44    0.60800    0.03164  94.8%     -   85s\n",
      "     0     0    0.03200    0   40    0.60800    0.03200  94.7%     -   86s\n",
      "     0     0    0.03200    0   44    0.60800    0.03200  94.7%     -   87s\n",
      "     0     0    0.03333    0   62    0.60800    0.03333  94.5%     -   89s\n",
      "     0     0    0.03333    0   62    0.60800    0.03333  94.5%     -   90s\n",
      "     0     0    0.03355    0   52    0.60800    0.03355  94.5%     -   92s\n",
      "     0     0    0.03358    0   58    0.60800    0.03358  94.5%     -   93s\n",
      "     0     0    0.03388    0   58    0.60800    0.03388  94.4%     -   94s\n",
      "     0     0    0.03388    0   50    0.60800    0.03388  94.4%     -   95s\n",
      "     0     0    0.03411    0   56    0.60800    0.03411  94.4%     -   97s\n",
      "     0     0    0.03433    0   56    0.60800    0.03433  94.4%     -   98s\n",
      "     0     0    0.03433    0   54    0.60800    0.03433  94.4%     -  100s\n",
      "     0     0    0.03433    0   56    0.60800    0.03433  94.4%     -  100s\n",
      "     0     0    0.03433    0   56    0.60800    0.03433  94.4%     -  102s\n",
      "     0     0    0.03433    0   56    0.60800    0.03433  94.4%     -  102s\n",
      "     0     0    0.03467    0   32    0.60800    0.03467  94.3%     -  105s\n",
      "     0     0    0.03527    0   54    0.60800    0.03527  94.2%     -  106s\n",
      "     0     0    0.03667    0   40    0.60800    0.03667  94.0%     -  109s\n",
      "     0     0    0.03667    0   40    0.60800    0.03667  94.0%     -  110s\n",
      "     0     0    0.03667    0   40    0.60800    0.03667  94.0%     -  112s\n",
      "     0     0    0.03667    0   40    0.60800    0.03667  94.0%     -  112s\n",
      "     0     2    0.03667    0   40    0.60800    0.03667  94.0%     -  114s\n",
      "     1     4    0.55549    1   48    0.60800    0.03667  94.0% 10584  125s\n",
      "     3     2     cutoff    2         0.60800    0.04397  92.8%  3785  135s\n",
      "\n",
      "Cutting planes:\n",
      "  MIR: 1\n",
      "  RLT: 224\n",
      "\n",
      "Explored 9 nodes (39154 simplex iterations) in 139.28 seconds (102.53 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 6: 0.608 1.512 2.34667 ... 6.12667\n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 6.079999999999e-01, best bound 6.079999999999e-01, gap 0.0000%\n",
      "================================================\n",
      "Gurobi Optimizer version 9.5.1 build v9.5.1rc2 (win64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 391534 rows, 39097 columns and 3148536 nonzeros\n",
      "Model fingerprint: 0x892f7fcb\n",
      "Variable types: 39001 continuous, 96 integer (96 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [1e-03, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 2e+00]\n",
      "Presolve removed 0 rows and 3900 columns (presolve time = 5s) ...\n",
      "Presolve removed 356700 rows and 35880 columns (presolve time = 10s) ...\n",
      "Presolve removed 356700 rows and 35880 columns\n",
      "Presolve time: 10.40s\n",
      "Presolved: 34834 rows, 3217 columns, 280176 nonzeros\n",
      "Variable types: 3121 continuous, 96 integer (96 binary)\n",
      "Found heuristic solution: objective 6.2000000\n",
      "\n",
      "Root simplex log...\n",
      "\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0      handle free variables                         11s\n",
      "     385   -0.0000000e+00   0.000000e+00   0.000000e+00     11s\n",
      "     385   -0.0000000e+00   0.000000e+00   0.000000e+00     11s\n",
      "     385    0.0000000e+00   0.000000e+00   0.000000e+00     11s\n",
      "\n",
      "Use crossover to convert LP symmetric solution to basic solution...\n",
      "\n",
      "Root crossover log...\n",
      "\n",
      "      75 PPushes remaining with PInf 0.0000000e+00                11s\n",
      "       0 PPushes remaining with PInf 0.0000000e+00                12s\n",
      "\n",
      "  Push phase complete: Pinf 0.0000000e+00, Dinf 0.0000000e+00     12s\n",
      "\n",
      "\n",
      "Root simplex log...\n",
      "\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "     463    0.0000000e+00   0.000000e+00   0.000000e+00     12s\n",
      "\n",
      "Root relaxation: objective 0.000000e+00, 463 iterations, 0.36 seconds (0.20 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0    0.00000    0   16    6.20000    0.00000   100%     -   12s\n",
      "H    0     0                       4.0000000    0.00000   100%     -   12s\n",
      "H    0     0                       3.4106667    0.00000   100%     -   13s\n",
      "     0     0    0.00000    0   25    3.41067    0.00000   100%     -   14s\n",
      "     0     0    0.00000    0   21    3.41067    0.00000   100%     -   15s\n",
      "H    0     0                       2.6013333    0.00000   100%     -   21s\n",
      "     0     0    0.00000    0   21    2.60133    0.00000   100%     -   22s\n",
      "     0     0    0.00000    0   26    2.60133    0.00000   100%     -   22s\n",
      "     0     0    0.00000    0   16    2.60133    0.00000   100%     -   25s\n",
      "     0     0    0.00089    0   16    2.60133    0.00089   100%     -   29s\n",
      "     0     0    0.00133    0   20    2.60133    0.00133   100%     -   30s\n",
      "     0     0    0.00533    0   20    2.60133    0.00533   100%     -   33s\n",
      "H    0     0                       2.0893333    0.00533   100%     -   34s\n",
      "H    0     0                       1.5666667    0.00533   100%     -   35s\n",
      "     0     0    0.00533    0   24    1.56667    0.00533   100%     -   35s\n",
      "     0     0    0.01467    0   26    1.56667    0.01467  99.1%     -   39s\n",
      "     0     0    0.01467    0   24    1.56667    0.01467  99.1%     -   40s\n",
      "     0     0    0.01467    0   29    1.56667    0.01467  99.1%     -   41s\n",
      "     0     0    0.01467    0   27    1.56667    0.01467  99.1%     -   42s\n",
      "     0     0    0.01556    0   20    1.56667    0.01556  99.0%     -   44s\n",
      "     0     0    0.01556    0   20    1.56667    0.01556  99.0%     -   45s\n",
      "H    0     0                       0.7626667    0.01556  98.0%     -   45s\n",
      "     0     0    0.01644    0   20    0.76267    0.01644  97.8%     -   46s\n",
      "     0     0    0.01644    0   20    0.76267    0.01644  97.8%     -   47s\n",
      "     0     0    0.01644    0   26    0.76267    0.01644  97.8%     -   48s\n",
      "     0     0    0.01644    0   25    0.76267    0.01644  97.8%     -   49s\n",
      "     0     0    0.01644    0   16    0.76267    0.01644  97.8%     -   50s\n",
      "     0     0    0.01778    0   23    0.76267    0.01778  97.7%     -   51s\n",
      "     0     0    0.01778    0   25    0.76267    0.01778  97.7%     -   53s\n",
      "     0     0    0.01778    0   30    0.76267    0.01778  97.7%     -   53s\n",
      "     0     0    0.01822    0   24    0.76267    0.01822  97.6%     -   54s\n",
      "     0     0    0.01822    0   24    0.76267    0.01822  97.6%     -   55s\n",
      "     0     0    0.02133    0   27    0.76267    0.02133  97.2%     -   58s\n",
      "     0     0    0.02133    0   23    0.76267    0.02133  97.2%     -   59s\n",
      "H    0     0                       0.6960000    0.02200  96.8%     -   62s\n",
      "     0     0    0.02200    0   16    0.69600    0.02200  96.8%     -   62s\n",
      "     0     0    0.02689    0   16    0.69600    0.02689  96.1%     -   65s\n",
      "     0     0    0.02822    0   24    0.69600    0.02822  95.9%     -   67s\n",
      "     0     0    0.03111    0   24    0.69600    0.03111  95.5%     -   69s\n",
      "     0     0    0.03156    0   16    0.69600    0.03156  95.5%     -   70s\n",
      "     0     0    0.03244    0   20    0.69600    0.03244  95.3%     -   72s\n",
      "     0     0    0.03322    0   42    0.69600    0.03322  95.2%     -   74s\n",
      "     0     0    0.03356    0   47    0.69600    0.03356  95.2%     -   78s\n",
      "     0     0    0.03356    0   42    0.69600    0.03356  95.2%     -   79s\n",
      "     0     0    0.03422    0   42    0.69600    0.03422  95.1%     -   84s\n",
      "     0     0    0.03422    0   42    0.69600    0.03422  95.1%     -   85s\n",
      "     0     0    0.03448    0   34    0.69600    0.03448  95.0%     -   88s\n",
      "     0     0    0.03448    0   34    0.69600    0.03448  95.0%     -   89s\n",
      "     0     0    0.03533    0   48    0.69600    0.03533  94.9%     -   95s\n",
      "     0     0    0.03533    0   50    0.69600    0.03533  94.9%     -   96s\n",
      "     0     0    0.03600    0   48    0.69600    0.03600  94.8%     -  101s\n",
      "     0     0    0.03600    0   40    0.69600    0.03600  94.8%     -  103s\n",
      "     0     0    0.03600    0   42    0.69600    0.03600  94.8%     -  106s\n",
      "     0     0    0.03644    0   32    0.69600    0.03644  94.8%     -  107s\n",
      "     0     0    0.03778    0   40    0.69600    0.03778  94.6%     -  111s\n",
      "     0     0    0.03778    0   32    0.69600    0.03778  94.6%     -  113s\n",
      "     0     0    0.03800    0   40    0.69600    0.03800  94.5%     -  115s\n",
      "     0     0    0.03800    0   32    0.69600    0.03800  94.5%     -  117s\n",
      "     0     0    0.03800    0   32    0.69600    0.03800  94.5%     -  119s\n",
      "     0     0    0.03933    0   40    0.69600    0.03933  94.3%     -  125s\n",
      "     0     0    0.03933    0   40    0.69600    0.03933  94.3%     -  127s\n",
      "     0     0    0.04067    0   40    0.69600    0.04067  94.2%     -  133s\n",
      "     0     0    0.04067    0   32    0.69600    0.04067  94.2%     -  135s\n",
      "     0     0    0.04067    0   42    0.69600    0.04067  94.2%     -  139s\n",
      "     0     0    0.04080    0   40    0.69600    0.04080  94.1%     -  140s\n",
      "     0     0    0.04133    0   40    0.69600    0.04133  94.1%     -  145s\n",
      "     0     0    0.04133    0   40    0.69600    0.04133  94.1%     -  146s\n",
      "     0     0    0.04133    0   40    0.69600    0.04133  94.1%     -  149s\n",
      "     0     0    0.04133    0   40    0.69600    0.04133  94.1%     -  149s\n",
      "     0     0    0.04133    0   40    0.69600    0.04133  94.1%     -  153s\n",
      "     0     0    0.04133    0   40    0.69600    0.04133  94.1%     -  154s\n",
      "     0     2    0.04133    0   40    0.69600    0.04133  94.1%     -  156s\n",
      "     1     2     cutoff    1         0.69600    0.04133  94.1% 37778  224s\n",
      "     3     4    0.06867    2   16    0.69600    0.04689  93.3% 12790  240s\n",
      "     9     2     cutoff    4         0.69600    0.08656  87.6%  5656  248s\n",
      "\n",
      "Cutting planes:\n",
      "  RLT: 199\n",
      "\n",
      "Explored 15 nodes (65590 simplex iterations) in 249.11 seconds (169.81 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 8: 0.696 0.762667 1.56667 ... 6.2\n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 6.960000000000e-01, best bound 6.960000000000e-01, gap 0.0000%\n",
      "================================================\n",
      "finish!!\n",
      "Gurobi Optimizer version 9.5.1 build v9.5.1rc2 (win64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 96 rows, 96 columns and 432 nonzeros\n",
      "Model fingerprint: 0x7653d5fb\n",
      "Variable types: 0 continuous, 96 integer (96 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 4e+00]\n",
      "Found heuristic solution: objective 4.0000000\n",
      "Presolve removed 0 rows and 8 columns\n",
      "Presolve time: 0.01s\n",
      "Presolved: 96 rows, 88 columns, 400 nonzeros\n",
      "Found heuristic solution: objective 0.0000000\n",
      "Variable types: 0 continuous, 88 integer (88 binary)\n",
      "\n",
      "Explored 0 nodes (0 simplex iterations) in 0.05 seconds (0.00 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 2: 0 4 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 0.000000000000e+00, best bound 0.000000000000e+00, gap 0.0000%\n",
      "====================================================\n",
      "[[1 0 0 1]\n",
      " [3 0 0 4]\n",
      " [3 2 2 4]\n",
      " [5 5 6 6]]\n",
      "the objective function 0.0\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8]\n",
      "Gurobi Optimizer version 9.5.1 build v9.5.1rc2 (win64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 160 rows, 96 columns and 624 nonzeros\n",
      "Model fingerprint: 0x14f73017\n",
      "Variable types: 0 continuous, 96 integer (96 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 4e+00]\n",
      "Found heuristic solution: objective 4.0000000\n",
      "Presolve removed 48 rows and 8 columns\n",
      "Presolve time: 0.01s\n",
      "Presolved: 112 rows, 88 columns, 528 nonzeros\n",
      "Found heuristic solution: objective 3.0000000\n",
      "Variable types: 0 continuous, 88 integer (88 binary)\n",
      "\n",
      "Root relaxation: objective 0.000000e+00, 24 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "*    0     0               0       0.0000000    0.00000  0.00%     -    0s\n",
      "\n",
      "Explored 1 nodes (24 simplex iterations) in 0.07 seconds (0.00 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 3: 0 3 4 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 0.000000000000e+00, best bound 0.000000000000e+00, gap 0.0000%\n",
      "====================================================\n",
      "[[3 1 0 0]\n",
      " [3 1 2 0]\n",
      " [6 4 5 0]\n",
      " [6 4 5 2]]\n",
      "the objective function 0.0\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "Gurobi Optimizer version 9.5.1 build v9.5.1rc2 (win64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 208 rows, 96 columns and 768 nonzeros\n",
      "Model fingerprint: 0x80e74690\n",
      "Variable types: 0 continuous, 96 integer (96 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 4e+00]\n",
      "Found heuristic solution: objective 6.0000000\n",
      "Presolve removed 80 rows and 8 columns\n",
      "Presolve time: 0.01s\n",
      "Presolved: 128 rows, 88 columns, 656 nonzeros\n",
      "Found heuristic solution: objective 2.0000000\n",
      "Variable types: 0 continuous, 88 integer (88 binary)\n",
      "\n",
      "Root relaxation: objective 0.000000e+00, 40 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "*    0     0               0       0.0000000    0.00000  0.00%     -    0s\n",
      "\n",
      "Explored 1 nodes (40 simplex iterations) in 0.05 seconds (0.00 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 3: 0 2 6 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 0.000000000000e+00, best bound 0.000000000000e+00, gap 0.0000%\n",
      "====================================================\n",
      "[[0 2 1 0]\n",
      " [0 2 1 0]\n",
      " [3 6 5 4]\n",
      " [3 6 5 4]]\n",
      "the objective function 0.0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "Gurobi Optimizer version 9.5.1 build v9.5.1rc2 (win64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 240 rows, 96 columns and 864 nonzeros\n",
      "Model fingerprint: 0x4c425b2c\n",
      "Variable types: 0 continuous, 96 integer (96 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 4e+00]\n",
      "Found heuristic solution: objective 6.0000000\n",
      "Presolve removed 72 rows and 8 columns\n",
      "Presolve time: 0.01s\n",
      "Presolved: 168 rows, 88 columns, 920 nonzeros\n",
      "Found heuristic solution: objective 2.0000000\n",
      "Variable types: 0 continuous, 88 integer (88 binary)\n",
      "\n",
      "Root relaxation: cutoff, 102 iterations, 0.01 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0     cutoff    0         2.00000    2.00000  0.00%     -    0s\n",
      "\n",
      "Explored 1 nodes (102 simplex iterations) in 0.07 seconds (0.00 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 2: 2 6 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 2.000000000000e+00, best bound 2.000000000000e+00, gap 0.0000%\n",
      "====================================================\n",
      "[[0 0 0 1]\n",
      " [2 2 0 1]\n",
      " [5 3 4 6]\n",
      " [5 3 4 6]]\n",
      "the objective function 2.0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4]\n",
      "Gurobi Optimizer version 9.5.1 build v9.5.1rc2 (win64)\n",
      "Thread count: 4 physical cores, 8 logical processors, using up to 8 threads\n",
      "Optimize a model with 256 rows, 96 columns and 912 nonzeros\n",
      "Model fingerprint: 0x4c9c2536\n",
      "Variable types: 0 continuous, 96 integer (96 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 4e+00]\n",
      "Found heuristic solution: objective 5.0000000\n",
      "Presolve removed 80 rows and 8 columns\n",
      "Presolve time: 0.01s\n",
      "Presolved: 176 rows, 88 columns, 1008 nonzeros\n",
      "Variable types: 0 continuous, 88 integer (88 binary)\n",
      "Found heuristic solution: objective 4.0000000\n",
      "\n",
      "Root relaxation: cutoff, 82 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0     cutoff    0         4.00000    4.00000  0.00%     -    0s\n",
      "\n",
      "Explored 1 nodes (82 simplex iterations) in 0.05 seconds (0.00 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 2: 4 5 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 4.000000000000e+00, best bound 4.000000000000e+00, gap 0.0000%\n",
      "====================================================\n",
      "[[0 2 0 0]\n",
      " [1 2 0 1]\n",
      " [5 3 4 6]\n",
      " [5 3 4 6]]\n",
      "the objective function 4.0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "size=3000\n",
    "size_=10000\n",
    "beta=0.75\n",
    "inst=[4,4,[2,2,2,2,2,2]]\n",
    "pcvar=[]\n",
    "ship_num=len(inst[2])\n",
    "import numpy as np\n",
    "# mean=np.arange(1,ship_num+1)\n",
    "mean=np.array([0.8,1,3,3.2,3.5,5.5])\n",
    "covl=[2 for i in range(ship_num)]\n",
    "# a=list(a)\n",
    "# a=random.sample(a,ship_num)\n",
    "# mean=np.sort(a)\n",
    "cov=np.zeros((ship_num,ship_num))\n",
    "for i in range(ship_num):\n",
    "    for j in range(ship_num):\n",
    "        if i==j:\n",
    "            cov[i][i]=covl[1]\n",
    "for i in range(ship_num):\n",
    "    for j in range(ship_num):\n",
    "        if i!=j:\n",
    "            cov[i][j]=0\n",
    "OC=[]\n",
    "EPS=0.1\n",
    "a=0\n",
    "UB=1000\n",
    "LB=0\n",
    "UB_k=[]\n",
    "while UB-LB>=EPS and a<10:\n",
    "    CVaR(len(inst[2]),inst[0],inst[1],inst[2],size,size_,beta,mean,cov,OC)\n",
    "    n=0\n",
    "    while n<beta*size:\n",
    "        n+=1\n",
    "    tau=n\n",
    "    alpha_=penalty[tau-1]\n",
    "\n",
    "    UB=((tau/size-beta)*alpha_+sum(penalty[tau:]))\n",
    "    UB_k.append(UB)\n",
    "    UB=min(UB_k)\n",
    "    for b in range(round(size*0.1*a),round(size*0.1*(a+1))):\n",
    "        OC.append(O[b])\n",
    "    a+=1\n",
    "    # print(OC)\n",
    "print(\"finish!!\")\n",
    "\n",
    "data_2 = np.random.multivariate_normal(mean, cov, size=size_)\n",
    "\n",
    "OR_=np.argsort(data_2)\n",
    "OR_=OR_+np.ones((size_,ship_num)).astype(int)\n",
    "OR=OR_.tolist()\n",
    "penalty_cvar=[]\n",
    "for k in OR:\n",
    "    OR2=k\n",
    "    a=0\n",
    "    for j in range(inst[0]):\n",
    "        for i in range(1,inst[1]):\n",
    "            for i_ in range(i+1,inst[1]+1):\n",
    "                if result[i-1][j]!=0 and result[i_-1][j]!=0:\n",
    "                    if OR2.index(result[i-1][j])>OR2.index(result[i_-1][j]):\n",
    "                        # print(j+1,height-i+1,O)\n",
    "                        a+=1\n",
    "                        # print(\"penalty!\")\n",
    "                            # print(i,i_,j+1,O)\n",
    "                        break\n",
    "    penalty_cvar.append(a)\n",
    "penalty_cvar=np.sort(penalty_cvar)\n",
    "pcvar=[]\n",
    "pcvar+=list(penalty_cvar)\n",
    "data=(tuple(pcvar),)\n",
    "l=[\"75%-CVaR\"]\n",
    "for g in range(1,6):\n",
    "  prob=[]\n",
    "  ship_num=len(inst[2])\n",
    "  robust(ship_num,inst[0],inst[1],inst[2],g,size,size_,mean,cov)\n",
    "  prob+=list(penalty_r)\n",
    "  print(prob)\n",
    "  data+=(tuple(prob),)\n",
    "  l.append(\"Gamma=\"+str(g))\n",
    "  # print(\"data=\",data)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.boxplot(data,labels=l)\n",
    "plt.show()\n",
    "plt.hist(penalty_cvar,bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_2 = np.random.multivariate_normal(mean, cov, size=size_)\n",
    "\n",
    "OR_=np.argsort(data_2)\n",
    "OR_=OR_+np.ones((size_,ship_num)).astype(int)\n",
    "OR=OR_.tolist()\n",
    "penalty_cvar=[]\n",
    "for k in OR:\n",
    "    OR2=k\n",
    "    a=0\n",
    "    for j in range(inst[0]):\n",
    "        for i in range(1,inst[1]):\n",
    "            for i_ in range(i+1,inst[1]+1):\n",
    "                if result[i-1][j]!=0 and result[i_-1][j]!=0:\n",
    "                    if OR2.index(result[i-1][j])>OR2.index(result[i_-1][j]):\n",
    "                        # print(j+1,height-i+1,O)\n",
    "                        a+=1\n",
    "                        # print(\"penalty!\")\n",
    "                            # print(i,i_,j+1,O)\n",
    "                        break\n",
    "    penalty_cvar.append(a)\n",
    "penalty_cvar=np.sort(penalty_cvar)\n",
    "import matplotlib.pyplot as plt\n",
    "penalty_cvar=penalty_cvar[round(0.75*size_):]\n",
    "# plt.hist(penalty_cvar,bins=50)\n",
    "plt.boxplot(penalty_cvar)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0bc652a6662848c169ddaad7e75fc7966486f1f662e7670e7ffb2b305ef6abae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
